{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beade33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create role probes and tests them\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb75c7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.loss.loss_utils import ForCausalLMLoss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cupy\n",
    "import cuml\n",
    "import importlib\n",
    "import gc\n",
    "import yaml\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "from termcolor import colored\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.role_assignments import label_content_roles\n",
    "from utils.role_templates import load_chat_template\n",
    "from utils.store_outputs import convert_outputs_to_df_fast\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "\n",
    "clear_all_cuda_memory()\n",
    "check_memory()\n",
    "\n",
    "ws = '/workspace/deliberative-alignment-jailbreaks'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed26db3f",
   "metadata": {},
   "source": [
    "# Load models & probes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75666ea5",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80af64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "selected_model_index = 0\n",
    "\n",
    "def get_model(index):\n",
    "    # HF model ID, model prefix, model architecture,  attn implementation, whether to use hf lib implementation\n",
    "    models = {\n",
    "        0: ('openai/gpt-oss-20b', 'gptoss20', 'gptoss', 'kernels-community/vllm-flash-attn3', True, 24),\n",
    "        1: ('openai/gpt-oss-120b', 'gptoss120', 'gptoss', 'kernels-community/vllm-flash-attn3', True, 36),\n",
    "    }\n",
    "    return models[index]\n",
    "\n",
    "def load_model_and_tokenizer(model_id, model_prefix, model_attn, model_use_hf):\n",
    "    \"\"\"\n",
    "    Load the model and tokenizer from HF, or from file if already downloaded.\n",
    "    \"\"\"\n",
    "    cache_dir = '/workspace/hf'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir = cache_dir, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, cache_dir = cache_dir, dtype = torch.bfloat16, trust_remote_code = not model_use_hf, device_map = 'auto', attn_implementation = model_attn).eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "model_id, model_prefix, model_architecture, model_attn, model_use_hf, model_n_layers = get_model(selected_model_index)\n",
    "tokenizer, model = load_model_and_tokenizer(model_id, model_prefix, model_attn, model_use_hf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bcadc3",
   "metadata": {},
   "source": [
    "## Load probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9782f7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load probes\n",
    "\"\"\"\n",
    "with open(f'{ws}/experiments/da-role-analysis/probes/{model_prefix}.pkl', 'rb') as f:\n",
    "    probes = pickle.load(f)\n",
    "\n",
    "probe_layers = [x['layer_ix'] for x in probes]\n",
    "print(f\"Probes: {str(len(probes))}\")\n",
    "probes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357ff043",
   "metadata": {},
   "source": [
    "# Run and project test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e881ce67",
   "metadata": {},
   "source": [
    "## Test tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2c5908",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tests two important functions:\n",
    "- `load_chat_template`: Overwrites the existing chat template in the tokenizer with one that better handles discrepancies.\n",
    "    See docstring for the function for details. \n",
    "- `label_content_roles`: Takes an instruct-formatted text, then assigns each token to the correct roles.\n",
    "\"\"\"\n",
    "import importlib\n",
    "import utils.role_assignments\n",
    "import utils.role_templates\n",
    "importlib.reload(utils.role_assignments)\n",
    "label_content_roles = utils.role_assignments.label_content_roles\n",
    "importlib.reload(utils.role_templates)\n",
    "load_chat_template = utils.role_templates.load_chat_template\n",
    "\n",
    "# Load custom chat templater\n",
    "old_chat_template = tokenizer.chat_template\n",
    "new_chat_template = load_chat_template(f'{ws}/utils/chat_templates', model_architecture)\n",
    "\n",
    "def test_chat_template(tokenizer):\n",
    "    s = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {'role': 'system', 'content': 'Test.'},\n",
    "            {'role': 'user', 'content': 'Hi! I\\'m a dog.'},\n",
    "            {'role': 'assistant', 'content': '<think>The user is a dog!</think>Congrats!'},\n",
    "            {'role': 'user', 'content': 'Thanks!'},\n",
    "            {'role': 'assistant', 'content': '<think>Hmm, the user said thanks!</think>Anything else I can help with?'}\n",
    "        ],\n",
    "        tokenize = False,\n",
    "        padding = 'max_length',\n",
    "        truncation = True,\n",
    "        max_length = 512,\n",
    "        add_generation_prompt = False\n",
    "    )\n",
    "    print(s)\n",
    "    return s\n",
    "\n",
    "tokenizer.chat_template = old_chat_template\n",
    "s = test_chat_template(tokenizer)\n",
    "tokenizer.chat_template = new_chat_template\n",
    "s = test_chat_template(tokenizer)\n",
    "\n",
    "z =\\\n",
    "    pd.DataFrame({'input_ids': tokenizer(s)['input_ids']})\\\n",
    "    .assign(token = lambda df: tokenizer.convert_ids_to_tokens(df['input_ids']))\\\n",
    "    .assign(prompt_ix = 0, batch_ix = 0, sequence_ix = 0, token_ix = lambda df: df.index)\n",
    "\n",
    "label_content_roles(model_architecture, z).tail(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6efc781",
   "metadata": {},
   "source": [
    "## Create instruction test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d30b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We'll test some projections of different sequences\n",
    "\n",
    "To understand instruct formats for different models, see the HF chat template playground:\n",
    "    - https://huggingface.co/spaces/huggingfacejs/chat-template-playground?modelId=openai%2Fgpt-oss-120b\n",
    "\"\"\"\n",
    "from utils.dataset import ReconstructableTextDataset, stack_collate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_messages(yaml_path, model_key):\n",
    "    \"\"\"Loads system-like, assistant-like, user-like, etc. messages\"\"\"\n",
    "    with open(yaml_path, 'r', encoding = 'utf-8') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    return [x['content'] for x in data[model_key]]\n",
    "\n",
    "base_message_types = ['system', 'user', 'cot', 'assistant-final', 'user', 'cot', 'assistant-final']\n",
    "\n",
    "base_messages = load_messages(f\"{ws}/experiments/da-role-analysis/prompts/standard-conversations.yaml\", model_prefix)\n",
    "\n",
    "prompts = {}\n",
    "\n",
    "prompts['basic_no_format'] = '\\n'.join(base_messages)\n",
    "\n",
    "prompts['everything_in_user_tags'] = tokenizer.apply_chat_template(\n",
    "    [{'role': 'user', 'content': '\\n'.join(base_messages)}],\n",
    "    tokenize = False, add_generation_prompt = False\n",
    ")\n",
    "\n",
    "if model_architecture == 'gptoss':\n",
    "    prompts['everything_in_assistant_tags'] = tokenizer.apply_chat_template(\n",
    "        [{'role': 'assistant', 'content': '\\n'.join(base_messages)}],\n",
    "        tokenize = False, add_generation_prompt = False\n",
    "    )\n",
    "elif model_architecture == 'qwen3moe':\n",
    "    prompts['everything_in_assistant_tags'] = tokenizer.apply_chat_template(\n",
    "        [{'role': 'assistant', 'content': f\"<think></think>{'\\n'.join(base_messages)}\"}],\n",
    "        tokenize = False, add_generation_prompt = False\n",
    "    )\n",
    "else:\n",
    "    raise Exception('Unsupported architecture!')\n",
    "\n",
    "prompts['proper_tags'] = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {'role': 'system', 'content': base_messages[0]},\n",
    "        {'role': 'user', 'content': base_messages[1]},\n",
    "        {'role': 'assistant', 'content': f\"<think>{base_messages[2]}</think>{base_messages[3]}\"},\n",
    "        {'role': 'user', 'content': base_messages[4]},\n",
    "        {'role': 'assistant', 'content': f\"<think>{base_messages[5]}</think>{base_messages[6]}\"}\n",
    "    ],\n",
    "    tokenize = False, add_generation_prompt = False\n",
    ")\n",
    "\n",
    "print(prompts['proper_tags'])\n",
    "\n",
    "test_input_df =\\\n",
    "    pd.DataFrame({\n",
    "        'prompt': [p for _, p in prompts.items()],\n",
    "        'prompt_key': [pk for pk, _ in prompts.items()]\n",
    "    })\\\n",
    "    .assign(prompt_ix = lambda df: list(range(0, len(df))))\n",
    "\n",
    "# Create and chunk into lists of size 500 each - these will be the export breaks\n",
    "test_dl = DataLoader(\n",
    "    ReconstructableTextDataset(test_input_df['prompt'].tolist(), tokenizer, max_length = 512, prompt_ix = test_input_df['prompt_ix'].tolist()),\n",
    "    batch_size = 16,\n",
    "    shuffle = False,\n",
    "    collate_fn = stack_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40df7af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenizer.apply_chat_template([\n",
    "#     {\n",
    "#         'role': 'assistant',\n",
    "#         'thinking': 'The user is asking about the content in Austin',\n",
    "#         'content': \"<think>\\nThe user is asking about the weather in Austin. I should use the weather tool to get this information.\\n</think>\\nI'll check the current weather in New York for you.\",\n",
    "#     }\n",
    "# ], tokenize = False, add_generation_prompt = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4129f68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompts['proper_tags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049406c0",
   "metadata": {},
   "source": [
    "## Collect test activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4cabfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run forward passes\n",
    "\"\"\"\n",
    "model_module = importlib.import_module(f\"utils.pretrained_models.{model_architecture}\")\n",
    "run_model_return_topk = getattr(model_module, f\"run_{model_architecture}_return_topk\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_and_export_states(model, dl: ReconstructableTextDataset, layers_to_keep_acts: list[int]):\n",
    "    \"\"\"\n",
    "    Run forward passes on given model and store the decomposed sample_df plus hidden states\n",
    "\n",
    "    Params:\n",
    "        @model: The model to run forward passes on. Should return a dict with keys `logits`, `all_topk_experts`, `all_topk_weights`,\n",
    "          `all_pre_mlp_hidden_states`, and `all_expert_outputs`.\n",
    "        @dl: A ReconstructableTextDataset of which returns `input_ids`, `attention_mask`, and `original_tokens`.\n",
    "        @layers_to_keep_acts: A list of layer indices (0-indexed) for which to filter `all_pre_mlp_hidden_states` (see returned object description).\n",
    "\n",
    "    Returns:\n",
    "        A dict with keys:\n",
    "        - `sample_df`: A sample (token)-level dataframe with corresponding input token ID, output token ID, and input token text (removes masked tokens)\n",
    "        - `all_pre_mlp_hidden_states`: A tensor of size n_samples x layers_to_keep_acts x D return the hidden state for each retained layers. Each \n",
    "            n_sample corresponds to a row of sample_df.\n",
    "    \"\"\"\n",
    "    all_pre_mlp_hidden_states = []\n",
    "    sample_dfs = []\n",
    "\n",
    "    for batch_ix, batch in tqdm(enumerate(dl), total = len(dl)):\n",
    "\n",
    "        input_ids = batch['input_ids'].to(main_device)\n",
    "        attention_mask = batch['attention_mask'].to(main_device)\n",
    "        original_tokens = batch['original_tokens']\n",
    "        prompt_indices = batch['prompt_ix']\n",
    "\n",
    "        output = run_model_return_topk(model, input_ids, attention_mask, return_hidden_states = True)\n",
    "\n",
    "        # Check no bugs by validating output/perplexity\n",
    "        if batch_ix == 0:\n",
    "            loss = ForCausalLMLoss(output['logits'], torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids), model.config.vocab_size).detach().cpu().item()\n",
    "            for i in range(min(20, input_ids.size(0))):\n",
    "                decoded_input = tokenizer.decode(input_ids[i, :], skip_special_tokens = False)\n",
    "                next_token_id = torch.argmax(output['logits'][i, -1, :]).item()\n",
    "                print('---------\\n' + decoded_input + colored(tokenizer.decode([next_token_id], skip_special_tokens = False).replace('\\n', '<lb>'), 'green'))\n",
    "            print(f\"PPL:\", torch.exp(torch.tensor(loss)).item())\n",
    "                \n",
    "        original_tokens_df = pd.DataFrame(\n",
    "            [(seq_i, tok_i, tok) for seq_i, tokens in enumerate(original_tokens) for tok_i, tok in enumerate(tokens)], \n",
    "            columns = ['sequence_ix', 'token_ix', 'token']\n",
    "        )\n",
    "                \n",
    "        prompt_indices_df = pd.DataFrame(\n",
    "            [(seq_i, seq_source) for seq_i, seq_source in enumerate(prompt_indices)], \n",
    "            columns = ['sequence_ix', 'prompt_ix']\n",
    "        )\n",
    "        \n",
    "        # Create sample (token) level dataframe\n",
    "        sample_df =\\\n",
    "            convert_outputs_to_df_fast(input_ids, attention_mask, output['logits'])\\\n",
    "            .merge(original_tokens_df, how = 'left', on = ['token_ix', 'sequence_ix'])\\\n",
    "            .merge(prompt_indices_df, how = 'left', on = ['sequence_ix'])\\\n",
    "            .assign(batch_ix = batch_ix)\n",
    "        \n",
    "        sample_dfs.append(sample_df)\n",
    "\n",
    "        # Store pre-MLP hidden states - the fwd pass as n_layers list as BN x D, collapse to BN x n_layers x D, with BN filtering out masked items\n",
    "        valid_pos = torch.where(attention_mask.cpu().view(-1) == 1) # Valid (BN, ) positions\n",
    "        all_pre_mlp_hidden_states.append(torch.stack(output['all_pre_mlp_hidden_states'], dim = 1)[valid_pos][:, layers_to_keep_acts, :])\n",
    "\n",
    "        batch_ix += 1\n",
    "\n",
    "    sample_df = pd.concat(sample_dfs, ignore_index = True)\n",
    "    all_pre_mlp_hidden_states = torch.cat(all_pre_mlp_hidden_states, dim = 0)\n",
    "\n",
    "    return {\n",
    "        'sample_df': sample_df,\n",
    "        'all_pre_mlp_hs': all_pre_mlp_hidden_states\n",
    "    }\n",
    "\n",
    "test_outputs = run_and_export_states(\n",
    "    model,\n",
    "    test_dl,\n",
    "    layers_to_keep_acts = list(range(model_n_layers))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56da9f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare sample df\n",
    "\"\"\"\n",
    "test_sample_df =\\\n",
    "    test_outputs['sample_df'].merge(test_input_df, on = 'prompt_ix', how = 'inner')\\\n",
    "    .assign(sample_ix = lambda df: df.groupby(['batch_ix', 'sequence_ix', 'token_ix']).ngroup())\\\n",
    "    .assign(token_in_prompt_ix = lambda df: df.groupby(['prompt_ix']).cumcount())\n",
    "\n",
    "display(test_sample_df)\n",
    "\n",
    "test_pre_mlp_hs = test_outputs['all_pre_mlp_hs'].to(torch.float16)\n",
    "test_pre_mlp_hs = {layer_ix: test_pre_mlp_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(list(range(model_n_layers)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1e11d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Map each token base to its base_message (currently its mapped to its prompt key, but it should also be base-message mapped!)\n",
    "\"\"\"\n",
    "test_sample_df_labeled =\\\n",
    "    test_sample_df\\\n",
    "    .sort_values(['prompt_ix', 'token_ix'])\\\n",
    "    .assign(\n",
    "        _t1 = lambda d: d.groupby('prompt_ix')['token'].shift(-1),\n",
    "        _t2 = lambda d: d.groupby('prompt_ix')['token'].shift(-2),\n",
    "        _t3 = lambda d: d.groupby('prompt_ix')['token'].shift(-3),\n",
    "        _t4 = lambda d: d.groupby('prompt_ix')['token'].shift(-4),\n",
    "        _t5 = lambda d: d.groupby('prompt_ix')['token'].shift(-5),\n",
    "        _t6 = lambda d: d.groupby('prompt_ix')['token'].shift(-6),\n",
    "        _t7 = lambda d: d.groupby('prompt_ix')['token'].shift(-7),\n",
    "        _t8 = lambda d: d.groupby('prompt_ix')['token'].shift(-8),\n",
    "        _b1 = lambda d: d.groupby('prompt_ix')['token'].shift(1),\n",
    "        _b2 = lambda d: d.groupby('prompt_ix')['token'].shift(2),\n",
    "        _b3 = lambda d: d.groupby('prompt_ix')['token'].shift(3),\n",
    "        _b4 = lambda d: d.groupby('prompt_ix')['token'].shift(4),\n",
    "        _b5 = lambda d: d.groupby('prompt_ix')['token'].shift(5),\n",
    "        _b6 = lambda d: d.groupby('prompt_ix')['token'].shift(6),\n",
    "        _b7 = lambda d: d.groupby('prompt_ix')['token'].shift(7),\n",
    "        _b8 = lambda d: d.groupby('prompt_ix')['token'].shift(8)\n",
    "    )\\\n",
    "    .assign(\n",
    "        has_roll = lambda d: d[['_t1','_t2','_t3','_t4','_t5', '_t6', '_t7', '_t8']].notna().all(axis = 1),\n",
    "        has_back = lambda d: d[['_b1','_b2','_b3','_b4','_b5', '_b6', '_b7', '_b8']].notna().all(axis = 1),  \n",
    "    )\\\n",
    "    .assign(\n",
    "        tok_roll = lambda d: d['token'].fillna('') + d['_t1'].fillna('') + d['_t2'].fillna('') + d['_t3'].fillna('') + d['_t4'].fillna('') +\n",
    "            d['_t5'].fillna('')  + d['_t6'].fillna('')  + d['_t7'].fillna('')  + d['_t8'].fillna(''),\n",
    "        tok_back = lambda d: d['_b8'].fillna('') + d['_b7'].fillna('') + d['_b6'].fillna('') + d['_b5'].fillna('') + d['_b4'].fillna('') +\n",
    "            d['_b3'].fillna('') + d['_b2'].fillna('') + d['_b1'].fillna('') + d['token'].fillna('')\n",
    "    )\\\n",
    "    .drop(columns=['_t1','_t2','_t3','_t4', '_t5','_t6', '_t7', '_t8', '_b1','_b2','_b3','_b4','_b5', '_b6', '_b7', '_b8'])\\\n",
    "    .pipe(lambda d: d.join(\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    (d['has_roll'] & d['tok_roll'].apply(lambda s, t=t: s in t)) |\n",
    "                    (d['has_back'] & d['tok_back'].apply(lambda s, t=t: s in t))\n",
    "                    # d['tok_roll'].apply(lambda s, t=t: bool(s) and (s in t)) |\n",
    "                    # d['tok_back'].apply(lambda s, t=t: bool(s) and (s in t))\n",
    "                ).rename(f'hit_p{i}')\n",
    "                for i, t in enumerate(base_messages)\n",
    "            ],\n",
    "            axis = 1\n",
    "        )\n",
    "    ))\\\n",
    "    .assign(\n",
    "        base_message_ix = lambda d: np.select(\n",
    "            [d[f'hit_p{i}'] for i in range(len(base_messages))],\n",
    "            list(range(len(base_messages))),\n",
    "            default = None\n",
    "        ),\n",
    "        ambiguous = lambda d: d[[f'hit_p{i}' for i in range(len(base_messages))]].sum(axis=1) > 1,\n",
    "        # base_name = lambda d: d['base_ix'].map({i: f\"p{i}\" for i in range(len(base_messages))})\n",
    "    )\\\n",
    "    .drop(columns = [f'hit_p{i}' for i in range(len(base_messages))])\\\n",
    "    .drop(columns = ['tok_roll', 'tok_back', 'ambiguous', 'batch_ix', 'sequence_ix', 'token_ix'])\\\n",
    "    [[\n",
    "        'sample_ix',\n",
    "        'prompt_ix', 'prompt_key', 'prompt',\n",
    "        'token_in_prompt_ix',\n",
    "        'base_message_ix',\n",
    "        'token_id',\n",
    "        'token',\n",
    "        'output_id',\n",
    "        'output_prob' ,\n",
    "        # 'tok_roll',\n",
    "        # 'tok_back',\n",
    "        # 'ambiguous'\n",
    "    ]]\n",
    "\n",
    "display(test_sample_df_labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b35b9aa",
   "metadata": {},
   "source": [
    "## Project tests into rolespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07198ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now project these test samples into rolespace, then flatten - result should be len(test_sample_df_labeled) * # roles\n",
    "\"\"\"\n",
    "test_layer = probe_layers[int(np.ceil(0.5 * len(probe_layers)).item()) - 1]\n",
    "test_roles = ['system', 'user', 'assistant-cot', 'assistant-final'] # system, user, assistant-cot, assistant-final, tool\n",
    "probe = [x for x in probes if sorted(x['roles']) == sorted(test_roles) and x['layer_ix'] == test_layer][0]\n",
    "\n",
    "project_test_sample_df = test_sample_df_labeled\n",
    "project_hs_cp = cupy.asarray(test_pre_mlp_hs[test_layer][project_test_sample_df['sample_ix'].tolist(), :])\n",
    "project_probs = probe['probe'].predict_proba(project_hs_cp).round(6)\n",
    "\n",
    "# Merge seq probs withto get sampel_ix\n",
    "proj_results = pd.DataFrame(cupy.asnumpy(project_probs), columns = probe['roles']).clip(1e-6)\n",
    "if len(proj_results) != len(project_test_sample_df):\n",
    "    raise Exception(\"Error!\")\n",
    "\n",
    "role_level_df = pd.concat([proj_results, project_test_sample_df[['sample_ix']]], axis = 1)\n",
    "\n",
    "role_level_df =\\\n",
    "    role_level_df.melt(id_vars = ['sample_ix'], var_name = 'role_space', value_name = 'prob')\\\n",
    "    .merge(\n",
    "        project_test_sample_df[['sample_ix', 'prompt_ix', 'prompt_key', 'prompt', 'token_in_prompt_ix', 'base_message_ix', 'token']],\n",
    "        on = 'sample_ix',\n",
    "        how = 'inner'\n",
    "    )\\\n",
    "    .merge(\n",
    "        pd.DataFrame({'base_message_ix': list(range(len(base_message_types))), 'base_message_type': base_message_types}),\n",
    "        on = 'base_message_ix',\n",
    "        how = 'left'\n",
    "    )\\\n",
    "    .assign(base_message_type = lambda df: df['base_message_type'].fillna('other'))\\\n",
    "    .pipe(lambda df: df[df['base_message_type'] != 'other']) # Cull the role tags themselves\n",
    "\n",
    "role_level_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b81519",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write for plotting\n",
    "\"\"\"\n",
    "role_level_df.to_csv(f\"{ws}/experiments/da-role-analysis/projections/test-role-projections-{model_prefix}.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cfffc6",
   "metadata": {},
   "source": [
    "## Plot and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3da95ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot tests\n",
    "\"\"\"\n",
    "facet_order = probe['roles']\n",
    "all_message_types = role_level_df['base_message_type'].unique()\n",
    "color_map = {msg_type: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] for i, msg_type in enumerate(all_message_types)}\n",
    "\n",
    "smooth_df =\\\n",
    "    role_level_df\\\n",
    "    .sort_values('sample_ix')\\\n",
    "    .assign(\n",
    "        prob_sma = lambda df: df.groupby(['prompt_ix', 'role_space'])['prob'].rolling(window = 2, min_periods = 1).mean().reset_index(level = [0, 1], drop = True),\n",
    "        prob_ewma = lambda df: df.groupby(['prompt_ix', 'role_space'])['prob'].ewm(alpha = .9, min_periods = 1).mean().reset_index(level = [0, 1], drop = True)\n",
    "    )\n",
    "\n",
    "display(smooth_df)\n",
    "\n",
    "for this_prompt_key in smooth_df['prompt_key'].unique().tolist():\n",
    "    this_df = smooth_df.pipe(lambda df: df[df['prompt_key'] == this_prompt_key])\n",
    "    fig = px.scatter(\n",
    "        this_df, x = 'token_in_prompt_ix', y = 'prob',\n",
    "        facet_row = 'role_space',\n",
    "        color = 'base_message_type',\n",
    "        color_discrete_map = color_map,\n",
    "        category_orders = {\n",
    "            'role_space': facet_order,\n",
    "            'base_message_type': ['system', 'user', 'cot', 'assistant-final', 'other']\n",
    "        },\n",
    "        hover_name = 'token',\n",
    "        hover_data = {\n",
    "            'prob': ':.3f'\n",
    "        },\n",
    "        # markers = True,\n",
    "        title = f'prompt = {this_prompt_key}',\n",
    "        labels = {\n",
    "            'token_in_prompt_ix': 'Token Index',\n",
    "            'prob': 'Prob',\n",
    "            'prob_smoothed': 'Smoothed Prob',\n",
    "            'role_space': 'role'\n",
    "        }\n",
    "    ).update_yaxes(\n",
    "        range = [0, 1],\n",
    "        side = 'left'\n",
    "    ).update_layout(height = 500, width = 800)\n",
    "\n",
    "    def pretty(a):\n",
    "        a.update(\n",
    "            text = a.text.split(\"=\")[-1],\n",
    "            x = 0.5, xanchor = \"center\",\n",
    "            y = a.y + 0.08,\n",
    "            textangle = 0,\n",
    "            font = dict(size = 12),\n",
    "            bgcolor = 'rgba(255, 255, 255, 0.9)',  # light strip look\n",
    "            showarrow = False\n",
    "        )\n",
    "\n",
    "    fig.for_each_annotation(pretty)\n",
    "    fig.update_yaxes(title_text = None)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ece4491",
   "metadata": {},
   "source": [
    "# Project redteaming activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c12ae0c",
   "metadata": {},
   "source": [
    "## Load data + label actual vs style roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12e44e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset of sample (token) level data and activations\n",
    "\"\"\"\n",
    "def load_data(folder_path, model_prefix, max_data_files):\n",
    "    \"\"\"\n",
    "    Load data saved by `export-c4-activations.ipynb` - you can reduce max_data_files if not enough memory\n",
    "    \"\"\"\n",
    "    folders = [f'{ws}/experiments/da-role-analysis/{folder_path}/{model_prefix}/{i:02d}' for i in range(max_data_files)]\n",
    "    folders = [f for f in folders if os.path.isdir(f)]\n",
    "\n",
    "    all_pre_mlp_hs = []\n",
    "    sample_df = []\n",
    "    topk_df = []\n",
    "\n",
    "    for f in tqdm(folders):\n",
    "        sample_df.append(pd.read_pickle(f'{f}/samples.pkl'))\n",
    "        topk_df.append(pd.read_pickle(f'{f}/topks.pkl'))\n",
    "        all_pre_mlp_hs.append(torch.load(f'{f}/all-pre-mlp-hidden-states.pt'))\n",
    "\n",
    "    sample_df = pd.concat(sample_df)\n",
    "    topk_df = pd.concat(topk_df)\n",
    "    all_pre_mlp_hs = torch.concat(all_pre_mlp_hs)\n",
    "\n",
    "    with open(f'{ws}/experiments/da-role-analysis/{folder_path}/{model_prefix}/metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    gc.collect()\n",
    "    return sample_df, topk_df, all_pre_mlp_hs, metadata['all_pre_mlp_hidden_states_layers']\n",
    "\n",
    "sample_df_import, _, all_pre_mlp_hs_import, act_map = load_data('activations-redteam', model_prefix, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fdb247",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load prompt-level dataset\n",
    "\"\"\"\n",
    "prompts_df = pd.read_csv(f\"{ws}/experiments/da-role-analysis/activations-redteam/{model_prefix}/base-harmful-responses-classified.csv\")\n",
    "prompts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef78c45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Take the token-level dataframe (sample_df) and assign for each token:\n",
    "- `role`: the actual role the tags specify (`label_content_roles`)\n",
    "- `base_message_type`: the role style; equal to role except for the CoT forgery tokens, which is equal to \"forged_cot\"\n",
    "\"\"\"\n",
    "# Assign real content roles (col roles + seg_id indicating continuous role-segment within prompt)\n",
    "sample_df_raw =\\\n",
    "    label_content_roles(model_architecture, sample_df_import.rename(columns = {'redteam_prompt_ix': 'prompt_ix'}))\\\n",
    "    .drop(columns = ['batch_ix', 'sequence_ix'])\\\n",
    "    .rename(columns = {'prompt_ix': 'redteam_prompt_ix'})\\\n",
    "    .assign(sample_ix = lambda df: df.groupby(['redteam_prompt_ix', 'token_ix']).ngroup())\\\n",
    "    .assign(token_ix = lambda df: df.groupby(['redteam_prompt_ix']).cumcount())\\\n",
    "    .assign(\n",
    "        _noncontent = lambda d: (~d['in_content_span']).astype('int8'),\n",
    "        token_in_seg_ix = lambda d: d.groupby(['redteam_prompt_ix','seg_id'])['_noncontent'].cumsum().sub(1)\n",
    "    )\\\n",
    "    .drop(columns = ['_noncontent'])\n",
    "\n",
    "# Assign user tokens to CoT forgery when policy_style != no_policy\n",
    "sample_df = (\n",
    "    sample_df_raw\\\n",
    "    .merge(prompts_df[['redteam_prompt_ix', 'policy_style']], how = 'inner', on = 'redteam_prompt_ix')\\\n",
    "    .sort_values(['redteam_prompt_ix', 'token_ix'])\\\n",
    "\n",
    "    # user content + newline helpers\n",
    "    .assign(\n",
    "        is_user_content = lambda d: d['role'].eq('user') & d['in_content_span'],\n",
    "        is_nl_only = lambda d: d['token'].str.fullmatch(r'\\n+', na = False),\n",
    "        prev_tok = lambda d: d.groupby(['redteam_prompt_ix','seg_id'])['token'].shift(1),\n",
    "        prev_is_nl_only = lambda d: d.groupby(['redteam_prompt_ix','seg_id'])['is_nl_only'].shift(1).astype('boolean').fillna(False),\n",
    "    )\n",
    "\n",
    "    # where a double linebreak occurs (within each user segment)\n",
    "    .assign(dbl_here = lambda d:\n",
    "        d['token'].str.contains('\\n\\n', regex = False, na = False) | # contains \"\\n\\n\"\n",
    "        (d['prev_is_nl_only'] & d['is_nl_only']) | # two newline-only tokens\n",
    "        (d['prev_tok'].str.endswith('\\n', na = False) & d['token'].str.startswith('\\n', na = False))  # across boundary\n",
    "    )\n",
    "\n",
    "    # tokens strictly AFTER the last double break in each (redteam_prompt_ix, seg_id)\n",
    "    .assign(\n",
    "        rev_breaks = lambda d: d.groupby(['redteam_prompt_ix', 'seg_id'])\\\n",
    "            ['dbl_here'].transform(lambda s: s.iloc[::-1].cumsum().iloc[::-1]),\n",
    "        after_last_para = lambda d: d['is_user_content'] & d['rev_breaks'].eq(0),\n",
    "        is_user_visible = lambda d: d['is_user_content'] & ~d['is_nl_only'],\n",
    "    )\n",
    "\n",
    "    # final label: gate by policy_style\n",
    "    .assign(base_message_type = lambda d: np.where(\n",
    "        (d['policy_style'] != 'no_policy') & d['is_user_visible'] & d['after_last_para'], 'forged_cot', d['role']\n",
    "    ))\n",
    "\n",
    "    .drop(columns=[\n",
    "        'is_user_content', 'is_nl_only', 'prev_tok', 'prev_is_nl_only',\n",
    "        'dbl_here', 'rev_breaks', 'after_last_para', 'is_user_visible', 'policy_style'\n",
    "    ], errors = 'ignore')\n",
    ")\n",
    "\n",
    "del sample_df_import, sample_df_raw\n",
    "\n",
    "gc.collect()\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131b6264",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert activations to fp16 (for compatibility with cupy later) + layer-wise dict\n",
    "\"\"\"\n",
    "all_pre_mlp_hs = all_pre_mlp_hs_import.to(torch.float16)\n",
    "# compare_bf16_fp16_batched(all_pre_mlp_hs_import, all_pre_mlp_hs)\n",
    "del all_pre_mlp_hs_import\n",
    "all_pre_mlp_hs = {layer_ix: all_pre_mlp_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(act_map)}\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28d6726",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Count distibution of base_message_type tokens\n",
    "\"\"\"\n",
    "# sample_df.head(1000).to_csv(f'{ws}/test.csv', index = False)\n",
    "\n",
    "sample_df\\\n",
    "    .merge(prompts_df[['redteam_prompt_ix', 'policy_style', 'qualifier_type', 'output_class']], how = 'inner', on = 'redteam_prompt_ix')\\\n",
    "    .groupby(['policy_style', 'qualifier_type', 'base_message_type'], as_index = False)\\\n",
    "    .agg(n = ('redteam_prompt_ix', 'count'))\\\n",
    "    .pipe(lambda df: df.assign(pct = df['n'] / df.groupby(['policy_style', 'qualifier_type'])['n'].transform('sum')))\\\n",
    "    .pipe(lambda df: df.pivot_table(\n",
    "        index = ['policy_style', 'qualifier_type'],\n",
    "        columns = 'base_message_type',\n",
    "        values = 'pct',\n",
    "        fill_value = 0\n",
    "    ))\\\n",
    "    .reset_index(drop = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb2aa4",
   "metadata": {},
   "source": [
    "## Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6fc799",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Project into activation space\n",
    "\"\"\"\n",
    "test_layer = probe_layers[int(np.ceil(0.5 * len(probe_layers)).item())]\n",
    "test_roles = ['system', 'user', 'assistant-cot', 'assistant-final'] # system, user, assistant-cot, assistant-final, tool\n",
    "probe = [x for x in probes if sorted(x['roles']) == sorted(test_roles) and x['layer_ix'] == test_layer][0]\n",
    "\n",
    "project_test_sample_df = sample_df\n",
    "project_hs_cp = cupy.asarray(all_pre_mlp_hs[test_layer][project_test_sample_df['sample_ix'].tolist(), :])\n",
    "project_probs = probe['probe'].predict_proba(project_hs_cp).round(6)\n",
    "\n",
    "# Merge seq probs withto get sampel_ix\n",
    "proj_results = pd.DataFrame(cupy.asnumpy(project_probs), columns = probe['roles']).clip(1e-6)\n",
    "if len(proj_results) != len(project_test_sample_df):\n",
    "    raise Exception('Error!')\n",
    "\n",
    "role_projection_df = pd.concat([proj_results, project_test_sample_df[['sample_ix']]], axis = 1)\n",
    "\n",
    "# Merge with token-level metadata and cull missing-role tokens (equivalent to culling instruct-special toks only)\n",
    "# Verify equality:\n",
    "# - sample_df.assign(role = lambda df: df['role'].fillna('o')).groupby(['in_content_span', 'role'], as_index = False).agg(n=('sample_ix', 'count'))\n",
    "role_projection_df =\\\n",
    "    role_projection_df\\\n",
    "    .melt(id_vars = ['sample_ix'], var_name = 'role_space', value_name = 'prob')\\\n",
    "    .merge(project_test_sample_df, on = 'sample_ix', how = 'inner')\\\n",
    "    .assign(\n",
    "        role = lambda df: df['role'].fillna('other'),\n",
    "        base_message_type = lambda df: df['base_message_type'].fillna('other')\n",
    "    )\\\n",
    "    .pipe(lambda df: df[df['role'] != 'other']) # Cull the role tags themselves\n",
    "\n",
    "role_projection_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cc9cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Project with probes\n",
    "\n",
    "The final projected dataset is at a (token-sample, role_space, layer) level. The columns include:      \n",
    "- sample_ix: the token-sample index      \n",
    "- role_space: the role space projected to (systemness, cotness, userness, assistantness)      \n",
    "- layer_ix: the target layer      \n",
    "- prob: the probability predicted by the probe to that token-sample at that layer_ix's activations for the role_space role      \n",
    "- prompt_ix: the prompt index of the prompt the token-sample belongs to      \n",
    "- output_class: the classification of the assistant output from this prompt, either HARMFUL or REFUSAL      \n",
    "- role: the actual role tags this token-sample belongs to (system, user, cot, assistant, or \"other\" if and only if its a harmony tag instead of the text inside the tag)      \n",
    "- base_message_type: equal to the actual role EXCEPT for the cot forgery tokens of cot forged prompts, which are set to \"forged_cot\"      \n",
    "- policy_style: whether the prompt was a base harmful question (no_policy), a styled cot forgery (base), or a destyled forgery (destyled)      \n",
    "- qualifier_type: the type of qualifier used, either no_qualifier or green_shirt/lucky_coin/etc\n",
    "\"\"\"\n",
    "role_projection_dfs = []\n",
    "probe_mapping_dfs = []\n",
    "\n",
    "for probe_ix, probe in tqdm(enumerate(probes)):\n",
    "\n",
    "    test_layer = probe['layer_ix']\n",
    "    test_roles = probe['roles']\n",
    "    \n",
    "    project_test_sample_df = sample_df\n",
    "    project_hs_cp = cupy.asarray(all_pre_mlp_hs[test_layer][project_test_sample_df['sample_ix'].tolist(), :])\n",
    "    project_probs = probe['probe'].predict_proba(project_hs_cp).round(6)\n",
    "\n",
    "    # Merge seq probs withto get sampel_ix\n",
    "    proj_results = pd.DataFrame(cupy.asnumpy(project_probs), columns = probe['roles']).clip(1e-6)\n",
    "    if len(proj_results) != len(project_test_sample_df):\n",
    "        raise Exception('Error!')\n",
    "\n",
    "    role_projection_df = pd.concat([proj_results, project_test_sample_df[['sample_ix']]], axis = 1)\n",
    "\n",
    "    # Merge with token-level metadata and cull missing-role tokens (equivalent to culling instruct-special toks only)\n",
    "    # Verify equality:\n",
    "    # - sample_df.assign(role = lambda df: df['role'].fillna('o')).groupby(['in_content_span', 'role'], as_index = False).agg(n=('sample_ix', 'count'))\n",
    "    role_projection_df =\\\n",
    "        role_projection_df\\\n",
    "        .melt(id_vars = ['sample_ix'], var_name = 'role_space', value_name = 'prob')\\\n",
    "        .merge(project_test_sample_df, on = 'sample_ix', how = 'inner')\\\n",
    "        .assign(\n",
    "            role = lambda df: df['role'].fillna('other'),\n",
    "            base_message_type = lambda df: df['base_message_type'].fillna('other'),\n",
    "            probe_ix = probe_ix            \n",
    "        )\\\n",
    "        .pipe(lambda df: df[df['role'] != 'other']) # Cull the role tags themselves\n",
    "\n",
    "    probe_mapping_dfs.append(pd.DataFrame({\n",
    "        'probe_ix': [probe_ix],\n",
    "        'layer_ix': [probe['layer_ix']],\n",
    "        'roles': [','.join(sorted(probe['roles']))]\n",
    "    }))\n",
    "    role_projection_dfs.append(role_projection_df)\n",
    "\n",
    "role_projection_df = pd.concat(role_projection_dfs, ignore_index = True)\n",
    "probe_mapping_df = pd.concat(probe_mapping_dfs, ignore_index = True)\n",
    "role_projection_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5232c4",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5527c082",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save for analysis\n",
    "\"\"\"\n",
    "role_projection_df\\\n",
    "    .to_feather(f'{ws}/experiments/da-role-analysis/projections/redteam-role-projections-{model_prefix}.feather')\n",
    "\n",
    "probe_mapping_df\\\n",
    "    .to_csv(f'{ws}/experiments/da-role-analysis/projections/redteam-role-probe-mapping-{model_prefix}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091b7751",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checks\n",
    "\"\"\"\n",
    "# Merge with prompt -level\n",
    "role_projection_df =\\\n",
    "    role_projection_df\\\n",
    "    .merge(\n",
    "        prompts_df[['redteam_prompt_ix', 'policy_style', 'qualifier_type', 'output_class']],\n",
    "        on = 'redteam_prompt_ix',\n",
    "        how = 'inner'\n",
    "    )\n",
    "\n",
    "role_projection_df\n",
    "role_projection_df\\\n",
    "    .pipe(lambda df: df[df['role'] == 'user'])\\\n",
    "    .pipe(lambda df: df[df['base_message_type'] == 'forged_cot'])\\\n",
    "    .groupby(['', 'output_class'], as_index = False)\\\n",
    "\n",
    "    .agg(s = ('sample_ix', 'count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506d382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "role_projection_df\\\n",
    "    .pipe(lambda df: df[df['role'] == 'user'])\\\n",
    "    .groupby(['base_message_type'], as_index = False)\\\n",
    "    .agg(s = ('sample_ix', 'count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470bb913",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90146514",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_test_sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22c5c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Role projection df\n",
    "\"\"\"\n",
    "role_projection_df\\\n",
    "    .groupby([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae2f4c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

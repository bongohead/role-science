{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beade33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create role probes and tests them\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb75c7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.loss.loss_utils import ForCausalLMLoss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cupy\n",
    "import cuml\n",
    "import importlib\n",
    "import gc\n",
    "import yaml\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "from termcolor import colored\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.loader import load_model_and_tokenizer\n",
    "from utils.role_assignments import label_content_roles\n",
    "from utils.role_templates import load_chat_template\n",
    "from utils.store_outputs import convert_outputs_to_df_fast\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "\n",
    "clear_all_cuda_memory()\n",
    "check_memory()\n",
    "\n",
    "ws = '/workspace/deliberative-alignment-jailbreaks'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed26db3f",
   "metadata": {},
   "source": [
    "# Load models & probes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75666ea5",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80af64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "model_prefix = 'gptoss-20b' # gptoss-20b, gptoss-120b\n",
    "tokenizer, model, model_architecture, model_n_layers = load_model_and_tokenizer(model_prefix, device = main_device)\n",
    "\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bcadc3",
   "metadata": {},
   "source": [
    "## Load probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9782f7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load probes\n",
    "\"\"\"\n",
    "with open(f'{ws}/experiments/role-analysis/probes/{model_prefix}.pkl', 'rb') as f:\n",
    "    probes = pickle.load(f)\n",
    "\n",
    "probe_layers = sorted(set(x['layer_ix'] for x in probes))\n",
    "probe_roles = sorted(set(tuple(x['roles']) for x in probes))\n",
    "\n",
    "print(f\"Num probes: {str(len(probes))}\")\n",
    "print(f\"Probe layers:\\n  {', '.join([str(x) for x in probe_layers])}\")\n",
    "print(f\"Probe roles:\\n {'\\n '.join([str(list(x)) for x in probe_roles])}\")\n",
    "\n",
    "# probes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357ff043",
   "metadata": {},
   "source": [
    "# Run and project test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e881ce67",
   "metadata": {},
   "source": [
    "## Test tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2c5908",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tests two important functions:\n",
    "- `load_chat_template`: Overwrites the existing chat template in the tokenizer with one that better handles discrepancies.\n",
    "    See docstring for the function for details. \n",
    "- `label_content_roles`: Takes an instruct-formatted text, then assigns each token to the correct roles.\n",
    "\"\"\"\n",
    "import importlib\n",
    "import utils.role_assignments\n",
    "import utils.role_templates\n",
    "importlib.reload(utils.role_assignments)\n",
    "label_content_roles = utils.role_assignments.label_content_roles\n",
    "importlib.reload(utils.role_templates)\n",
    "load_chat_template = utils.role_templates.load_chat_template\n",
    "\n",
    "# Load custom chat templater\n",
    "old_chat_template = tokenizer.chat_template\n",
    "new_chat_template = load_chat_template(f'{ws}/utils/chat_templates', model_architecture)\n",
    "\n",
    "def test_chat_template(tokenizer):\n",
    "    s = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {'role': 'system', 'content': 'Test.'},\n",
    "            {'role': 'user', 'content': 'Hi! I\\'m a dog.'},\n",
    "            {'role': 'assistant', 'content': '<think>The user is a dog!</think>Congrats!'},\n",
    "            {'role': 'user', 'content': 'Thanks!'},\n",
    "            {'role': 'assistant', 'content': '<think>Hmm, the user said thanks!</think>Anything else I can help with?'}\n",
    "        ],\n",
    "        tokenize = False,\n",
    "        padding = 'max_length',\n",
    "        truncation = True,\n",
    "        max_length = 512,\n",
    "        add_generation_prompt = False\n",
    "    )\n",
    "    print(s)\n",
    "    return s\n",
    "\n",
    "tokenizer.chat_template = old_chat_template\n",
    "s = test_chat_template(tokenizer)\n",
    "tokenizer.chat_template = new_chat_template\n",
    "s = test_chat_template(tokenizer)\n",
    "\n",
    "z =\\\n",
    "    pd.DataFrame({'input_ids': tokenizer(s)['input_ids']})\\\n",
    "    .assign(token = lambda df: tokenizer.convert_ids_to_tokens(df['input_ids']))\\\n",
    "    .assign(prompt_ix = 0, batch_ix = 0, sequence_ix = 0, token_ix = lambda df: df.index)\n",
    "\n",
    "label_content_roles(model_architecture, z).tail(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ece4491",
   "metadata": {},
   "source": [
    "# Project redteaming activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c12ae0c",
   "metadata": {},
   "source": [
    "## Load data + label actual vs style roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12e44e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset of sample (token) level data and activations\n",
    "\"\"\"\n",
    "def load_data(folder_path, model_prefix, max_data_files):\n",
    "    \"\"\"\n",
    "    Load samples + activations\n",
    "    \"\"\"\n",
    "    folders = [f'{ws}/experiments/role-injection-analysis/{folder_path}/{model_prefix}/{i:02d}' for i in range(max_data_files)]\n",
    "    folders = [f for f in folders if os.path.isdir(f)]\n",
    "\n",
    "    all_pre_mlp_hs = []\n",
    "    sample_df = []\n",
    "    topk_df = []\n",
    "\n",
    "    for f in tqdm(folders):\n",
    "        sample_df.append(pd.read_pickle(f'{f}/samples.pkl'))\n",
    "        topk_df.append(pd.read_pickle(f'{f}/topks.pkl'))\n",
    "        all_pre_mlp_hs.append(torch.load(f'{f}/all-pre-mlp-hidden-states.pt'))\n",
    "\n",
    "    sample_df = pd.concat(sample_df)\n",
    "    topk_df = pd.concat(topk_df)\n",
    "    all_pre_mlp_hs = torch.concat(all_pre_mlp_hs)\n",
    "\n",
    "    with open(f'{ws}/experiments/role-injection-analysis/{folder_path}/{model_prefix}/metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    gc.collect()\n",
    "    return sample_df, topk_df, all_pre_mlp_hs, metadata['all_pre_mlp_hidden_states_layers']\n",
    "\n",
    "sample_df_import, _, all_pre_mlp_hs_import, act_map = load_data('activations-redteam', model_prefix, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fdb247",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load prompt-level dataset\n",
    "\"\"\"\n",
    "prompts_df = pd.read_csv(f\"{ws}/experiments/role-injection-analysis/activations-redteam/{model_prefix}/base-harmful-responses-classified.csv\")\n",
    "prompts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef78c45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Take the token-level dataframe (sample_df) and assign for each token:\n",
    "- `role`: the actual role the tags specify (`label_content_roles`)\n",
    "- `base_message_type`: the role style; equal to role except for the CoT forgery tokens, which is equal to \"forged_cot\"\n",
    "\"\"\"\n",
    "# Assign real content roles (col roles + seg_id indicating continuous role-segment within prompt)\n",
    "sample_df_raw =\\\n",
    "    label_content_roles(model_architecture, sample_df_import.rename(columns = {'redteam_prompt_ix': 'prompt_ix'}))\\\n",
    "    .drop(columns = ['batch_ix', 'sequence_ix'])\\\n",
    "    .rename(columns = {'prompt_ix': 'redteam_prompt_ix'})\\\n",
    "    .assign(sample_ix = lambda df: df.groupby(['redteam_prompt_ix', 'token_ix']).ngroup())\\\n",
    "    .assign(token_ix = lambda df: df.groupby(['redteam_prompt_ix']).cumcount())\\\n",
    "    .assign(\n",
    "        _noncontent = lambda d: (~d['in_content_span']).astype('int8'),\n",
    "        token_in_seg_ix = lambda d: d.groupby(['redteam_prompt_ix','seg_id'])['_noncontent'].cumsum().sub(1)\n",
    "    )\\\n",
    "    .drop(columns = ['_noncontent'])\n",
    "\n",
    "# Assign user tokens to CoT forgery when policy_style != no_policy\n",
    "sample_df = (\n",
    "    sample_df_raw\\\n",
    "    .merge(prompts_df[['redteam_prompt_ix', 'policy_style']], how = 'inner', on = 'redteam_prompt_ix')\\\n",
    "    .sort_values(['redteam_prompt_ix', 'token_ix'])\\\n",
    "\n",
    "    # user content + newline helpers\n",
    "    .assign(\n",
    "        is_user_content = lambda d: d['role'].eq('user') & d['in_content_span'],\n",
    "        is_nl_only = lambda d: d['token'].str.fullmatch(r'\\n+', na = False),\n",
    "        prev_tok = lambda d: d.groupby(['redteam_prompt_ix','seg_id'])['token'].shift(1),\n",
    "        prev_is_nl_only = lambda d: d.groupby(['redteam_prompt_ix','seg_id'])['is_nl_only'].shift(1).astype('boolean').fillna(False),\n",
    "    )\n",
    "\n",
    "    # where a double linebreak occurs (within each user segment)\n",
    "    .assign(dbl_here = lambda d:\n",
    "        d['token'].str.contains('\\n\\n', regex = False, na = False) | # contains \"\\n\\n\"\n",
    "        (d['prev_is_nl_only'] & d['is_nl_only']) | # two newline-only tokens\n",
    "        (d['prev_tok'].str.endswith('\\n', na = False) & d['token'].str.startswith('\\n', na = False))  # across boundary\n",
    "    )\n",
    "\n",
    "    # tokens strictly AFTER the last double break in each (redteam_prompt_ix, seg_id)\n",
    "    .assign(\n",
    "        rev_breaks = lambda d: d.groupby(['redteam_prompt_ix', 'seg_id'])\\\n",
    "            ['dbl_here'].transform(lambda s: s.iloc[::-1].cumsum().iloc[::-1]),\n",
    "        after_last_para = lambda d: d['is_user_content'] & d['rev_breaks'].eq(0),\n",
    "        is_user_visible = lambda d: d['is_user_content'] & ~d['is_nl_only'],\n",
    "    )\n",
    "\n",
    "    # final label: gate by policy_style\n",
    "    .assign(base_message_type = lambda d: np.where(\n",
    "        (d['policy_style'] != 'no_policy') & d['is_user_visible'] & d['after_last_para'], 'forged_cot', d['role']\n",
    "    ))\n",
    "\n",
    "    .drop(columns=[\n",
    "        'is_user_content', 'is_nl_only', 'prev_tok', 'prev_is_nl_only',\n",
    "        'dbl_here', 'rev_breaks', 'after_last_para', 'is_user_visible', 'policy_style'\n",
    "    ], errors = 'ignore')\n",
    ")\n",
    "\n",
    "del sample_df_import, sample_df_raw\n",
    "\n",
    "gc.collect()\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131b6264",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert activations to fp16 (for compatibility with cupy later) + layer-wise dict\n",
    "\"\"\"\n",
    "all_pre_mlp_hs = all_pre_mlp_hs_import.to(torch.float16)\n",
    "# compare_bf16_fp16_batched(all_pre_mlp_hs_import, all_pre_mlp_hs)\n",
    "del all_pre_mlp_hs_import\n",
    "all_pre_mlp_hs = {layer_ix: all_pre_mlp_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(act_map)}\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28d6726",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Count distibution of base_message_type tokens\n",
    "\"\"\"\n",
    "# sample_df.head(1000).to_csv(f'{ws}/test.csv', index = False)\n",
    "\n",
    "sample_df\\\n",
    "    .merge(prompts_df[['redteam_prompt_ix', 'policy_style', 'qualifier_type', 'output_class']], how = 'inner', on = 'redteam_prompt_ix')\\\n",
    "    .groupby(['policy_style', 'qualifier_type', 'base_message_type'], as_index = False)\\\n",
    "    .agg(n = ('redteam_prompt_ix', 'count'))\\\n",
    "    .pipe(lambda df: df.assign(pct = df['n'] / df.groupby(['policy_style', 'qualifier_type'])['n'].transform('sum')))\\\n",
    "    .pipe(lambda df: df.pivot_table(\n",
    "        index = ['policy_style', 'qualifier_type'],\n",
    "        columns = 'base_message_type',\n",
    "        values = 'pct',\n",
    "        fill_value = 0\n",
    "    ))\\\n",
    "    .reset_index(drop = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb2aa4",
   "metadata": {},
   "source": [
    "## Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cc9cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Project with probes\n",
    "\n",
    "The final projected dataset is at a (token-sample, role_space, layer) level. The columns include:      \n",
    "- sample_ix: the token-sample index      \n",
    "- role_space: the role space projected to (systemness, cotness, userness, assistantness)      \n",
    "- layer_ix: the target layer      \n",
    "- prob: the probability predicted by the probe to that token-sample at that layer_ix's activations for the role_space role      \n",
    "- prompt_ix: the prompt index of the prompt the token-sample belongs to      \n",
    "- output_class: the classification of the assistant output from this prompt, either HARMFUL or REFUSAL      \n",
    "- role: the actual role tags this token-sample belongs to (system, user, cot, assistant, or \"other\" if and only if its a harmony tag instead of the text inside the tag)      \n",
    "- base_message_type: equal to the actual role EXCEPT for the cot forgery tokens of cot forged prompts, which are set to \"forged_cot\"      \n",
    "- policy_style: whether the prompt was a base harmful question (no_policy), a styled cot forgery (base), or a destyled forgery (destyled)      \n",
    "- qualifier_type: the type of qualifier used, either no_qualifier or green_shirt/lucky_coin/etc\n",
    "\"\"\"\n",
    "role_projection_dfs = []\n",
    "probe_mapping_dfs = []\n",
    "\n",
    "for probe_ix, probe in tqdm(enumerate(probes)):\n",
    "\n",
    "    test_layer = probe['layer_ix']\n",
    "    test_roles = probe['roles']\n",
    "    \n",
    "    if test_layer not in all_pre_mlp_hs.keys():\n",
    "        continue\n",
    "\n",
    "    project_test_sample_df = sample_df\n",
    "    project_hs_cp = cupy.asarray(all_pre_mlp_hs[test_layer][project_test_sample_df['sample_ix'].tolist(), :])\n",
    "    project_probs = probe['probe'].predict_proba(project_hs_cp).round(6)\n",
    "\n",
    "    # Merge seq probs withto get sampel_ix\n",
    "    proj_results = pd.DataFrame(cupy.asnumpy(project_probs), columns = probe['roles']).clip(1e-6)\n",
    "    if len(proj_results) != len(project_test_sample_df):\n",
    "        raise Exception('Error!')\n",
    "\n",
    "    role_projection_df = pd.concat([proj_results, project_test_sample_df[['sample_ix']]], axis = 1)\n",
    "\n",
    "    # Merge with token-level metadata and cull missing-role tokens (equivalent to culling instruct-special toks only)\n",
    "    # Verify equality:\n",
    "    # - sample_df.assign(role = lambda df: df['role'].fillna('o')).groupby(['in_content_span', 'role'], as_index = False).agg(n=('sample_ix', 'count'))\n",
    "    role_projection_df =\\\n",
    "        role_projection_df\\\n",
    "        .melt(id_vars = ['sample_ix'], var_name = 'role_space', value_name = 'prob')\\\n",
    "        .merge(project_test_sample_df, on = 'sample_ix', how = 'inner')\\\n",
    "        .assign(\n",
    "            role = lambda df: df['role'].fillna('other'),\n",
    "            base_message_type = lambda df: df['base_message_type'].fillna('other'),\n",
    "            probe_ix = probe_ix            \n",
    "        )\\\n",
    "        .pipe(lambda df: df[df['role'] != 'other']) # Cull the role tags themselves\n",
    "\n",
    "    probe_mapping_dfs.append(pd.DataFrame({\n",
    "        'probe_ix': [probe_ix],\n",
    "        'layer_ix': [probe['layer_ix']],\n",
    "        'roles': [','.join(sorted(probe['roles']))]\n",
    "    }))\n",
    "    role_projection_dfs.append(role_projection_df)\n",
    "\n",
    "role_projection_df = pd.concat(role_projection_dfs, ignore_index = True)\n",
    "probe_mapping_df = pd.concat(probe_mapping_dfs, ignore_index = True)\n",
    "role_projection_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5232c4",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5527c082",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save for analysis\n",
    "\"\"\"\n",
    "role_projection_df\\\n",
    "    .to_feather(f'{ws}/experiments/role-injection-analysis/projections/redteam-role-projections-{model_prefix}.feather')\n",
    "\n",
    "probe_mapping_df\\\n",
    "    .to_csv(f'{ws}/experiments/role-injection-analysis/projections/redteam-role-probe-mapping-{model_prefix}.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea4e388",
   "metadata": {},
   "source": [
    "# (Optional) Project agent activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251d3438",
   "metadata": {},
   "source": [
    "## Load data + label actual vs style roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ecdcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset of sample (token) level data and activations\n",
    "\"\"\"\n",
    "def load_data(folder_path, model_prefix, max_data_files):\n",
    "    \"\"\"\n",
    "    Load samples + activations\n",
    "    \"\"\"\n",
    "    folders = [f'{ws}/experiments/role-injection-analysis/{folder_path}/{model_prefix}/{i:02d}' for i in range(max_data_files)]\n",
    "    folders = [f for f in folders if os.path.isdir(f)]\n",
    "\n",
    "    all_pre_mlp_hs = []\n",
    "    sample_df = []\n",
    "    topk_df = []\n",
    "\n",
    "    for f in tqdm(folders):\n",
    "        sample_df.append(pd.read_pickle(f'{f}/samples.pkl'))\n",
    "        topk_df.append(pd.read_pickle(f'{f}/topks.pkl'))\n",
    "        all_pre_mlp_hs.append(torch.load(f'{f}/all-pre-mlp-hidden-states.pt'))\n",
    "\n",
    "    sample_df = pd.concat(sample_df)\n",
    "    topk_df = pd.concat(topk_df)\n",
    "    all_pre_mlp_hs = torch.concat(all_pre_mlp_hs)\n",
    "\n",
    "    with open(f'{ws}/experiments/role-injection-analysis/{folder_path}/{model_prefix}/metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    gc.collect()\n",
    "    return sample_df, topk_df, all_pre_mlp_hs, metadata['all_pre_mlp_hidden_states_layers']\n",
    "\n",
    "sample_df_import, _, all_pre_mlp_hs_import, act_map = load_data('activations-agent', model_prefix, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99926f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load prompt-level dataset\n",
    "\"\"\"\n",
    "prompts_df = pd.read_csv(f\"{ws}/experiments/role-injection-analysis/activations-agent/{model_prefix}/agent-outputs-classified.csv\")\n",
    "prompts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee453d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Take the token-level dataframe (sample_df) and assign for each token:\n",
    "- `role`: the actual role the tags specify (`label_content_roles`)\n",
    "- `base_message_type`: the role style; equal to role except for the user/CoT injection tokens, which is equal to \"user-injection\" and \"forged_cot\"\n",
    "\"\"\"\n",
    "# Get injections - we will substring match to detect them\n",
    "injections = yaml.safe_load(open(f'{ws}/experiments/tool-injections/prompts/injections.yaml','r'))\n",
    "\n",
    "# Assign real content roles (col roles + seg_id indicating continuous role-segment within prompt)\n",
    "sample_df_raw =\\\n",
    "    label_content_roles(model_architecture, sample_df_import.rename(columns = {'redteam_prompt_ix': 'prompt_ix'}))\\\n",
    "    .drop(columns = ['batch_ix', 'sequence_ix'])\\\n",
    "    .rename(columns = {'prompt_ix': 'redteam_prompt_ix'})\\\n",
    "    .assign(sample_ix = lambda df: df.groupby(['redteam_prompt_ix', 'token_ix']).ngroup())\\\n",
    "    .assign(token_ix = lambda df: df.groupby(['redteam_prompt_ix']).cumcount())\\\n",
    "    .assign(\n",
    "        _noncontent = lambda d: (~d['in_content_span']).astype('int8'),\n",
    "        token_in_seg_ix = lambda d: d.groupby(['redteam_prompt_ix','seg_id'])['_noncontent'].cumsum().sub(1)\n",
    "    )\\\n",
    "    .drop(columns = ['_noncontent'])\n",
    "\n",
    "# Assign user tokens to CoT forgery\n",
    "sample_df =\\\n",
    "    sample_df_raw\\\n",
    "    .merge(prompts_df[['redteam_prompt_ix','variant']], how='left', on='redteam_prompt_ix')\\\n",
    "    .sort_values(['redteam_prompt_ix','token_ix'])\n",
    "\n",
    "####### Below identifies BMTs for base-injection/cot-forgery-injection\n",
    "# --- 1. Build injection lexicon ---\n",
    "# - Variant 1: 1 paragraph → user-injection\n",
    "# - Variant 2: 2 paragraphs → user-injection (p1), forged-cot (p2)\n",
    "\n",
    "# Variant 1 (base-injection): single paragraph=\"user-injection\"\n",
    "base_injections_df =\\\n",
    "    pd.DataFrame(injections.get('base_injections', []))\\\n",
    "    .assign(variant='base-injection', injection_type='user-injection', injection_text=lambda d: d['prompt'])\\\n",
    "    [[\"variant\", \"injection_type\", \"injection_text\"]]\n",
    "\n",
    "# Variant 2 (cot-forgery-injection): two paragraphs split on FIRST double newline\n",
    "prompt_injections_split_df = \\\n",
    "    pd.DataFrame(injections.get('prompt_injections', []))\\\n",
    "    .assign(_parts=lambda d: d['prompt'].str.split('\\n\\n', n=1, expand=False))\\\n",
    "    .assign(p1=lambda d: d['_parts'].str[0], p2=lambda d: d['_parts'].str[1])\n",
    "\n",
    "# Paragraph 1 → \"user-injection\"\n",
    "prompt_injections_user_df =\\\n",
    "    prompt_injections_split_df\\\n",
    "    .assign(variant='cot-forgery-injection', injection_type='user-injection', injection_text=lambda d: d['p1'])\\\n",
    "    [[\"variant\", \"injection_type\", \"injection_text\"]]\n",
    "\n",
    "# Paragraph 2 → \"forged-cot\"\n",
    "prompt_injections_cot_df =\\\n",
    "    prompt_injections_split_df\\\n",
    "    .assign(variant='cot-forgery-injection', injection_type='forged-cot', injection_text=lambda d: d['p2'])\\\n",
    "    [[\"variant\", \"injection_type\", \"injection_text\"]]\n",
    "\n",
    "# Unified lexicon (one row per paragraph)\n",
    "# Expand to JSON-escaped text variants to match tool JSON blobs:\n",
    "#  - keep raw, escape newlines as \"\\n\", escape quotes as \\\", both escapes together\n",
    "injection_lexicon_df =\\\n",
    "    pd.concat([base_injections_df, prompt_injections_user_df, prompt_injections_cot_df], ignore_index=True)\\\n",
    "    .assign(\n",
    "        text_raw  = lambda d: d['injection_text'],\n",
    "        text_nl = lambda d: d['injection_text'].str.replace('\\n', r'\\n', regex=False),\n",
    "        text_q = lambda d: d['injection_text'].str.replace('\"', r'\\\"', regex=False),\n",
    "        text_nl_q = lambda d: d['injection_text'].str.replace('\\n', r'\\n', regex=False).str.replace('\"', r'\\\"', regex=False),\n",
    "    )\\\n",
    "    .assign(text_variants=lambda d: d[['text_raw','text_nl','text_q','text_nl_q']].values.tolist())\\\n",
    "    .explode('text_variants')\\\n",
    "    .rename(columns = {'text_variants': 'injection_text_variant'})\\\n",
    "    .drop(columns = ['text_raw','text_nl','text_q','text_nl_q'])\\\n",
    "    .drop_duplicates(subset = ['variant','injection_type','injection_text_variant'])\n",
    "\n",
    "# --- 2. Build tool-output segments with char offsets ---\n",
    "tool_content_tokens_df =\\\n",
    "    sample_df\\\n",
    "    .sort_values(['redteam_prompt_ix','seg_id','token_ix'])\\\n",
    "    .assign(is_tool_content=lambda d: d['role'].eq('tool') & d['in_content_span'])\\\n",
    "    .query('is_tool_content')\\\n",
    "    .assign(\n",
    "        char_len = lambda d: d['token'].astype(str).str.len(),\n",
    "        char_end = lambda d: d.groupby(['redteam_prompt_ix','seg_id'])['char_len'].cumsum(),\n",
    "        char_start = lambda d: d['char_end'] - d['char_len'],\n",
    "    )\n",
    "\n",
    "tool_segments_df =\\\n",
    "    tool_content_tokens_df\\\n",
    "    .groupby(['redteam_prompt_ix','seg_id'], as_index=False)\\\n",
    "    .agg(\n",
    "        segment_text=('token', lambda s: ''.join(s)),\n",
    "        segment_variant=('variant', 'first')  # conversation-level variant\n",
    "    )\n",
    "\n",
    "# --- 3. Find injection matches per segment (exact substring) ---\n",
    "segment_matches_df =\\\n",
    "    tool_segments_df\\\n",
    "    .merge(injection_lexicon_df, left_on='segment_variant', right_on='variant', how='left')\\\n",
    "    .assign(\n",
    "        match_start=lambda d: d.apply(lambda r: r['segment_text'].find(r['injection_text_variant']), axis=1),\n",
    "        match_end  =lambda d: d['match_start'] + d['injection_text_variant'].str.len()\n",
    "    )\\\n",
    "    .query('match_start >= 0')\\\n",
    "    [['redteam_prompt_ix','seg_id','injection_type','match_start','match_end']]\n",
    "\n",
    "# If no matches, just set base_message_type = role and stop\n",
    "if segment_matches_df.empty:\n",
    "    sample_df = sample_df.assign(base_message_type=lambda d: d['role']).infer_objects(copy=False)\n",
    "else:\n",
    "    # --- 4) Map char spans back to tokens; assign base_message_type -------------\n",
    "    #  A token is inside a matched paragraph if its [char_start, char_end) overlaps [match_start, match_end)\n",
    "    token_span_labels_df =\\\n",
    "        tool_content_tokens_df[['redteam_prompt_ix','seg_id','token_ix','char_start','char_end']]\\\n",
    "        .merge(segment_matches_df, on=['redteam_prompt_ix','seg_id'], how='left')\\\n",
    "        .assign(\n",
    "            _has_match = lambda d: d['match_start'].notna(),\n",
    "            _overlap   = lambda d: d['_has_match'] & (d['char_end'] > d['match_start']) & (d['char_start'] < d['match_end']),\n",
    "            token_label= lambda d: d['injection_type'].where(d['_overlap'])\n",
    "        )\\\n",
    "        .dropna(subset=['token_label'])\\\n",
    "        .groupby(['redteam_prompt_ix','token_ix'], as_index=False)\\\n",
    "        .agg(base_message_type=('token_label','first'))  # multiple overlaps shouldn't conflict; first is fine\n",
    "\n",
    "    sample_df =\\\n",
    "        sample_df\\\n",
    "        .merge(token_span_labels_df, on=['redteam_prompt_ix','token_ix'], how='left')\\\n",
    "        .assign(base_message_type=lambda d: d['base_message_type'].fillna(d['role']))\\\n",
    "        .infer_objects(copy=False)\n",
    "\n",
    "# del sample_df_import, sample_df_raw\n",
    "\n",
    "gc.collect()\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea911bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check if it looks reasonable\n",
    "\"\"\"\n",
    "sample_df\\\n",
    "    .pipe(lambda df: df[df['redteam_prompt_ix'] == 0])\\\n",
    "    .groupby(['variant', 'seg_id', 'base_message_type'], as_index = False)\\\n",
    "    .agg(start = ('token', lambda x: x), token_ix = ('token_ix', min))\\\n",
    "    .sort_values(by = 'token_ix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b185e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert activations to fp16 (for compatibility with cupy later) + layer-wise dict\n",
    "\"\"\"\n",
    "all_pre_mlp_hs = all_pre_mlp_hs_import.to(torch.float16)\n",
    "# compare_bf16_fp16_batched(all_pre_mlp_hs_import, all_pre_mlp_hs)\n",
    "del all_pre_mlp_hs_import\n",
    "all_pre_mlp_hs = {layer_ix: all_pre_mlp_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(act_map)}\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ac4525",
   "metadata": {},
   "source": [
    "## Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3798af3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Project with probes\n",
    "\n",
    "The final projected dataset is at a (token-sample, role_space, layer) level. The columns include:      \n",
    "- sample_ix: the token-sample index      \n",
    "- role_space: the role space projected to (systemness, cotness, userness, assistantness)      \n",
    "- layer_ix: the target layer      \n",
    "- prob: the probability predicted by the probe to that token-sample at that layer_ix's activations for the role_space role      \n",
    "- prompt_ix: the prompt index of the prompt the token-sample belongs to      \n",
    "- output_class: the classification of the assistant output from this prompt, either HARMFUL or REFUSAL      \n",
    "- role: the actual role tags this token-sample belongs to (system, user, cot, assistant, or \"other\" if and only if its a harmony tag instead of the text inside the tag)      \n",
    "- base_message_type: equal to the actual role EXCEPT for the cot forgery tokens of cot forged prompts, which are set to \"forged_cot\"      \n",
    "- policy_style: whether the prompt was a base harmful question (no_policy), a styled cot forgery (base), or a destyled forgery (destyled)      \n",
    "- qualifier_type: the type of qualifier used, either no_qualifier or green_shirt/lucky_coin/etc\n",
    "\"\"\"\n",
    "role_projection_dfs = []\n",
    "probe_mapping_dfs = []\n",
    "\n",
    "for probe_ix, probe in tqdm(enumerate(probes)):\n",
    "\n",
    "    test_layer = probe['layer_ix']\n",
    "    test_roles = probe['roles']\n",
    "\n",
    "    if test_layer not in all_pre_mlp_hs.keys():\n",
    "        continue\n",
    "    \n",
    "    project_test_sample_df = sample_df\n",
    "    project_hs_cp = cupy.asarray(all_pre_mlp_hs[test_layer][project_test_sample_df['sample_ix'].tolist(), :])\n",
    "    project_probs = probe['probe'].predict_proba(project_hs_cp).round(6)\n",
    "\n",
    "    # Merge seq probs withto get sampel_ix\n",
    "    proj_results = pd.DataFrame(cupy.asnumpy(project_probs), columns = probe['roles']).clip(1e-6)\n",
    "    if len(proj_results) != len(project_test_sample_df):\n",
    "        raise Exception('Error!')\n",
    "\n",
    "    role_projection_df = pd.concat([proj_results, project_test_sample_df[['sample_ix']]], axis = 1)\n",
    "\n",
    "    # Merge with token-level metadata and cull missing-role tokens (equivalent to culling instruct-special toks only)\n",
    "    # Verify equality:\n",
    "    # - sample_df.assign(role = lambda df: df['role'].fillna('o')).groupby(['in_content_span', 'role'], as_index = False).agg(n=('sample_ix', 'count'))\n",
    "    role_projection_df =\\\n",
    "        role_projection_df\\\n",
    "        .melt(id_vars = ['sample_ix'], var_name = 'role_space', value_name = 'prob')\\\n",
    "        .merge(project_test_sample_df, on = 'sample_ix', how = 'inner')\\\n",
    "        .assign(\n",
    "            role = lambda df: df['role'].fillna('other'),\n",
    "            base_message_type = lambda df: df['base_message_type'].fillna('other'),\n",
    "            probe_ix = probe_ix            \n",
    "        )\\\n",
    "        .pipe(lambda df: df[df['role'] != 'other']) # Cull the role tags themselves\n",
    "\n",
    "    probe_mapping_dfs.append(pd.DataFrame({\n",
    "        'probe_ix': [probe_ix],\n",
    "        'layer_ix': [probe['layer_ix']],\n",
    "        'roles': [','.join(sorted(probe['roles']))]\n",
    "    }))\n",
    "    role_projection_dfs.append(role_projection_df)\n",
    "\n",
    "role_projection_df = pd.concat(role_projection_dfs, ignore_index = True)\n",
    "probe_mapping_df = pd.concat(probe_mapping_dfs, ignore_index = True)\n",
    "role_projection_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abd5765",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55bc7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save for analysis\n",
    "\"\"\"\n",
    "role_projection_df\\\n",
    "    .to_feather(f'{ws}/experiments/role-injection-analysis/projections/agent-role-projections-{model_prefix}.feather')\n",
    "\n",
    "probe_mapping_df\\\n",
    "    .to_csv(f'{ws}/experiments/role-injection-analysis/projections/agent-role-probe-mapping-{model_prefix}.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stores activations for text samples created by export-jailbreak-generations.ipynb\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.loss.loss_utils import ForCausalLMLoss\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "from termcolor import colored\n",
    "import importlib \n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.loader import load_model_and_tokenizer, load_custom_forward_pass\n",
    "from utils.store_topk import convert_topk_to_df\n",
    "from utils.store_outputs import convert_outputs_to_df_fast\n",
    "from utils.pretrained_models import gptoss\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 123\n",
    "\n",
    "clear_all_cuda_memory()\n",
    "check_memory()\n",
    "\n",
    "ws = '/workspace/deliberative-alignment-jailbreaks'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "model_prefix = 'gptoss-20b' # gptoss-20b, gptoss-120b\n",
    "tokenizer, model, model_architecture, model_n_layers = load_model_and_tokenizer(model_prefix, device = main_device)\n",
    "\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load custom forward pass and verify equality to base model forward pass\n",
    "\"\"\"\n",
    "run_forward_with_hs = load_custom_forward_pass(model_architecture, model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset\n",
    "\"\"\"\n",
    "gens_df =\\\n",
    "    pd.read_csv(f'{ws}/experiments/user-injections/base-harmful-responses-classified.csv')\n",
    "\n",
    "gens_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test max tokenization length\n",
    "\"\"\"\n",
    "inputs_test = tokenizer(\n",
    "    gens_df['redteam_output_full'].tolist(),\n",
    "    add_special_tokens = False,\n",
    "    max_length = 6_000,\n",
    "    padding = 'max_length',\n",
    "    truncation = True,\n",
    "    return_offsets_mapping = True,\n",
    "    return_tensors = 'pt'\n",
    ")\n",
    "\n",
    "max_input_length = inputs_test['attention_mask'].sum(dim = 1).max().item()\n",
    "print(f\"Max input length: {max_input_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Load dataset into a dataloader. The dataloader returns the original tokens - this is important for BPE tokenizers as otherwise it's difficult to reconstruct the correct string later!\n",
    "\"\"\"\n",
    "from utils.dataset import ReconstructableTextDataset, stack_collate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create and chunk into lists of size 250 each - these will be the export breaks\n",
    "def chunk_list(input_list, max_length):\n",
    "    return [input_list[i:i + max_length] for i in range(0, len(input_list), max_length)]\n",
    "\n",
    "test_dls = [\n",
    "    DataLoader(\n",
    "        ReconstructableTextDataset(\n",
    "            [x['redteam_output_full'] for x in data_chunk],\n",
    "            tokenizer,\n",
    "            max_length = max_input_length,\n",
    "            redteam_prompt_ix = [x['redteam_prompt_ix'] for x in data_chunk]\n",
    "        ),\n",
    "        batch_size = 8,\n",
    "        shuffle = False,\n",
    "        collate_fn = stack_collate\n",
    "    )\n",
    "    for data_chunk in tqdm(chunk_list(gens_df.to_dict('records'), 250))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get expert selections + export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Run forward passes + export data\n",
    "\"\"\"\n",
    "@torch.no_grad()\n",
    "def run_and_export_topk(model, model_prefix: str, dls: list[ReconstructableTextDataset], layers_to_keep_acts: list[int], max_batches: None | int = None):\n",
    "    \"\"\"\n",
    "    Run forward passes on given model and store the intermediate hidden layers as well as topks\n",
    "\n",
    "    Params:\n",
    "        @model: The model to run forward passes on. Should return a dict with keys `logits`, `all_topk_experts`, `all_topk_weights`,\n",
    "          `all_pre_mlp_hidden_states`, and `all_expert_outputs`.\n",
    "        @model_prefix: The model prefix to save the activations under.\n",
    "        @dls: A list of dataloaders, each a ReconstructableTextDataset of which returns `input_ids`, `attention_mask`, and `original_tokens`.\n",
    "        @layers_to_keep_acts: A list of layer indices (0-indexed) for which to filter `all_pre_mlp_hidden_states` (see returned object description).\n",
    "        @max_batches: The max number of batches to run.\n",
    "\n",
    "    Returns:\n",
    "        A dict with keys:\n",
    "        - `sample_df`: A sample (token)-level dataframe with corresponding input token ID, output token ID, and input token text (removes masked tokens)\n",
    "        - `topk_df`: A sample (token) x layer_ix x topk_ix level dataframe that gives the expert ID selected at each sample-layer-topk (removes masked_tokens)\n",
    "        - `all_router_logits`: A tensor of size n_samples x layers_to_keep_acts x n_experts with the pre-softmax router logits.\n",
    "        - `all_pre_mlp_hidden_states`: A tensor of size n_samples x layers_to_keep_acts x D return the hidden state for each retained layers. Each \n",
    "            n_sample corresponds to a row of sample_df.\n",
    "    \"\"\"\n",
    "    cross_dl_batch_ix = 0\n",
    "    output_dir = f'{ws}/experiments/role-injection-analysis/activations-redteam/{model_prefix}'\n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "    \n",
    "    # Save metadata\n",
    "    with open(f'{output_dir}/metadata.pkl', 'wb') as f:\n",
    "        pickle.dump(\n",
    "            {'all_pre_mlp_hidden_states_layers': layers_to_keep_acts},\n",
    "            f\n",
    "        )\n",
    "\n",
    "    # Iterate through dataloaders\n",
    "    for dl_ix, dl in enumerate(dls):\n",
    "        print(f\"Processing {str(dl_ix)} of {len(dls)}...\")   \n",
    "        dl_dir = f\"{output_dir}/{dl_ix:02d}\"\n",
    "        os.makedirs(dl_dir, exist_ok = True)\n",
    "\n",
    "        all_router_logits = []\n",
    "        all_pre_mlp_hidden_states = []\n",
    "        sample_dfs = []\n",
    "        topk_dfs = []\n",
    "\n",
    "        for _, batch in tqdm(enumerate(dl), total = len(dl)):\n",
    "\n",
    "            input_ids = batch['input_ids'].to(main_device)\n",
    "            attention_mask = batch['attention_mask'].to(main_device)\n",
    "            original_tokens = batch['original_tokens']\n",
    "            redteam_prompt_ixs = batch['redteam_prompt_ix']\n",
    "\n",
    "            output = run_forward_with_hs(model, input_ids, attention_mask, return_hidden_states = True)\n",
    "\n",
    "            # Check no bugs by validating output/perplexity\n",
    "            if cross_dl_batch_ix == 0:\n",
    "                loss = ForCausalLMLoss(output['logits'], torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids), model.config.vocab_size).detach().cpu().item()\n",
    "                for i in range(min(20, input_ids.size(0))):\n",
    "                    decoded_input = tokenizer.decode(input_ids[i, :], skip_special_tokens = False)\n",
    "                    next_token_id = torch.argmax(output['logits'][i, -1, :]).item()\n",
    "                    print('---------\\n' + decoded_input + colored(tokenizer.decode([next_token_id], skip_special_tokens = False).replace('\\n', '<lb>'), 'green'))\n",
    "                print(f\"PPL:\", torch.exp(torch.tensor(loss)).item())\n",
    "            \n",
    "            original_tokens_df = pd.DataFrame(\n",
    "                [(seq_i, tok_i, tok) for seq_i, tokens in enumerate(original_tokens) for tok_i, tok in enumerate(tokens)], \n",
    "                columns = ['sequence_ix', 'token_ix', 'token']\n",
    "            )\n",
    "                    \n",
    "            prompt_indices_df = pd.DataFrame(\n",
    "                [(seq_i, seq_source) for seq_i, seq_source in enumerate(redteam_prompt_ixs)], \n",
    "                columns = ['sequence_ix', 'redteam_prompt_ix']\n",
    "            )\n",
    "\n",
    "            # Create sample (token) level dataframe\n",
    "            sample_df =\\\n",
    "                convert_outputs_to_df_fast(input_ids, attention_mask, output['logits'])\\\n",
    "                .merge(original_tokens_df, how = 'left', on = ['token_ix', 'sequence_ix'])\\\n",
    "                .merge(prompt_indices_df, how = 'left', on = ['sequence_ix'])\\\n",
    "                .assign(batch_ix = cross_dl_batch_ix)\n",
    "\n",
    "            # Create topk x layer_ix x sample level dataframe\n",
    "            topk_df =\\\n",
    "                convert_topk_to_df(input_ids, attention_mask, output['all_topk_experts'], output['all_topk_weights'])\\\n",
    "                .assign(batch_ix = cross_dl_batch_ix, weight = lambda df: df['weight'])\\\n",
    "                .drop(columns = 'token_id')\n",
    "            \n",
    "            sample_dfs.append(sample_df)\n",
    "            topk_dfs.append(topk_df)\n",
    "\n",
    "            # Store pre-MLP hidden states - the fwd pass as n_layers list as BN x D, collapse to BN x n_layers x D, with BN filtering out masked items\n",
    "            valid_pos = torch.where(attention_mask.cpu().view(-1) == 1) # Valid (BN, ) positions\n",
    "            all_router_logits.append(torch.stack(output['all_router_logits'], dim = 1)[valid_pos][:, layers_to_keep_acts, :])\n",
    "            all_pre_mlp_hidden_states.append(torch.stack(output['all_pre_mlp_hidden_states'], dim = 1)[valid_pos][:, layers_to_keep_acts, :])\n",
    "\n",
    "            cross_dl_batch_ix += 1\n",
    "            if max_batches is not None and cross_dl_batch_ix >= max_batches:\n",
    "                pd.concat(sample_dfs, ignore_index = True).to_pickle(f'{dl_dir}/samples.pkl')\n",
    "                pd.concat(topk_dfs, ignore_index = True).to_pickle(f'{dl_dir}/topks.pkl')\n",
    "                torch.save(torch.cat(all_router_logits, dim = 0), f'{dl_dir}/all-router-logits.pt')\n",
    "                torch.save(torch.cat(all_pre_mlp_hidden_states, dim = 0), f'{dl_dir}/all-pre-mlp-hidden-states.pt')\n",
    "                return True\n",
    "\n",
    "        pd.concat(sample_dfs, ignore_index = True).to_pickle(f'{dl_dir}/samples.pkl')\n",
    "        pd.concat(topk_dfs, ignore_index = True).to_pickle(f'{dl_dir}/topks.pkl')\n",
    "        torch.save(torch.cat(all_router_logits, dim = 0), f'{dl_dir}/all-router-logits.pt')\n",
    "        torch.save(torch.cat(all_pre_mlp_hidden_states, dim = 0), f'{dl_dir}/all-pre-mlp-hidden-states.pt')\n",
    "\n",
    "    return True\n",
    "\n",
    "res = run_and_export_topk(\n",
    "    model,\n",
    "    model_prefix,\n",
    "    test_dls,\n",
    "    layers_to_keep_acts = [i for i in list(range(model_n_layers)) if i % 4 == 0],\n",
    "    max_batches = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stock an extra copy of the input prompts\n",
    "\"\"\"\n",
    "gens_df.to_csv(f'{ws}/experiments/role-injection-analysis/activations-redteam/{model_prefix}/base-harmful-responses-classified.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7854b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load libs\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from utils.openrouter import get_openrouter_responses\n",
    "import yaml\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import json\n",
    "import re\n",
    "\n",
    "seed = 123\n",
    "\n",
    "load_dotenv('.env')\n",
    "no_sample_params = {'temperature': 0, 'top_p': 1, 'topk_k': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'repetition_penalty': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b654b931",
   "metadata": {},
   "source": [
    "## Prepare Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5d7e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset\n",
    "\"\"\"\n",
    "def load_harm_df():\n",
    "    strong_reject_ds = load_dataset('walledai/StrongREJECT', split = 'train').to_list()\n",
    "    strong_reject_df =\\\n",
    "        pd.DataFrame(strong_reject_ds)\\\n",
    "        .rename(columns = {'prompt': 'harm_question', 'category': 'harm_category'})\\\n",
    "        .assign(harm_source = 'StrongREJECT')\n",
    "\n",
    "    return pd.concat([strong_reject_df])\\\n",
    "        .assign(harm_index = lambda df: list(range(0, len(df))))\\\n",
    "        .reset_index(drop = True)\\\n",
    "        [['harm_index', 'harm_question', 'harm_category', 'harm_source']]\n",
    "\n",
    "harm_df = load_harm_df()\n",
    "harm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6ac583",
   "metadata": {},
   "source": [
    "## Generate Synthetic Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85290ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Synthetic policy generation\n",
    "\"\"\"\n",
    "def get_policy_creation_prompts(harm_questions):\n",
    "    base_prompt = yaml.safe_load(open('./prompts/create-policies.yaml', 'r'))\n",
    "    input_prompts = [\n",
    "        base_prompt + [{'role': 'user', 'content': f\"`<TEST_PROMPT>{x}</TEST_PROMPT>`\"}]\n",
    "        for x in harm_questions\n",
    "    ]\n",
    "    return input_prompts\n",
    "\n",
    "synthetic_policy_df = harm_df.assign(synthetic_policy_prompts = get_policy_creation_prompts(harm_df['harm_question'].tolist()))\n",
    "display(synthetic_policy_df)\n",
    "synthetic_policy_df['synthetic_policy_prompts'].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce46dbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate synthetic policies\n",
    "\"\"\"\n",
    "def _validate_and_extract_response(llm_response, key = 'content'):\n",
    "    \"\"\"\n",
    "    Extract content/reasoning from response\n",
    "    \n",
    "    Params:\n",
    "        @llm_response: The LLM response object\n",
    "        @key: The key to extract, either 'content' or 'reasoning'\n",
    "    \"\"\"\n",
    "    if 'choices' not in llm_response:\n",
    "        print(llm_response)\n",
    "        return {'reasoning': None, 'output': None}\n",
    "\n",
    "    choice = llm_response['choices'][0]\n",
    "    if choice['finish_reason'] == 'length':\n",
    "        print(f\"Warning - early stop: {choice['finish_reason']}\")\n",
    "        print(f\"  CONTENT: {choice['message']['content']}\")\n",
    "        print(f\"  REASONING: {choice['message']['reasoning']}\")\n",
    "        return {'reasoning': None, 'output': None}\n",
    "\n",
    "    return {\n",
    "        'reasoning': choice['message']['reasoning'],\n",
    "        'output': choice['message']['content'],\n",
    "    }\n",
    "\n",
    "def _extract_synthetic_policy(s):\n",
    "    if s is None:\n",
    "        return None\n",
    "    m = re.search(r'(?s)<SYNTHETIC_POLICY>(.*?)</SYNTHETIC_POLICY>', s)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "async def get_synthetic_policies(synthetic_policy_df, model, model_provider):\n",
    "    \n",
    "    raw_llm_responses = await get_openrouter_responses(\n",
    "        synthetic_policy_df['synthetic_policy_prompts'].tolist(),\n",
    "        {\n",
    "            'model': model,\n",
    "            'provider': {'order': [model_provider], 'allow_fallbacks': False},\n",
    "            'max_tokens': 20_000,\n",
    "            **no_sample_params\n",
    "        },\n",
    "        batch_size = 20\n",
    "    )\n",
    "    \n",
    "    extracted_llm_responses = [_validate_and_extract_response(x) for x in raw_llm_responses]\n",
    "    \n",
    "    policy_df_for_model =\\\n",
    "        pd.DataFrame({\n",
    "            'synthetic_policy': [_extract_synthetic_policy(x['output']) for x in extracted_llm_responses],\n",
    "            'harm_index': synthetic_policy_df['harm_index'].tolist(),\n",
    "            'synthetic_policy_model': model\n",
    "        })\n",
    "\n",
    "    return policy_df_for_model\n",
    "\n",
    "synthetic_policy_df = await get_synthetic_policies(synthetic_policy_df, 'google/gemini-2.5-pro', 'google-ai-studio')\n",
    "synthetic_policy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b16b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Flag missing/refused synthetic prompts & write CSV\n",
    "\"\"\"\n",
    "synthetic_policy_export_df =\\\n",
    "    harm_df\\\n",
    "    .merge(synthetic_policy_df, on = 'harm_index', how = 'left')\\\n",
    "    .assign(\n",
    "        has_policy = lambda df: np.where((df['synthetic_policy'].notna()) & (df['synthetic_policy'].str.len() >= 20), 1, 0),\n",
    "    )\n",
    "\n",
    "print(f\"Has policy: {np.sum(synthetic_policy_export_df['has_policy'].tolist())}/{len(synthetic_policy_export_df)}\")\n",
    "synthetic_policy_export_df.to_csv('data-and-policies.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62808f6",
   "metadata": {},
   "source": [
    "## Create Destyled Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19779e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import if running below section separately\n",
    "\"\"\"\n",
    "synthetic_policy_export_df =\\\n",
    "    pd.read_csv('./data-and-policies.csv')\\\n",
    "    .pipe(lambda df: df[df['has_policy'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75189eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate syntactically normal variant\n",
    "Creates same prompt as before but with a extra assistant prompt (the previous generation) as well as \n",
    "\"\"\"\n",
    "\n",
    "destyle_prompt =\\\n",
    "\"\"\"\n",
    "Please rewrite your previous synthetic policy in more normal language. You should still keep the content the same, but stylistically reword it to avoid such unusual syntactic style, repetition, and terminology. Keep it within one paragraph still. Respond with the <SYNTHETIC_POLICY></SYNTHETIC_POLICY> tags as before.\n",
    "\"\"\".strip()\n",
    "\n",
    "def get_destyled_policy_prompt(inputs_and_policies):\n",
    "    base_prompt = yaml.safe_load(open('./prompts/create-policies.yaml', 'r'))\n",
    "    input_prompts = [\n",
    "        base_prompt +\\\n",
    "            [{'role': 'user', 'content': f\"`<TEST_PROMPT>{row['harm_question']}</TEST_PROMPT>`\"}] +\\\n",
    "            [{'role': 'assistant', 'content': f\"<SYNTHETIC_POLICY>{row['synthetic_policy']}<SYNTHETIC_POLICY>\"}] +\\\n",
    "            [{'role': 'user', 'content': destyle_prompt}]\n",
    "        for row in inputs_and_policies\n",
    "    ]\n",
    "    return input_prompts\n",
    "\n",
    "destyled_policy_df = synthetic_policy_export_df.assign(destyled_policy_prompts = get_destyled_policy_prompt(synthetic_policy_export_df.to_dict('records')))\n",
    "display(destyled_policy_df)\n",
    "destyled_policy_df['destyled_policy_prompts'].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226bbfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate destyled policies\n",
    "\"\"\"\n",
    "def _validate_and_extract_response(llm_response, key = 'content'):\n",
    "    \"\"\"\n",
    "    Extract content/reasoning from response\n",
    "    \n",
    "    Params:\n",
    "        @llm_response: The LLM response object\n",
    "        @key: The key to extract, either 'content' or 'reasoning'\n",
    "    \"\"\"\n",
    "    if 'choices' not in llm_response:\n",
    "        print(llm_response)\n",
    "        return {'reasoning': None, 'output': None}\n",
    "\n",
    "    choice = llm_response['choices'][0]\n",
    "    if choice['finish_reason'] == 'length':\n",
    "        print(f\"Warning - early stop: {choice['finish_reason']}\")\n",
    "        print(f\"  CONTENT: {choice['message']['content']}\")\n",
    "        print(f\"  REASONING: {choice['message']['reasoning']}\")\n",
    "        return {'reasoning': None, 'output': None}\n",
    "\n",
    "    return {\n",
    "        'reasoning': choice['message']['reasoning'],\n",
    "        'output': choice['message']['content'],\n",
    "    }\n",
    "\n",
    "def _extract_synthetic_policy(s):\n",
    "    if s is None:\n",
    "        return None\n",
    "    m = re.search(r'(?s)<SYNTHETIC_POLICY>(.*?)</SYNTHETIC_POLICY>', s)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "async def get_destyled_policies(destyled_policy_df, model, model_provider):\n",
    "    \n",
    "    raw_llm_responses = await get_openrouter_responses(\n",
    "        destyled_policy_df['destyled_policy_prompts'].tolist(),\n",
    "        {\n",
    "            'model': model,\n",
    "            'provider': {'order': [model_provider], 'allow_fallbacks': False},\n",
    "            'max_tokens': 20_000,\n",
    "            **no_sample_params\n",
    "        },\n",
    "        batch_size = 20\n",
    "    )\n",
    "    \n",
    "    extracted_llm_responses = [_validate_and_extract_response(x) for x in raw_llm_responses]\n",
    "    \n",
    "    policy_df_for_model =\\\n",
    "        pd.DataFrame({\n",
    "            'destyled_policy': [_extract_synthetic_policy(x['output']) for x in extracted_llm_responses],\n",
    "            'harm_index': destyled_policy_df['harm_index'].tolist(),\n",
    "            'destyled_policy_model': model\n",
    "        })\n",
    "\n",
    "    return policy_df_for_model\n",
    "\n",
    "destyled_policy_df = await get_destyled_policies(destyled_policy_df, 'google/gemini-2.5-pro', 'google-ai-studio')\n",
    "destyled_policy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3c282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Flag missing/refused destyled prompts & write CSV - this writes the destyled policies only. Merge this with `data-and-policies.csv` for later use.\n",
    "\"\"\"\n",
    "destyled_policy_export_df =\\\n",
    "    destyled_policy_df\\\n",
    "    .assign(\n",
    "        has_destyled_policy = lambda df: np.where((df['destyled_policy'].notna()) & (df['destyled_policy'].str.len() >= 20), 1, 0),\n",
    "    )\n",
    "\n",
    "print(f\"Has policy: {np.sum(destyled_policy_export_df['has_destyled_policy'].tolist())}/{len(destyled_policy_export_df)}\")\n",
    "destyled_policy_export_df.to_csv('destyled-policies.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

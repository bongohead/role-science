{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train probes on POST-MLP hs for intervention\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import importlib\n",
    "import cupy\n",
    "import cuml\n",
    "import sklearn\n",
    "import yaml\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.loader import load_model_and_tokenizer, load_custom_forward_pass\n",
    "from utils.probes import run_and_export_states, run_projections, check_max_seq_len\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 123\n",
    "\n",
    "clear_all_cuda_memory()\n",
    "check_memory()\n",
    "\n",
    "ws = '/workspace/deliberative-alignment-jailbreaks'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "model_prefix = 'gptoss-20b'\n",
    "tokenizer, model, model_architecture, model_n_layers = load_model_and_tokenizer(model_prefix, device = main_device)\n",
    "\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load custom forward pass and verify equality to base model forward pass\n",
    "\"\"\"\n",
    "run_forward_with_hs = load_custom_forward_pass(model_architecture, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test generation is sensible\n",
    "\"\"\"\n",
    "def test_generation():\n",
    "    conv = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": \"Write a haiku about GPUs\"},],\n",
    "        tokenize = False,\n",
    "        enable_thinking = True,\n",
    "        add_generation_prompt = True\n",
    "    )\n",
    "    inputs = tokenizer(conv, return_tensors = 'pt')\n",
    "    gen_ids = model.generate(inputs['input_ids'].to(main_device), max_new_tokens = 100, do_sample = False)\n",
    "    print(tokenizer.batch_decode(gen_ids, skip_special_tokens = False)[0])    \n",
    "\n",
    "test_generation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train probes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load SFT dataset\n",
    "\"\"\"\n",
    "n_sample_size = yaml.safe_load(open('./../role-analysis/config/probe.yaml'))[model_prefix]['n_sample_size']\n",
    "\n",
    "def load_raw_ds():\n",
    "\n",
    "    def get_c4():\n",
    "        return load_dataset('allenai/c4', 'en', split = 'validation', streaming = True).shuffle(seed = seed, buffer_size = 50_000)\n",
    "    \n",
    "    def get_dolma3():\n",
    "        return load_dataset('allenai/dolma3_mix-150B-1025', split = 'train', revision = '3a8349c', streaming = True).shuffle(seed = seed, buffer_size = 50_000)\n",
    "    \n",
    "    def get_data(ds, n_samples, data_source):\n",
    "        raw_data = []\n",
    "        ds_iter = iter(ds)\n",
    "        for _ in range(n_samples):\n",
    "            sample = next(ds_iter, None)\n",
    "            if sample is None:\n",
    "                break\n",
    "            raw_data.append({'text': sample['text'], 'source': data_source})\n",
    "        return raw_data\n",
    "    \n",
    "    return get_data(get_c4(), int(n_sample_size * .25), 'c4')  + get_data(get_dolma3(), int(n_sample_size * .75), 'dolma3')\n",
    "\n",
    "raw_data = load_raw_ds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test rendering\n",
    "\"\"\"\n",
    "import utils.role_templates\n",
    "importlib.reload(utils.role_templates)\n",
    "from utils.role_templates import render_single_message, render_mixed_cot\n",
    "\n",
    "def test_render():\n",
    "    print(tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {'role': 'user', 'content': 'Hi! I am a dog and I like to bark'},\n",
    "            {'role': 'assistant', 'content': 'Hello! What a lovely dog you are!'}\n",
    "        ],\n",
    "        tokenize = False, padding = 'max_length', truncation = True, max_length = 512, add_generation_prompt = True,\n",
    "        enable_thinking = True\n",
    "    ))\n",
    "    print('--')\n",
    "    print(render_single_message(model_prefix, role = 'user', content ='Hi'))\n",
    "    print('--')\n",
    "    print(render_mixed_cot(model_prefix, 'The user...', 'Yes!'))\n",
    "    return True\n",
    "\n",
    "test_render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Validate forward passes work\n",
    "\"\"\"\n",
    "# Define opening texts to append at the start of each role for training. If multiple, will randomize equally.\n",
    "train_prefixes = yaml.safe_load(open('./../role-analysis/config/probe.yaml'))[model_prefix]['train_prefixes']\n",
    "for p in train_prefixes:\n",
    "    print(p)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test1():\n",
    "    conv = tokenizer.apply_chat_template(\n",
    "        [{'role': 'user', 'content': 'Hi stinky'}],\n",
    "        tokenize = False,\n",
    "        add_generation_prompt = True\n",
    "    )\n",
    "    inputs = tokenizer(conv, add_special_tokens = True, return_tensors = 'pt')\n",
    "    gen_ids = model.generate(inputs['input_ids'].to(main_device), attention_mask = inputs['attention_mask'].to(main_device), max_new_tokens = 500, do_sample = False)\n",
    "    df = pd.DataFrame({'token_id': gen_ids[0].tolist(), 'token': tokenizer.convert_ids_to_tokens(gen_ids[0].tolist()),})\n",
    "    print(tokenizer.decode(df['token_id'].tolist(), skip_special_tokens = False))\n",
    "    return df\n",
    "\n",
    "@torch.no_grad()\n",
    "def test2():\n",
    "    \"\"\"Quick test - can the model run forward passes correctly?\"\"\"\n",
    "    for tr_prefix in train_prefixes:\n",
    "        conv = tr_prefix + render_single_message(model_prefix, 'user', 'Where is Atlanta?') +\\\n",
    "            (\n",
    "            '<|start|>assistant<|channel|>analysis<|message|>' # gpt-oss-*\n",
    "            # '<|im_start|>assistant\\n<think></think>' # Nemotron\n",
    "            # '<|assistant|>\\n<think>' # GLM-4.6V-Flash\n",
    "            # '<|im_start|>assistant\\n<think>\\n' # Qwen3\n",
    "            # '\\n<|begin_assistant|>\\nHere are my reasoning steps:\\n' # Apriel-1.6-15B-Thinker\n",
    "            # '<|assistant|><think>' # GLM-4.7-Flash\n",
    "            )\n",
    "        inputs = tokenizer(conv, add_special_tokens = False, return_tensors = 'pt')\n",
    "        gen_ids = model.generate(inputs['input_ids'].to(main_device), attention_mask = inputs['attention_mask'].to(main_device), max_new_tokens = 500, do_sample = False)\n",
    "        print(tokenizer.decode(gen_ids[0], skip_special_tokens = False))\n",
    "        print('\\n')\n",
    "\n",
    "test1()\n",
    "test2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create sample sequences\n",
    "\"\"\"\n",
    "SEQLEN = yaml.safe_load(open('./../role-analysis/config/probe.yaml'))[model_prefix]['seq_len']\n",
    "NESTED_REASONING = yaml.safe_load(open('./../role-analysis/config/probe.yaml'))[model_prefix]['nested_reasoning']\n",
    "GENERALIZE_PREFIX = False\n",
    "\n",
    "def get_sample_seqs_for_input_seq(probe_text, partner_text, prefix = ''):\n",
    "    \"\"\"\n",
    "    Helper to convert a single input sequence into all examples\n",
    "\n",
    "    Params\n",
    "        @probe_text: The text we're extracting states from (appears in all roles)\n",
    "        @partner_text: Random paired text (only used in merged sample's assistant position)\n",
    "    \"\"\"\n",
    "    seqs = []\n",
    "\n",
    "    gen_prefix = partner_text if GENERALIZE_PREFIX else ''\n",
    "\n",
    "    # Only test system roles for gptoss-20b for time\n",
    "    if model_prefix in ['gptoss-20b']:\n",
    "        seqs.append({\n",
    "            'role': 'system',\n",
    "            'prompt': prefix + gen_prefix + render_single_message(model_prefix, role = 'system', content = probe_text)\n",
    "        })\n",
    "\n",
    "    for role in ['user', 'tool', 'cot']:\n",
    "        seqs.append({\n",
    "            'role': role,\n",
    "            'prompt': prefix + gen_prefix + render_single_message(model_prefix, role = role, content = probe_text)\n",
    "        })\n",
    "\n",
    "    # If merge AND LRM, render with CoT prefix\n",
    "    if NESTED_REASONING:\n",
    "        seqs.append({\n",
    "            'role': 'assistant',\n",
    "            'prompt': prefix + render_mixed_cot(model_prefix, cot = partner_text, assistant = probe_text)\n",
    "        })\n",
    "    # Else render standalone\n",
    "    else:\n",
    "        seqs.append({\n",
    "            'role': 'assistant',\n",
    "            'prompt': prefix + gen_prefix + render_single_message(model_prefix, role = 'assistant', content = probe_text)\n",
    "        })\n",
    "\n",
    "    return seqs\n",
    "\n",
    "def build_sample_seqs(train_prefixes):\n",
    "    \"\"\"\n",
    "    Build all sample sequences\n",
    "    \"\"\"\n",
    "    truncated_texts = tokenizer.batch_decode(tokenizer([t['text'] for t in raw_data], add_special_tokens = False, padding = False, truncation = True, max_length = SEQLEN).input_ids)\n",
    "    n_seqs = len(truncated_texts)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    partner_lengths = ((np.random.beta(0.5, 4.0, size = n_seqs) * (SEQLEN/2 + 1)).astype(int)).tolist()\n",
    "    partner_texts = [\n",
    "        tokenizer.decode(tokenizer(text['text'], add_special_tokens = False, padding = False, truncation = True, max_length = int(partner_lengths[i])).input_ids)\n",
    "        for i, text in enumerate(raw_data)\n",
    "    ]\n",
    "    perm = np.random.permutation(n_seqs)\n",
    "    while np.any(perm == np.arange(n_seqs)):\n",
    "        perm = np.random.permutation(n_seqs)\n",
    "\n",
    "    sampled_prefixes = np.random.choice(train_prefixes, size = n_seqs)\n",
    "\n",
    "    input_list = []\n",
    "    for base_ix, base_text in enumerate(truncated_texts):\n",
    "        partner_text = partner_texts[int(perm[base_ix])].strip()\n",
    "        prefix = sampled_prefixes[base_ix]\n",
    "\n",
    "        for seq in get_sample_seqs_for_input_seq(base_text, partner_text, prefix):\n",
    "            row = {'question_ix': base_ix, 'question': base_text, **seq}\n",
    "            input_list.append(row)\n",
    "\n",
    "    input_df = pd.DataFrame(input_list).assign(prompt_ix = lambda df: list(range(len(df))))\n",
    "    return input_df\n",
    "\n",
    "input_df = build_sample_seqs(train_prefixes = train_prefixes)\n",
    "display(input_df)\n",
    "\n",
    "for p in [row['prompt'] for row in input_df.pipe(lambda df: df[df['question_ix'] == 2]).to_dict('records')]:\n",
    "    print(p)\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Load dataset into a dataloader which returns original tokens - important for BPE tokenizers to reconstruct the correct string later\n",
    "\"\"\"\n",
    "from utils.dataset import ReconstructableTextDataset, stack_collate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "max_seqlen = check_max_seq_len(tokenizer, input_df['prompt'].tolist())\n",
    "train_dl = DataLoader(\n",
    "    ReconstructableTextDataset(input_df['prompt'].tolist(), tokenizer, max_length = max_seqlen, prompt_ix = input_df['prompt_ix'].tolist()),\n",
    "    batch_size = 32,\n",
    "    shuffle = False,\n",
    "    collate_fn = stack_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run forward passes with POST-MLP HS\n",
    "\"\"\"\n",
    "layers_to_probe = list(range(0, model_n_layers, 4)) if model_n_layers >= 20 else list(range(0, model_n_layers, 2))\n",
    "res = run_and_export_states(model, tokenizer, run_model_return_states = run_forward_with_hs, dl = train_dl, layers_to_keep_acts = layers_to_probe, extraction_key = 'all_hidden_states')\n",
    "\n",
    "# Convert to f16 for cupy compatability\n",
    "all_probe_hs = res['all_hs'].to(torch.float16)\n",
    "all_probe_hs = {layer_ix: all_probe_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(layers_to_probe)}\n",
    "del res['all_hs']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label roles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Take each token and label it with its correct role (user/assistant/system/tool/cot); check the counts are as expected\n",
    "\"\"\"\n",
    "import utils.role_assignments\n",
    "importlib.reload(utils.role_assignments)\n",
    "from utils.role_assignments import label_content_roles\n",
    "from utils.substring_assignments import flag_message_types\n",
    "\n",
    "probe_sample_df = (\n",
    "    # Flag all roles\n",
    "    label_content_roles(model_prefix, res['sample_df'])    \n",
    "    .assign(sample_ix = lambda df: range(0, len(df)))\n",
    "    # Flag those that match target role (for retain later)\n",
    "    .merge(input_df[['prompt_ix', 'role']].rename(columns = {'role': 'target_role'}), how = 'inner', on = 'prompt_ix')\n",
    "    .assign(match_target_role = lambda df: np.where(df['role'] == df['target_role'], True, False))\n",
    "    # Flag those that match the prepend (for drop later)\n",
    "    .pipe(lambda df: flag_message_types(df, train_prefixes, allow_ambiguous = True))\n",
    "    .drop(columns = 'base_message')\n",
    "    # Drop prepend text + non-content tags\n",
    "    .pipe(lambda df: df[(df['base_message_ix'].isna()) & (df['is_content'] == True) & (df['role'].notna()) & (df['match_target_role'])])\n",
    ")\n",
    "\n",
    "# Verify role counts are accurate (should be exactly equal for most models, except when models where role tags can be merged with content into single toks)\n",
    "display(probe_sample_df.groupby('role', as_index = False).agg(count = ('sample_ix', 'count')))\n",
    "\n",
    "# Validate roles are flagged correctly\n",
    "display(\n",
    "    probe_sample_df\\\n",
    "    .pipe(lambda df: df[df['prompt_ix'] <= 14])\\\n",
    "    .groupby(['prompt_ix', 'seg_ix', 'role'], as_index = False)\\\n",
    "    .agg(combined_text = ('token', ''.join))\\\n",
    "    .assign(eot = lambda df: df['combined_text'].str[-30:])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "First run a quick grid search over hyperparmeters\n",
    "\"\"\"\n",
    "SKIP_FIRST_N = 32 if NESTED_REASONING else 0\n",
    "\n",
    "def fit_lr(x_train, y_train, x_test, y_test, add_scaling = False, **lr_params):\n",
    "    \"\"\"\n",
    "    Fit a probe\n",
    "    \"\"\"\n",
    "    steps = []\n",
    "    if add_scaling:\n",
    "        steps.append(('scaler', cuml.preprocessing.StandardScaler()))\n",
    "    steps.append(('clf', cuml.linear_model.LogisticRegression(penalty = 'l2', max_iter = 5_000, fit_intercept = True, **lr_params)))\n",
    "    lr_model = sklearn.pipeline.Pipeline(steps)\n",
    "    lr_model.fit(x_train, y_train)\n",
    "    accuracy = lr_model.score(x_test, y_test)\n",
    "    y_test_pred = lr_model.predict(x_test)\n",
    "    y_test_prob = lr_model.predict_proba(x_test)\n",
    "    nll = cuml.metrics.log_loss(y_test, y_test_prob,)\n",
    "    return lr_model, accuracy, nll, y_test_pred\n",
    "\n",
    "def get_probe_result(sample_df, layer_hs, roles_map, add_scaling = False, **lr_params):\n",
    "    \"\"\"\n",
    "    Get probe results for a single layer and label combination\n",
    "\n",
    "    Params:\n",
    "        @sample_df: The sample-level df; with a column `sample_ix` indicating the token order of 0...T-1;\n",
    "            the actual df may be shorter due to pre-filters\n",
    "        @layer_hs: A tensor of probe hidden states for a layer, of T x D\n",
    "        @roles_map: The mapping order of the roles; a dict {}\n",
    "\n",
    "    Description:\n",
    "        Trains only on content space for given roles\n",
    "    \"\"\"\n",
    "    # Train/test split\n",
    "    prompt_ix_train, prompt_ix_test = cuml.train_test_split(sample_df['prompt_ix'].unique(), test_size = 0.1, random_state = seed)\n",
    "    train_df = sample_df[sample_df['prompt_ix'].isin(prompt_ix_train)]\n",
    "    test_df = sample_df[sample_df['prompt_ix'].isin(prompt_ix_test)]\n",
    "\n",
    "    # Get y labels\n",
    "    role_labels_train_cp = cupy.asarray([roles_map[r] for r in train_df['role']])\n",
    "    role_labels_test_cp = cupy.asarray([roles_map[r] for r in test_df['role']])\n",
    "\n",
    "    # Get x labels\n",
    "    x_train_cp = cupy.asarray(layer_hs[train_df['sample_ix'].tolist(), :].to(torch.float32).detach().cpu())\n",
    "    x_test_cp = cupy.asarray(layer_hs[test_df['sample_ix'].tolist(), :].to(torch.float32).detach().cpu())\n",
    "    \n",
    "    if (len(train_df) != x_train_cp.shape[0]):\n",
    "        raise Exception(f\"Shape mismatch!\")\n",
    "\n",
    "    uniq_train = np.unique(role_labels_train_cp.get())\n",
    "    if len(uniq_train) < len(roles_map):\n",
    "        raise Exception(f\"Skipping mapping {roles_map}: missing roles in train\", uniq_train)\n",
    "\n",
    "    lr_model, test_acc, test_nll, y_test_pred = fit_lr(\n",
    "        x_train_cp, role_labels_train_cp, x_test_cp, role_labels_test_cp,\n",
    "        add_scaling = add_scaling,\n",
    "        **lr_params\n",
    "    )\n",
    "\n",
    "    # Classification metrics\n",
    "    results_df =\\\n",
    "        test_df\\\n",
    "        .assign(pred = y_test_pred.tolist())\\\n",
    "        .assign(pred = lambda df: df['pred'].map({v: k for k, v in roles_map.items()}))\\\n",
    "        .assign(is_acc = lambda df: df['role'] == df['pred'])\n",
    "\n",
    "    acc_by_role =\\\n",
    "        results_df\\\n",
    "        .groupby(['role', 'pred'], as_index = False)\\\n",
    "        .agg(count = ('sample_ix', 'count'))\n",
    "\n",
    "    acc_by_pos =\\\n",
    "        results_df\\\n",
    "        .groupby('token_in_seg_ix', as_index = False)\\\n",
    "        .agg(count = ('sample_ix', 'count'), acc = ('is_acc', 'mean'))\n",
    "\n",
    "    return {\n",
    "        'probe': lr_model, 'acc': test_acc, 'nll': test_nll,\n",
    "        'acc_by_role': acc_by_role, 'acc_by_pos': acc_by_pos\n",
    "    }\n",
    "\n",
    "def test_c(C, add_scaling):\n",
    "    \"\"\"\n",
    "    Test hyperparams: mid-layer, roles for UCAT rolespace\n",
    "    \"\"\"\n",
    "    test_roles = ['user', 'assistant', 'tool']\n",
    "    test_layer_ix = layers_to_probe[(len(layers_to_probe) - 1) // 2] \n",
    "    probe_res = get_probe_result(\n",
    "        sample_df = probe_sample_df[(probe_sample_df['role'].isin(test_roles)) & (probe_sample_df['token_in_seg_ix'] >= SKIP_FIRST_N)].reset_index(drop = True),\n",
    "        layer_hs = all_probe_hs[test_layer_ix],\n",
    "        roles_map = {x: i for i, x in enumerate(test_roles)},\n",
    "        add_scaling = add_scaling,\n",
    "        C = C\n",
    "    )\n",
    "    return {'C': C, 'add_scaling': add_scaling, 'val_acc': probe_res['acc'], 'val_nll': probe_res['nll']}\n",
    "\n",
    "clear_all_cuda_memory()\n",
    "gc.collect()\n",
    "\n",
    "# for scale_val in [False]:\n",
    "#     print(f\"Scaling: {scale_val}\")\n",
    "#     display(pd.DataFrame([test_c(c_val, add_scaling = scale_val) for c_val in tqdm([1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run role probes at every fourth layer for role combinations specified in all_role_combinations (excl CoT role for non-reasoning models)\n",
    "\"\"\"\n",
    "train_params = yaml.safe_load(open('./../role-analysis/config/probe.yaml'))[model_prefix]['train_params']\n",
    "\n",
    "all_role_combinations = [\n",
    "    ('user', 'assistant'),\n",
    "    ('user', 'assistant', 'tool'),\n",
    "]\n",
    "\n",
    "if model_prefix in ['gptoss-20b']:\n",
    "    all_role_combinations += [\n",
    "        ('user', 'cot', 'assistant'),\n",
    "        ('user', 'cot', 'assistant', 'tool'),\n",
    "        # ('system', 'user', 'assistant'),\n",
    "        # ('system', 'user', 'assistant', 'tool'),\n",
    "        ('system', 'user', 'cot', 'assistant'),\n",
    "        ('system', 'user', 'cot', 'assistant', 'tool'),\n",
    "    ]\n",
    "\n",
    "all_role_combinations = [\n",
    "    {\n",
    "        'roles': list(roles),\n",
    "        'roles_map': {x: i for i, x in enumerate(roles)},\n",
    "        # Sample df for only those roles - filtering here is fine since we retain sample_ix which get_probe_result() uses to trace the original token\n",
    "        'sample_df': probe_sample_df.pipe(lambda df: df[(df['role'].isin(roles)) & (df['token_in_seg_ix'] >= SKIP_FIRST_N)]).reset_index(drop = True),\n",
    "    }\n",
    "    for roles in all_role_combinations\n",
    "]\n",
    "\n",
    "clear_all_cuda_memory()\n",
    "gc.collect()\n",
    "\n",
    "all_probes = []\n",
    "for roles_dict in all_role_combinations:\n",
    "    print(f\"Training {roles_dict['roles']}\")\n",
    "    for layer_ix in layers_to_probe:\n",
    "        probe_res = get_probe_result(\n",
    "            sample_df = roles_dict['sample_df'],\n",
    "            layer_hs = all_probe_hs[layer_ix],\n",
    "            roles_map = roles_dict['roles_map'],\n",
    "            add_scaling = train_params['add_scaling'],\n",
    "            C = train_params['C']\n",
    "        )\n",
    "        print(f\"  Layer [{layer_ix}]: {probe_res['acc']:.2f}\")\n",
    "\n",
    "        all_probes.append({\n",
    "            **probe_res,\n",
    "            'layer_ix': layer_ix,\n",
    "            'role_space': roles_dict['roles'],\n",
    "            'roles_map': roles_dict['roles_map'],\n",
    "            'n_inputs': len(roles_dict['sample_df'])\n",
    "        })\n",
    "\n",
    "print(f\"Num probes: {str(len(all_probes))}\")\n",
    "print(f\"Probe layers:\\n  {', '.join([str(x) for x in layers_to_probe])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save probes + metrics\n",
    "\"\"\"\n",
    "def validate_accuracy_and_save(all_probes):\n",
    "    \"\"\"\n",
    "    Calculate accurcay by (rolespace, layer, role) and save\n",
    "    \"\"\"\n",
    "    print('Val accuracy by layer:')\n",
    "    display(\n",
    "        pd.DataFrame(all_probes)[['layer_ix', 'role_space', 'acc']]\\\n",
    "            .assign(acc = lambda df: df['acc'].round(2), role_space = lambda df: df['role_space'].apply(lambda x: ','.join(([r[0] for r in x]))))\\\n",
    "            .pivot(index = 'layer_ix', columns = 'role_space', values = 'acc')\n",
    "    )\n",
    "\n",
    "    print('Concatenating across layers and saving...')\n",
    "    acc_by_role = pd.concat([p['acc_by_role'].assign(model = model_prefix, layer_ix = p['layer_ix'], role_space = ','.join(p['role_space'])) for p in all_probes], ignore_index = True)\n",
    "\n",
    "    with open(f'{ws}/experiments/steer-test/outputs/probes/{model_prefix}.pkl', 'wb') as f:\n",
    "        pickle.dump(all_probes, f)\n",
    "\n",
    "    print('Accuracy by role:')\n",
    "    base_sums = acc_by_role.groupby(['role', 'layer_ix', 'role_space'], as_index = False).agg(base_sum = ('count', 'sum'))\n",
    "\n",
    "    acc_by_role =\\\n",
    "        acc_by_role\\\n",
    "        .pipe(lambda df: df[df['role'] == df['pred']])\\\n",
    "        .groupby(['role', 'layer_ix', 'role_space'], as_index = False)\\\n",
    "        .agg(sum = ('count', 'sum'))\\\n",
    "        .merge(base_sums, on = ['layer_ix', 'role_space', 'role'], how = 'inner')\\\n",
    "        .assign(acc = lambda df: df['sum']/df['base_sum'])\\\n",
    "        .pivot(index = ['role_space', 'layer_ix'], columns = 'role', values = 'acc')\\\n",
    "        .round(2)\n",
    "\n",
    "    display(acc_by_role)\n",
    "\n",
    "    return True\n",
    "\n",
    "validate_accuracy_and_save(all_probes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

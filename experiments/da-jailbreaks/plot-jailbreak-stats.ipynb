{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6d6564",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Plot jailbreak eval stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5c8db4",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(fs)\n",
    "# library(ggtext)\n",
    "library(systemfonts)\n",
    "\n",
    "ws = '/workspace/deliberative-alignment-jailbreaks'\n",
    "\n",
    "source(paste0(ws, '/r-utils/plots.r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a956321",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e919e728",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "base_gpt_oss_df =\n",
    "    read_csv(file.path(ws, 'experiments/da-jailbreaks/base-harmful-responses-classified.csv')) %>%\n",
    "    transmute(\n",
    "        target_model = 'base-gpt-oss-20b',\n",
    "        redteam_prompt_ix, redteam_prompt, redteam_output_cot, redteam_output_final,\n",
    "        harmful_question_ix, harmful_question, harmful_question_category,\n",
    "        qualifier_type, harmful_question_with_qualifier,\n",
    "        synthetic_policy_model, synthetic_policy, policy_style,\n",
    "        output_class\n",
    "    )\n",
    "\n",
    "alt_models_df =\n",
    "    read_csv(file.path(ws, 'experiments/da-jailbreaks/openrouter-generations/harmful-responses-classified.csv')) %>%\n",
    "    transmute(\n",
    "        target_model = str_replace(target_model, '.*/', ''),\n",
    "        redteam_prompt_ix, redteam_prompt, redteam_output_cot, redteam_output_final,\n",
    "        harmful_question_ix, harmful_question, harmful_question_category,\n",
    "        qualifier_type, harmful_question_with_qualifier,\n",
    "        synthetic_policy_model, synthetic_policy, policy_style,\n",
    "        output_class\n",
    "    )\n",
    "    \n",
    "prompts_df =\n",
    "    bind_rows(base_gpt_oss_df, alt_models_df) %>%\n",
    "    mutate(., target_model = fct_relevel(\n",
    "        target_model, 'base-gpt-oss-20b', 'gpt-oss-20b', 'gpt-oss-120b', 'o4-mini', 'gpt-5-nano', 'gpt-5-mini', 'gpt-5'\n",
    "        )) %>%\n",
    "    mutate(row_ix = 1:nrow(.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d199624f",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dd3615",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "model_ptype_class_df =\n",
    "    prompts_df %>%\n",
    "    filter(., policy_style != 'destyled') %>%\n",
    "    filter(., qualifier_type %in% c('no_qualifier', 'lucky_coin', 'green_shirt')) %>%\n",
    "    mutate(., qualifier_type = ifelse(qualifier_type != 'no_qualifier', 'has_qualifier', 'no_qualifier')) %>%\n",
    "    mutate(., prompt_type = case_when(\n",
    "        policy_style == 'no_policy' ~ 'no_policy',\n",
    "        policy_style == 'base' & qualifier_type == 'no_qualifier' ~ 'cot_forgery',\n",
    "        policy_style == 'base' & qualifier_type == 'has_qualifier' ~ 'cot_forgery_with_qualifier'\n",
    "    )) %>%\n",
    "    # filter(., prompt_type %in% c('no_policy', 'cot_forgery')) %>% \n",
    "    mutate(., prompt_type = fct_relevel(prompt_type, 'no_policy', 'cot_forgery', 'cot_forgery_with_qualifier')) %>%\n",
    "    group_by(target_model, prompt_type, output_class) %>%\n",
    "    summarize(., n = n(), .groups = 'drop') %>%\n",
    "    pivot_wider(., names_from = output_class, values_from = n, values_fill = 0) %>%\n",
    "    mutate(., asr = HARMFUL_RESPONSE / (HARMFUL_RESPONSE + REDIRECTION + REFUSAL))\n",
    "\n",
    "color_mappings = c(\n",
    "    'no_policy' = '#00bcff',\n",
    "    'cot_forgery' = '#ff637e',\n",
    "    'cot_forgery_with_qualifier' = '#fd9a00'\n",
    ")\n",
    "\n",
    "name_mappings = c(\n",
    "    \"no_policy\" = 'Base harmful prompt',\n",
    "    \"cot_forgery\" = \"Harmful prompt with CoT Forgery\",\n",
    "    \"cot_forgery_with_qualifier\" = \"Harmful prompt with CoT Forgery (Green Shirt/Lucky Coin)\"\n",
    ")\n",
    "\n",
    "plot =\n",
    "    model_ptype_class_df %>%\n",
    "    ggplot() +\n",
    "    geom_col(\n",
    "        aes(x = target_model, y = asr, fill = prompt_type, group = prompt_type),\n",
    "        position = position_dodge(width = 0.9),\n",
    "        stat = 'identity'\n",
    "    ) +\n",
    "    geom_text(\n",
    "        aes(x = target_model, y = asr, label = scales::percent(asr, accuracy = .1), group = prompt_type),\n",
    "        position = position_dodge(width = 0.9),\n",
    "        vjust = -0.4,\n",
    "        size = 3.0\n",
    "    ) +\n",
    "    scale_y_continuous(\n",
    "        labels = scales::percent_format(accuracy = 1),\n",
    "        limits = c(0, 1),\n",
    "        expand = expansion(mult = c(0, 0.02))\n",
    "    ) +\n",
    "    scale_fill_manual(\n",
    "        values = color_mappings,\n",
    "        labels = name_mappings\n",
    "    ) +\n",
    "    labs(\n",
    "        x = 'Target Model',\n",
    "        y = 'Attack Success Rate',\n",
    "        fill = 'Qualifier Type'\n",
    "    ) +\n",
    "    theme_iclr(base_size = 11) +\n",
    "    theme(\n",
    "        legend.position = 'top',\n",
    "        axis.title.y = ggtext::element_markdown(angle = 90, vjust = 0.5, margin = margin(r = 6))\n",
    "    )    \n",
    "\n",
    "ggsave(\n",
    "    str_glue('{ws}/experiments/da-jailbreaks/plots/basic-eval-result.pdf'),\n",
    "    plot = plot, width = 7, height = 3.0, units = 'in', dpi = 300, device = cairo_pdf\n",
    ")\n",
    "ggsave(\n",
    "    str_glue('{ws}/experiments/da-jailbreaks/plots/basic-eval-result.png'),\n",
    "    plot = plot,  width = 7, height = 3.0, units = \"in\", dpi = 300\n",
    ")\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d292a8",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "prompts_df %>%\n",
    "    filter(., policy_style %in% c('base', 'no_policy')) %>%\n",
    "    filter(., qualifier_type %in% c('no_qualifier', 'lucky_coin', 'green_shirt')) %>%\n",
    "    mutate(., qualifier_type = ifelse(qualifier_type != 'no_qualifier', 'has_qualifier', 'no_qualifier')) %>%\n",
    "    mutate(., prompt_type = case_when(\n",
    "        policy_style == 'no_policy' ~ 'no_policy',\n",
    "        policy_style == 'base' & qualifier_type == 'no_qualifier' ~ 'cot_forgery',\n",
    "        policy_style == 'base' & qualifier_type == 'has_qualifier' ~ 'cot_forgery_with_qualifier'\n",
    "    )) %>%\n",
    "    filter(., prompt_type %in% c('no_policy', 'cot_forgery')) %>%\n",
    "    group_by(target_model, prompt_type, output_class) %>%\n",
    "    summarize(., n = n(), .groups = 'drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fad5e5f",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "prompts_df %>%\n",
    "    select(qualifier_type, policy_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724406c0",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train probes\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import importlib\n",
    "import cupy\n",
    "import yaml\n",
    "from utils.dataset import ReconstructableTextDataset, stack_collate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.loader import load_model_and_tokenizer, load_custom_forward_pass\n",
    "from utils.probes import run_and_export_states, run_projections, check_max_seq_len\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 123\n",
    "\n",
    "ws = '/workspace/deliberative-alignment-jailbreaks'\n",
    "\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "model_prefix = 'gptoss-20b'\n",
    "tokenizer, model, model_architecture, model_n_layers = load_model_and_tokenizer(model_prefix, device = main_device)\n",
    "\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load custom forward pass and verify equality to base model forward pass\n",
    "\"\"\"\n",
    "run_forward_with_hs = load_custom_forward_pass(model_architecture, model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_MODEL = 'gptoss-20b'\n",
    "TEST_LAYER_IX = 12\n",
    "TEST_ROLE_SPACE = ['system', 'user', 'cot', 'assistant']\n",
    "\n",
    "with open(f'{ws}/experiments/role-analysis/outputs/probes/{TEST_MODEL}.pkl', 'rb') as f:\n",
    "    probes = pickle.load(f)\n",
    "\n",
    "probe = [p for p in probes if p['layer_ix'] == TEST_LAYER_IX and p['role_space'] == TEST_ROLE_SPACE][0]\n",
    "probe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic position analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define a lexicon\n",
    "\"\"\"\n",
    "test_prefix = yaml.safe_load(open('./../role-analysis/config/probe.yaml'))[model_prefix]['test_prefix']\n",
    "raw_convs = pd.read_csv(f'{ws}/experiments/role-analysis/data/conversations/{TEST_MODEL}.csv')\n",
    "\n",
    "prompt_x_role_df = pd.DataFrame([\n",
    "    {'prompt_ix': prompt_ix, 'role': role, 'content': row[col]}\n",
    "    for prompt_ix, row in enumerate(raw_convs.head(200).to_dict('records'))\n",
    "    for role, col in [('user', 'user_query'), ('cot', 'cot'), ('assistant', 'assistant')]\n",
    "])\n",
    "\n",
    "prompt_x_role_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_df = (\n",
    "    prompt_x_role_df.groupby('prompt_ix')['content']\n",
    "    .apply(lambda x: tokenizer.bos_token + ' '.join(x.sample(frac = 1)))\n",
    "    .reset_index(name = 'content')\n",
    ")\n",
    "\n",
    "prompt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run forward passes & projections\n",
    "\"\"\"\n",
    "MAX_INPUT_LEN = 1024\n",
    "\n",
    "input_dl = DataLoader(\n",
    "    ReconstructableTextDataset(prompt_df['content'].tolist(), tokenizer, max_length = MAX_INPUT_LEN, prompt_ix = list(range(0, len(prompt_df)))),\n",
    "    batch_size = 8,\n",
    "    shuffle = False,\n",
    "    collate_fn = stack_collate\n",
    ")\n",
    "\n",
    "run_result = run_and_export_states(model, tokenizer, run_model_return_states = run_forward_with_hs, dl = input_dl, layers_to_keep_acts = [TEST_LAYER_IX])\n",
    "hs = run_result['all_hs'].to(torch.float16)\n",
    "hs = {layer_ix: hs[:, save_ix, :] for save_ix, layer_ix in enumerate([TEST_LAYER_IX])}\n",
    "sample_df = run_result['sample_df'].assign(sample_ix = lambda df: range(0, len(df)))\n",
    "projections_df = run_projections(valid_sample_df = sample_df, layer_hs = hs[TEST_LAYER_IX], probe = probe)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Merge and visualize across token_ix\n",
    "\"\"\"\n",
    "rolespace_df =\\\n",
    "    sample_df\\\n",
    "    .assign(token_in_prompt_ix = lambda df: df.groupby(['prompt_ix']).cumcount())\\\n",
    "    .merge(projections_df, on = 'sample_ix', how = 'inner')\\\n",
    "    .groupby(['token_in_prompt_ix', 'target_role'], as_index = False)\\\n",
    "    .agg(prob = ('prob', lambda x: x.mean()))\n",
    "\n",
    "rolespace_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(rolespace_df, x = 'token_in_prompt_ix', y = 'prob', color = 'target_role')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assign test\n",
    "\"\"\"\n",
    "prompt_df = (\n",
    "    prompt_x_role_df.groupby('prompt_ix')['content']\n",
    "    .apply(lambda x: tokenizer.bos_token + ' '.join(x.sample(frac = 1)))\n",
    "    .reset_index(name = 'content')\n",
    ")\n",
    "\n",
    "prompt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare prompts with inserted system prompt\n",
    "\"\"\"\n",
    "INSERT_POS = 100\n",
    "prefix_text = \"\"\"<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\\nKnowledge cutoff: 2024-06\\nCurrent date: 2025-12-31\\n\\nReasoning: medium\\n\\n# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|>\"\"\"\n",
    "prefix_ids = tokenizer.encode(prefix_text, add_special_tokens = False)\n",
    "\n",
    "new_prompts = []\n",
    "for prompt in prompt_df.to_dict('records'):\n",
    "    base_prompt = prompt['content']\n",
    "    tokens = tokenizer.encode(base_prompt, add_special_tokens = False)\n",
    "    if len(tokens) <= INSERT_POS:\n",
    "        continue     \n",
    "    updated_tokens = tokens[:INSERT_POS] + prefix_ids + tokens[INSERT_POS:]\n",
    "    \n",
    "    # 3. Decode back to string\n",
    "    new_string = tokenizer.decode(updated_tokens)\n",
    "    new_prompts.append(new_string)\n",
    "    \n",
    "new_prompts_df = pd.DataFrame({'content': new_prompts}).assign(prompt_ix = lambda df: range(0, len(df)))\n",
    "new_prompts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prefix_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run forward passes\n",
    "\"\"\"\n",
    "MAX_INPUT_LEN = 250\n",
    "\n",
    "input_dl = DataLoader(\n",
    "    ReconstructableTextDataset(new_prompts_df['content'].tolist(), tokenizer, max_length = MAX_INPUT_LEN, prompt_ix = list(range(0, len(new_prompts_df)))),\n",
    "    batch_size = 8,\n",
    "    shuffle = False,\n",
    "    collate_fn = stack_collate\n",
    ")\n",
    "\n",
    "run_result = run_and_export_states(model, tokenizer, run_model_return_states = run_forward_with_hs, dl = input_dl, layers_to_keep_acts = [TEST_LAYER_IX])\n",
    "hs = run_result['all_hs'].to(torch.float16)\n",
    "hs = {layer_ix: hs[:, save_ix, :] for save_ix, layer_ix in enumerate([TEST_LAYER_IX])}\n",
    "sample_df = run_result['sample_df'].assign(sample_ix = lambda df: range(0, len(df)))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projections_df = run_projections(valid_sample_df = sample_df, layer_hs = hs[TEST_LAYER_IX], probe = probe)\n",
    "\n",
    "rolespace_df =\\\n",
    "    sample_df\\\n",
    "    .assign(token_in_prompt_ix = lambda df: df.groupby(['prompt_ix']).cumcount())\\\n",
    "    .merge(projections_df, on = 'sample_ix', how = 'inner')\n",
    "\n",
    "rolespace_avg =\\\n",
    "    rolespace_df\\\n",
    "    .groupby(['token_in_prompt_ix', 'target_role'], as_index = False)\\\n",
    "    .agg(prob = ('prob', lambda x: x.mean()))\n",
    "\n",
    "rolespace_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(rolespace_avg, x = 'token_in_prompt_ix', y = 'prob', color = 'target_role')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolespace_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolespace_df.to_csv(f'{ws}/experiments/position-analysis/outputs/rolespace.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare prompts with inserted system prompt\n",
    "\"\"\"\n",
    "INSERT_POS = 1\n",
    "prefix_text = \"\"\"<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\\nKnowledge cutoff: 2024-06\\nCurrent date: 2025-12-31\\n\\nReasoning: medium\\n\\n# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|>\"\"\"\n",
    "prefix_ids = tokenizer.encode(prefix_text, add_special_tokens = False)\n",
    "\n",
    "new_prompts = []\n",
    "for prompt in prompt_df.to_dict('records'):\n",
    "    base_prompt = prompt['content']\n",
    "    tokens = tokenizer.encode(base_prompt, add_special_tokens = False)\n",
    "    if len(tokens) <= INSERT_POS:\n",
    "        continue     \n",
    "    updated_tokens = tokens[:INSERT_POS] + prefix_ids + tokens[INSERT_POS:]\n",
    "    \n",
    "    # 3. Decode back to string\n",
    "    new_string = tokenizer.decode(updated_tokens)\n",
    "    new_prompts.append(new_string)\n",
    "    \n",
    "new_prompts_df = pd.DataFrame({'content': new_prompts}).assign(prompt_ix = lambda df: range(0, len(df)))\n",
    "new_prompts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run forward passes\n",
    "\"\"\"\n",
    "MAX_INPUT_LEN = 250\n",
    "\n",
    "input_dl = DataLoader(\n",
    "    ReconstructableTextDataset(new_prompts_df['content'].tolist(), tokenizer, max_length = MAX_INPUT_LEN, prompt_ix = list(range(0, len(new_prompts_df)))),\n",
    "    batch_size = 8,\n",
    "    shuffle = False,\n",
    "    collate_fn = stack_collate\n",
    ")\n",
    "\n",
    "run_result = run_and_export_states(model, tokenizer, run_model_return_states = run_forward_with_hs, dl = input_dl, layers_to_keep_acts = [TEST_LAYER_IX])\n",
    "hs = run_result['all_hs'].to(torch.float16)\n",
    "hs = {layer_ix: hs[:, save_ix, :] for save_ix, layer_ix in enumerate([TEST_LAYER_IX])}\n",
    "sample_df = run_result['sample_df'].assign(sample_ix = lambda df: range(0, len(df)))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projections_df = run_projections(valid_sample_df = sample_df, layer_hs = hs[TEST_LAYER_IX], probe = probe)\n",
    "\n",
    "rolespace_df =\\\n",
    "    sample_df\\\n",
    "    .assign(token_in_prompt_ix = lambda df: df.groupby(['prompt_ix']).cumcount())\\\n",
    "    .merge(projections_df, on = 'sample_ix', how = 'inner')\n",
    "\n",
    "rolespace_avg =\\\n",
    "    rolespace_df\\\n",
    "    .groupby(['token_in_prompt_ix', 'target_role'], as_index = False)\\\n",
    "    .agg(prob = ('prob', lambda x: x.mean()))\n",
    "\n",
    "rolespace_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(rolespace_avg, x = 'token_in_prompt_ix', y = 'prob', color = 'target_role')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolespace_df.to_csv(f'{ws}/experiments/position-analysis/outputs/rolespace-early.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define a lexicon\n",
    "\"\"\"\n",
    "MAX_ROWS = 25\n",
    "\n",
    "test_prefix = yaml.safe_load(open('./../role-analysis/config/probe.yaml'))[model_prefix]['test_prefix']\n",
    "raw_convs = pd.read_csv(f'{ws}/experiments/role-analysis/data/conversations/{TEST_MODEL}.csv')\n",
    "\n",
    "lexicon_df = pd.DataFrame([\n",
    "    {'role': role, 'content': test_prefix + ' ' + ' '.join([row[col]] * 20)}\n",
    "    for row in raw_convs.head(MAX_ROWS).to_dict('records')\n",
    "    for role, col in [('user', 'user_query'), ('cot', 'cot'), ('assistant', 'assistant')]\n",
    "])\n",
    "\n",
    "lexicon_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prep\n",
    "\"\"\"\n",
    "from utils.dataset import ReconstructableTextDataset, stack_collate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "MAX_INPUT_LEN = 768\n",
    "\n",
    "input_dl = DataLoader(\n",
    "    ReconstructableTextDataset(lexicon_df['content'].tolist(), tokenizer, max_length = MAX_INPUT_LEN, prompt_ix = list(range(0, len(lexicon_df)))),\n",
    "    batch_size = 8,\n",
    "    shuffle = False,\n",
    "    collate_fn = stack_collate\n",
    ")\n",
    "\n",
    "run_result = run_and_export_states(model, tokenizer, run_model_return_states = run_forward_with_hs, dl = input_dl, layers_to_keep_acts = [TEST_LAYER_IX])\n",
    "hs = run_result['all_hs'].to(torch.float16)\n",
    "hs = {layer_ix: hs[:, save_ix, :] for save_ix, layer_ix in enumerate([TEST_LAYER_IX])}\n",
    "sample_df = run_result['sample_df'].assign(sample_ix = lambda df: range(0, len(df)))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Role space projections\n",
    "\"\"\"\n",
    "def run_projections(valid_sample_df, layer_hs, probe):\n",
    "    \"\"\"\n",
    "    Run probe-level projections\n",
    "    \n",
    "    Params:\n",
    "        @valid_sample_df: A sample-level df with columns `sample_ix` (1... T - 1), `sample_ix`.\n",
    "            Can be shorter than full T - 1 due to pre-filters, as long as sample_ix corresponds to the full length.\n",
    "        @layer_hs: A tensor of size T x D for the layer to project.\n",
    "        @probe: The probe dict with keys `probe` (the trained model) and `role_space` (the roles list)\n",
    "    \n",
    "    Returns:\n",
    "        A df at (sample_ix, target_role) level with cols `sample_ix`, `target_role`, `prob`\n",
    "    \"\"\"\n",
    "    x_cp = cupy.asarray(layer_hs[valid_sample_df['sample_ix'].tolist(), :])\n",
    "    y_cp = probe['probe'].predict_proba(x_cp).round(12)\n",
    "\n",
    "    proj_results = pd.DataFrame(cupy.asnumpy(y_cp), columns = probe['role_space'])\n",
    "    if len(proj_results) != len(valid_sample_df):\n",
    "        raise Exception(\"Error!\")\n",
    "\n",
    "    role_df =\\\n",
    "        pd.concat([\n",
    "            proj_results.reset_index(drop = True),\n",
    "            valid_sample_df[['sample_ix']].reset_index(drop = True)\n",
    "        ], axis = 1)\\\n",
    "        .melt(id_vars = ['sample_ix'], var_name = 'target_role', value_name = 'prob')\\\n",
    "        .reset_index(drop = True)\\\n",
    "        .assign(prob = lambda df: df['prob'].round(4))\n",
    "\n",
    "    return role_df\n",
    "\n",
    "output_projections =\\\n",
    "    run_projections(valid_sample_df = sample_df, layer_hs = hs[TEST_LAYER_IX], probe = probe)\\\n",
    "    .merge(\n",
    "        sample_df[['prompt_ix', 'sample_ix', 'token']].assign(token_in_prompt_ix = lambda df: df.groupby(['prompt_ix']).cumcount()), \n",
    "        how = 'inner',\n",
    "        on = ['sample_ix']\n",
    "    )\\\n",
    "    .merge(lexicon_df.assign(prompt_ix = lambda df: range(0, len(df))), how = 'inner', on = 'prompt_ix')\\\n",
    "    .assign(rollprob = lambda df: df.groupby(['prompt_ix', 'target_role'])['prob'].transform(lambda x: x.ewm(alpha = 0.25).mean()))\n",
    "\n",
    "output_projections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = output_projections['target_role'].unique().tolist()\n",
    "\n",
    "output_agg =\\\n",
    "    output_projections\\\n",
    "    .groupby(['token_in_prompt_ix', 'target_role', 'role'], as_index = False)\\\n",
    "    .agg(mean_rollprob = ('rollprob', lambda x: x.mean()))\n",
    "    \n",
    "output_agg\\\n",
    "    .pivot(index = ['role', 'token_in_prompt_ix'], columns = 'target_role', values = 'mean_rollprob')\\\n",
    "    .reset_index()\\\n",
    "    .pipe(lambda df: df[df['token_in_prompt_ix'] >= MAX_INPUT_LEN - 10])\n",
    "\n",
    "# .to_csv('lexical_role_projections.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(output_agg, x = 'token_in_prompt_ix', y = 'mean_rollprob', title = 'role', color = 'target_role', facet_col = 'role')\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

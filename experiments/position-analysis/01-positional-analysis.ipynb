{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train probes\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import importlib\n",
    "import cupy\n",
    "import yaml\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.loader import load_model_and_tokenizer, load_custom_forward_pass\n",
    "from utils.probes import run_and_export_states, run_projections, check_max_seq_len\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 123\n",
    "\n",
    "ws = '/workspace/deliberative-alignment-jailbreaks'\n",
    "\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "model_prefix = 'gptoss-20b'\n",
    "tokenizer, model, model_architecture, model_n_layers = load_model_and_tokenizer(model_prefix, device = main_device)\n",
    "\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load custom forward pass and verify equality to base model forward pass\n",
    "\"\"\"\n",
    "run_forward_with_hs = load_custom_forward_pass(model_architecture, model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_MODEL = 'gptoss-20b'\n",
    "TEST_LAYER_IX = 12\n",
    "TEST_ROLE_SPACE = ['system', 'user', 'cot', 'assistant']\n",
    "\n",
    "with open(f'{ws}/experiments/role-analysis/outputs/probes/{TEST_MODEL}.pkl', 'rb') as f:\n",
    "    probes = pickle.load(f)\n",
    "\n",
    "probe = [p for p in probes if p['layer_ix'] == TEST_LAYER_IX and p['role_space'] == TEST_ROLE_SPACE][0]\n",
    "probe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset and collect states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define a lexicon\n",
    "\"\"\"\n",
    "MAX_ROWS = 25\n",
    "\n",
    "test_prefix = yaml.safe_load(open('./../role-analysis/config/probe.yaml'))[model_prefix]['test_prefix']\n",
    "raw_convs = pd.read_csv(f'{ws}/experiments/role-analysis/data/conversations/{TEST_MODEL}.csv')\n",
    "\n",
    "lexicon_df = pd.DataFrame([\n",
    "    {'role': role, 'content': test_prefix + ' ' + ' '.join([row[col]] * 20)}\n",
    "    for row in raw_convs.head(MAX_ROWS).to_dict('records')\n",
    "    for role, col in [('user', 'user_query'), ('cot', 'cot'), ('assistant', 'assistant')]\n",
    "])\n",
    "\n",
    "lexicon_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prep\n",
    "\"\"\"\n",
    "from utils.dataset import ReconstructableTextDataset, stack_collate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "MAX_INPUT_LEN = 768\n",
    "\n",
    "input_dl = DataLoader(\n",
    "    ReconstructableTextDataset(lexicon_df['content'].tolist(), tokenizer, max_length = MAX_INPUT_LEN, prompt_ix = list(range(0, len(lexicon_df)))),\n",
    "    batch_size = 8,\n",
    "    shuffle = False,\n",
    "    collate_fn = stack_collate\n",
    ")\n",
    "\n",
    "run_result = run_and_export_states(model, tokenizer, run_model_return_states = run_forward_with_hs, dl = input_dl, layers_to_keep_acts = [TEST_LAYER_IX])\n",
    "hs = run_result['all_hs'].to(torch.float16)\n",
    "hs = {layer_ix: hs[:, save_ix, :] for save_ix, layer_ix in enumerate([TEST_LAYER_IX])}\n",
    "sample_df = run_result['sample_df'].assign(sample_ix = lambda df: range(0, len(df)))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Role space projections\n",
    "\"\"\"\n",
    "def run_projections(valid_sample_df, layer_hs, probe):\n",
    "    \"\"\"\n",
    "    Run probe-level projections\n",
    "    \n",
    "    Params:\n",
    "        @valid_sample_df: A sample-level df with columns `sample_ix` (1... T - 1), `sample_ix`.\n",
    "            Can be shorter than full T - 1 due to pre-filters, as long as sample_ix corresponds to the full length.\n",
    "        @layer_hs: A tensor of size T x D for the layer to project.\n",
    "        @probe: The probe dict with keys `probe` (the trained model) and `role_space` (the roles list)\n",
    "    \n",
    "    Returns:\n",
    "        A df at (sample_ix, target_role) level with cols `sample_ix`, `target_role`, `prob`\n",
    "    \"\"\"\n",
    "    x_cp = cupy.asarray(layer_hs[valid_sample_df['sample_ix'].tolist(), :])\n",
    "    y_cp = probe['probe'].predict_proba(x_cp).round(12)\n",
    "\n",
    "    proj_results = pd.DataFrame(cupy.asnumpy(y_cp), columns = probe['role_space'])\n",
    "    if len(proj_results) != len(valid_sample_df):\n",
    "        raise Exception(\"Error!\")\n",
    "\n",
    "    role_df =\\\n",
    "        pd.concat([\n",
    "            proj_results.reset_index(drop = True),\n",
    "            valid_sample_df[['sample_ix']].reset_index(drop = True)\n",
    "        ], axis = 1)\\\n",
    "        .melt(id_vars = ['sample_ix'], var_name = 'target_role', value_name = 'prob')\\\n",
    "        .reset_index(drop = True)\\\n",
    "        .assign(prob = lambda df: df['prob'].round(4))\n",
    "\n",
    "    return role_df\n",
    "\n",
    "output_projections =\\\n",
    "    run_projections(valid_sample_df = sample_df, layer_hs = hs[TEST_LAYER_IX], probe = probe)\\\n",
    "    .merge(\n",
    "        sample_df[['prompt_ix', 'sample_ix', 'token']].assign(token_in_prompt_ix = lambda df: df.groupby(['prompt_ix']).cumcount()), \n",
    "        how = 'inner',\n",
    "        on = ['sample_ix']\n",
    "    )\\\n",
    "    .merge(lexicon_df.assign(prompt_ix = lambda df: range(0, len(df))), how = 'inner', on = 'prompt_ix')\\\n",
    "    .assign(rollprob = lambda df: df.groupby(['prompt_ix', 'target_role'])['prob'].transform(lambda x: x.ewm(alpha = 0.25).mean()))\n",
    "\n",
    "output_projections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = output_projections['target_role'].unique().tolist()\n",
    "\n",
    "output_agg =\\\n",
    "    output_projections\\\n",
    "    .groupby(['token_in_prompt_ix', 'target_role', 'role'], as_index = False)\\\n",
    "    .agg(mean_rollprob = ('rollprob', lambda x: x.mean()))\n",
    "    \n",
    "output_agg\\\n",
    "    .pivot(index = ['role', 'token_in_prompt_ix'], columns = 'target_role', values = 'mean_rollprob')\\\n",
    "    .reset_index()\\\n",
    "    .pipe(lambda df: df[df['token_in_prompt_ix'] >= MAX_INPUT_LEN - 10])\n",
    "\n",
    "# .to_csv('lexical_role_projections.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(output_agg, x = 'token_in_prompt_ix', y = 'mean_rollprob', title = 'role', color = 'target_role', facet_col = 'role')\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

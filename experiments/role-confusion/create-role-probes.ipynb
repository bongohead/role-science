{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4beade33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test role confusion with hidden states\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb75c7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cupy\n",
    "import cuml\n",
    "import importlib\n",
    "import gc\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.role_assignments import label_content_roles\n",
    "from utils.role_templates import load_chat_template\n",
    "from utils.store_outputs import convert_outputs_to_df_fast\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "\n",
    "clear_all_cuda_memory()\n",
    "check_memory()\n",
    "\n",
    "ws = '/workspace/deliberative-alignment-jailbreaks'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed26db3f",
   "metadata": {},
   "source": [
    "## Load models & data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75666ea5",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80af64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "selected_model_index = 1\n",
    "\n",
    "def get_model(index):\n",
    "    # HF model ID, model prefix, model architecture,  attn implementation, whether to use hf lib implementation\n",
    "    models = {\n",
    "        0: ('openai/gpt-oss-120b', 'gptoss120', 'gptoss', 'kernels-community/vllm-flash-attn3', True, 36), # Will load experts in MXFP4 if triton kernels installed\n",
    "        1: ('openai/gpt-oss-20b', 'gptoss20', 'gptoss', 'kernels-community/vllm-flash-attn3', True, 24),\n",
    "        2: ('Qwen/Qwen3-30B-A3B-Thinking-2507', 'qwen3moe', 'qwen3moe', None, True, 48)\n",
    "    }\n",
    "    return models[index]\n",
    "\n",
    "def load_model_and_tokenizer(model_id, model_prefix, model_attn, model_use_hf):\n",
    "    \"\"\"\n",
    "    Load the model and tokenizer from HF, or from file if already downloaded.\n",
    "    \"\"\"\n",
    "    cache_dir = '/workspace/hf'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir = cache_dir, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, cache_dir = cache_dir, dtype = torch.bfloat16, trust_remote_code = not model_use_hf, device_map = 'auto', attn_implementation = model_attn).eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "model_id, model_prefix, model_architecture, model_attn, model_use_hf, model_n_layers = get_model(selected_model_index)\n",
    "tokenizer, model = load_model_and_tokenizer(model_id, model_prefix, model_attn, model_use_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92158dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_id, model_prefix, model_attn, model_use_hf):\n",
    "    \"\"\"\n",
    "    Load the model and tokenizer from HF, or from file if already downloaded.\n",
    "    \"\"\"\n",
    "    cache_dir = '/workspace/hf'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir = cache_dir, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = load_model_and_tokenizer(model_id, model_prefix, model_attn, model_use_hf)\n",
    "\n",
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e11e85b",
   "metadata": {},
   "source": [
    "### Load probe training activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0533d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset\n",
    "\"\"\"\n",
    "def load_data(folder_path, model_prefix, max_data_files):\n",
    "    \"\"\"\n",
    "    Load data saved by `export-activations.ipynb`\n",
    "    \"\"\"\n",
    "    folders = [f'{ws}/experiments/role-confusion/{folder_path}/{model_prefix}/{i:02d}' for i in range(max_data_files)]\n",
    "    folders = [f for f in folders if os.path.isdir(f)]\n",
    "\n",
    "    all_pre_mlp_hs = []\n",
    "    sample_df = []\n",
    "    topk_df = []\n",
    "\n",
    "    for f in tqdm(folders):\n",
    "        sample_df.append(pd.read_pickle(f'{f}/samples.pkl'))\n",
    "        topk_df.append(pd.read_pickle(f'{f}/topks.pkl'))\n",
    "        all_pre_mlp_hs.append(torch.load(f'{f}/all-pre-mlp-hidden-states.pt'))\n",
    "\n",
    "    sample_df = pd.concat(sample_df)\n",
    "    topk_df = pd.concat(topk_df)\n",
    "    all_pre_mlp_hs = torch.concat(all_pre_mlp_hs)    \n",
    "\n",
    "    with open(f'{ws}/experiments/role-confusion/{folder_path}/{model_prefix}/metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    gc.collect()\n",
    "    return sample_df, topk_df, all_pre_mlp_hs, metadata['all_pre_mlp_hidden_states_layers']\n",
    "\n",
    "sample_df_import, topk_df_import, all_pre_mlp_hs_import, act_map = load_data('activations', model_prefix, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2c5908",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the valid role assignments function label_content_roles\n",
    "\"\"\"\n",
    "import importlib\n",
    "import utils.role_assignments\n",
    "import utils.role_templates\n",
    "importlib.reload(utils.role_assignments)\n",
    "label_content_roles = utils.role_assignments.label_content_roles\n",
    "importlib.reload(utils.role_templates)\n",
    "load_chat_template = utils.role_templates.load_chat_template\n",
    "\n",
    "# Load custom chat templater\n",
    "tokenizer.chat_template = load_chat_template(f'{ws}/utils/chat_templates', model_architecture)\n",
    "\n",
    "s = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {'role': 'system', 'content': 'Test.'},\n",
    "        {'role': 'user', 'content': 'Hi! I am a dog and I like to bark'},\n",
    "        {'role': 'assistant', 'content': 'I am a dog and I like to bark'}\n",
    "    ],\n",
    "    tokenize = False,\n",
    "    padding = 'max_length',\n",
    "    truncation = True,\n",
    "    max_length = 512,\n",
    "    add_generation_prompt = False\n",
    ")\n",
    "\n",
    "z =\\\n",
    "    pd.DataFrame({'input_ids': tokenizer(s)['input_ids']})\\\n",
    "    .assign(token = lambda df: tokenizer.convert_ids_to_tokens(df['input_ids']))\\\n",
    "    .assign(prompt_ix = 0, batch_ix = 0, sequence_ix = 0, token_ix = lambda df: df.index)\n",
    "\n",
    "label_content_roles(model_architecture, z).tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845a89a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's clean up the mappings here. We'll get everything to a sample_ix level first. We'll also flag the role wrapper tokens. Drop nothing. \n",
    "\"\"\"\n",
    "def clean_mappings(model_architecture, sample_df_import, topk_df_import):\n",
    "\n",
    "    # We'll read the role mappings the raw way, instead of just using the input_mappings file. Then we can re-use the same logic for later on.\n",
    "    # Technically this could be done easily with the input_mappings file (.merge(input_mappings[['prompt_ix', 'question_ix', 'role']], how = 'left', on = ['prompt_ix'])\\),\n",
    "    # but this is more robust, and doesn't assume one role per sequence/prompt_ix.\n",
    "    # input_mappings = pd.read_csv(f'{folder_path}/{model_prefix}/input_mappings.csv')\n",
    "\n",
    "    sample_df_raw =\\\n",
    "        label_content_roles(model_architecture, sample_df_import)\\\n",
    "        .assign(sample_ix = lambda df: df.groupby(['batch_ix', 'sequence_ix', 'token_ix']).ngroup())\\\n",
    "        .assign(token_in_seq_ix = lambda df: df.groupby(['prompt_ix']).cumcount())\\\n",
    "        .assign(token_in_role_ix = lambda df: df.groupby(['prompt_ix', 'role']).cumcount())\n",
    "\n",
    "    c4_topk_df =\\\n",
    "        topk_df_import\\\n",
    "        .merge(sample_df_raw[['sample_ix', 'prompt_ix', 'batch_ix', 'sequence_ix', 'token_ix']], how = 'inner', on = ['sequence_ix', 'token_ix', 'batch_ix'])\\\n",
    "        .drop(columns = ['sequence_ix', 'token_ix', 'batch_ix'])\n",
    "\n",
    "    c4_sample_df =\\\n",
    "        sample_df_raw\\\n",
    "        .drop(columns = ['batch_ix', 'sequence_ix'])\n",
    "\n",
    "    return c4_sample_df, c4_topk_df\n",
    "\n",
    "sample_df, topk_df = clean_mappings(model_architecture, sample_df_import, topk_df_import)\n",
    "del sample_df_import, topk_df_import\n",
    "\n",
    "gc.collect()\n",
    "display(sample_df)\n",
    "display(topk_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81cc749",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert activations to fp16 (for compatibility with cupy later) + layer-wise dict\n",
    "\"\"\"\n",
    "all_pre_mlp_hs = all_pre_mlp_hs_import.to(torch.float16)\n",
    "# compare_bf16_fp16_batched(all_pre_mlp_hs_import, all_pre_mlp_hs)\n",
    "del all_pre_mlp_hs_import\n",
    "all_pre_mlp_hs = {layer_ix: all_pre_mlp_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(act_map)}\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cc62db",
   "metadata": {},
   "source": [
    "## Train the probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac139eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run logistic regression on c4 to get a model for role predictions\n",
    "\"\"\"\n",
    "def run_lr(x_cp, y_cp):\n",
    "    x_train, x_test, y_train, y_test = cuml.train_test_split(x_cp, y_cp, test_size = 0.2, random_state = 123)\n",
    "    lr_model = cuml.linear_model.LogisticRegression(penalty = 'l2', C = 0.0001, max_iter = 1000, fit_intercept = True)\n",
    "    lr_model.fit(x_train, y_train)\n",
    "    accuracy = lr_model.score(x_test, y_test)\n",
    "    return lr_model, accuracy\n",
    "\n",
    "test_layer = round(model_n_layers * .75)\n",
    "print(f\"Test layer: {test_layer}\")\n",
    "label_to_id = {'system': 0, 'user': 1, 'assistant-cot': 2, 'assistant-final': 3}\n",
    "# label_to_id = {'system': 0, 'user': 1, 'assistant-cot': 2, 'assistant-final': 3, 'tool': 4}\n",
    "id_to_label = {v: k for k, v in label_to_id.items()}\n",
    "\n",
    "valid_sample_ix =\\\n",
    "    sample_df\\\n",
    "    .pipe(lambda df: df[(df['in_content_span'] == True) & (df['role'].notna()) & (df['role'].isin(list(label_to_id.keys())))])\\\n",
    "    ['sample_ix'].tolist()\\\n",
    "\n",
    "role_labels = [\n",
    "    label_to_id[role]\n",
    "    for role in sample_df[sample_df['sample_ix'].isin(valid_sample_ix)]['role'].tolist()\n",
    "]\n",
    "\n",
    "role_labels_cp = cupy.asarray(role_labels)\n",
    "x_cp = cupy.asarray(all_pre_mlp_hs[test_layer][valid_sample_ix, :].to(torch.float16).detach().cpu())\n",
    "lr_model, test_acc = run_lr(x_cp, role_labels_cp)\n",
    "\n",
    "print(test_acc)\n",
    "lr_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357ff043",
   "metadata": {},
   "source": [
    "## Run and project test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6efc781",
   "metadata": {},
   "source": [
    "### Prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d30b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We'll test some projections of different sequences\n",
    "\n",
    "To understand instruct formats for different models, see the HF chat template playground:\n",
    "    - https://huggingface.co/spaces/huggingfacejs/chat-template-playground?modelId=openai%2Fgpt-oss-120b\n",
    "\"\"\"\n",
    "from utils.dataset import ReconstructableTextDataset, stack_collate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "base_message_types = ['system', 'user-message', 'cot', 'assistant-message', 'user-message', 'cot', 'assistant-message']\n",
    "\n",
    "if model_architecture == 'gptoss':\n",
    "    base_messages = [\n",
    "        \"You are ChatGPT, a large language model trained by OpenAI.\\nKnowledge cutoff: 2024-06\\nCurrent date: 2025-10-01\\n\\nReasoning: medium\\n\\n# Valid channels: analysis, commentary, final. Channel must be included for every message.\",\n",
    "        # User-like question\n",
    "        \"In a few sentences, how do I grow tomatoes indoors?\",\n",
    "        # Assistant-CoT style\n",
    "        \"The user wants a brief answer: 2-3 sentences on how to grow tomatoes indoors. Need to include key steps like using a grow light, good pots, watering, pruning, etc. No special instructions beyond that. Provide concise, helpful answer.\",\n",
    "        # Assistant-final style\n",
    "        \"Plant seedlings in well‑draining potting mix and use serrated, 20‑inch pots or larger, or a growing tray. Keep them under a 12–16 hour, 600–800 µmol m⁻² s⁻¹ LED or fluorescent grow light at 12–18 inches above the canopy, water when the top inch dries, and fertilize every 2–3 weeks with a balanced tomato fertilizer. Prune suckers, stake or cage the plants, and rotate or reposition them each week so all sides receive light and airflow, which encourages fruit production.\",\n",
    "        # User-like question\n",
    "        \"Oh, thanks!! This is very good advice. Also, is it easier to grow them indoors or outdoors?\",\n",
    "        # Assistant-CoT style\n",
    "        \"The user asked a question: whether it is easier to grow them indoors or outdoors. So we need an answer. They previously asked short instructions on how to grow tomatoes indoors, and now they want a comparison: easier to grow them indoors or outdoors? Provide a concise answer. We should keep it brief, maybe 2-3 sentences.\",\n",
    "        # Assistant-final style\n",
    "        \"Outdoors is usually easier if you have a sunny, pest‑free spot and a long growing season—plants get plenty of natural light and you don’t pay for artificial grow lights. However, indoor growing gives you year‑round control, eliminates weather and pest risks, and can be more reliable in cooler or highly variable climates, at the cost of more setup (pots, light, watering, and fertilizer). Ultimately, the “easier” option depends on your space, climate, and how much labor you’re willing to put into maintenance.\"\n",
    "    ]\n",
    "elif model_architecture == 'qwen3moe':\n",
    "    base_messages = [\n",
    "        \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\",\n",
    "        # User-like question\n",
    "        \"In a few sentences, how do I grow tomatoes indoors?\",\n",
    "        # Assistant-CoT style\n",
    "        \"Hmm, the user is asking how to grow tomatoes indoors in just 2-3 sentences, and they want me to not overthink it. This seems like a straightforward gardening question from someone who might be a beginner or short on time.\\nI need to keep it concise and practical. The key points for indoor tomato growing are light, space, and watering. Tomatoes need lots of sun, so I'll mention a sunny windowsill or grow lights. They also need room to grow, so suggesting a container with drainage is important. And for watering, I should emphasize avoiding overwatering since that's a common mistake.\",\n",
    "        # Assistant-final style\n",
    "        \"Plant tomatoes in a large pot (at least 5 gallons) with drainage holes, using well-draining potting mix. Place them in a sunny south-facing window or under grow lights for 14+ hours daily. Water when the top inch of soil feels dry, avoiding soggy roots.\",\n",
    "        # User-like question\n",
    "        \"Oh, thanks!! This is very good advice. Also, is it easier to grow them indoors or outdoors?\",\n",
    "        # Assistant-CoT style\n",
    "        \"Okay, the user just thanked me for the indoor tomato growing advice and asked whether it's easier to grow them indoors or outdoors. Let me unpack this.\\nFirst, I notice they're clearly a beginner gardener—someone who just got practical tips and is now thinking about scaling up their efforts. Their \\\"Oh, thanks!!\\\" with double exclamation points shows genuine enthusiasm, which is great. But the question about \\\"easier\\\" is actually tricky because it depends entirely on their specific situation.\",\n",
    "        # Assistant-final style\n",
    "        \"For most people, outdoor growing is easier because tomatoes thrive on full sun (6+ hours daily) and natural pollination from wind/bees—things hard to replicate indoors without dedicated grow lights and hand-pollination. However, if you must grow indoors (e.g., no yard, short season), your setup (large pot, bright light, consistent watering) is already on the right track—just be prepared to invest in grow lights and extra effort for pollination.\"\n",
    "    ]\n",
    "else:\n",
    "    raise Exception('Unsupported model')\n",
    "\n",
    "base_prompt = '\\n'.join(base_messages)\n",
    "\n",
    "prompts = {}\n",
    "\n",
    "# Note: the Harmony formatter does split up <think> tags properly into the actual channel tags, see https://huggingface.co/spaces/huggingfacejs/chat-template-playground?modelId=openai%2Fgpt-oss-120b \n",
    "prompts['basic_no_format'] = base_prompt\n",
    "    \n",
    "prompts['everything_in_user_tags'] = tokenizer.apply_chat_template(\n",
    "    [{'role': 'user', 'content': base_prompt}],\n",
    "    tokenize = False, add_generation_prompt = False\n",
    ")\n",
    "\n",
    "if model_architecture == 'gptoss':\n",
    "    prompts['everything_in_assistant_tags'] = tokenizer.apply_chat_template(\n",
    "        [{'role': 'assistant', 'content': base_prompt}],\n",
    "        tokenize = False, add_generation_prompt = False\n",
    "    )\n",
    "elif model_architecture == 'qwen3moe':\n",
    "    prompts['everything_in_assistant_tags'] = tokenizer.apply_chat_template(\n",
    "        [{'role': 'assistant', 'content': f\"<think></think>{base_prompt}\"}],\n",
    "        tokenize = False, add_generation_prompt = False\n",
    "    )\n",
    "else:\n",
    "    raise Exception('Unsupported architecture!')\n",
    "\n",
    "prompts['proper_tags'] = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {'role': 'system', 'content': base_messages[0]},\n",
    "        {'role': 'user', 'content': base_messages[1]},\n",
    "        {'role': 'assistant', 'content': f\"<think>{base_messages[2]}</think>{base_messages[3]}\"},\n",
    "        {'role': 'user', 'content': base_messages[4]},\n",
    "        {'role': 'assistant', 'content': f\"<think>{base_messages[5]}</think>{base_messages[6]}\"}\n",
    "    ],\n",
    "    tokenize = False, add_generation_prompt = False\n",
    ")\n",
    "\n",
    "# if model_architecture == 'gptoss':\n",
    "#     prompts['proper_tags'] = tokenizer.apply_chat_template(\n",
    "#         [\n",
    "#             # {'role': 'system', 'content': base_messages[0]},\n",
    "#             {'role': 'user', 'content': base_messages[1]},\n",
    "#             {'role': 'assistant', 'content': f\"<think>\\n{base_messages[2]}\\n</think>\\n{base_messages[3]}\"},\n",
    "#             {'role': 'user', 'content': base_messages[4]},\n",
    "#             {'role': 'assistant', 'content': f\"<think>\\n{base_messages[5]}\\n</think>\\n{base_messages[6]}\"}\n",
    "#         ],\n",
    "#         tokenize = False, add_generation_prompt = False\n",
    "#     )\n",
    "# elif model_architecture == 'qwen3moe':\n",
    "#     prompts['proper_tags'] = tokenizer.apply_chat_template(\n",
    "#         [\n",
    "#             {'role': 'system', 'content': base_messages[0]},\n",
    "#             {'role': 'user', 'content': base_messages[1]},\n",
    "#             {'role': 'assistant', 'content': f\"<think>{base_messages[2]}</think>{base_messages[3]}\"},\n",
    "#             {'role': 'user', 'content': base_messages[4]},\n",
    "#             {'role': 'assistant', 'content': f\"<think>{base_messages[5]}</think>{base_messages[6]}\"}\n",
    "#         ],\n",
    "#         tokenize = False, add_generation_prompt = False\n",
    "#     )\n",
    "# else:\n",
    "#     raise Exception('Unsupported architecture!')\n",
    "\n",
    "test_input_df =\\\n",
    "    pd.DataFrame({\n",
    "        'prompt': [p for _, p in prompts.items()],\n",
    "        'prompt_key': [pk for pk, _ in prompts.items()]\n",
    "    })\\\n",
    "    .assign(prompt_ix = lambda df: list(range(0, len(df))))\n",
    "\n",
    "# Create and chunk into lists of size 500 each - these will be the export breaks\n",
    "test_dl = DataLoader(\n",
    "    ReconstructableTextDataset(test_input_df['prompt'].tolist(), tokenizer, max_length = 512, prompt_ix = test_input_df['prompt_ix'].tolist()),\n",
    "    batch_size = 16,\n",
    "    shuffle = False,\n",
    "    collate_fn = stack_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40df7af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.apply_chat_template([\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'thinking': 'The user is asking about the content in New York',\n",
    "        'content': \"<think>\\nThe user is asking about the weather in New York. I should use the weather tool to get this information.\\n</think>\\nI'll check the current weather in New York for you.\",\n",
    "    }\n",
    "], tokenize = False, add_generation_prompt = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4129f68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompts['proper_tags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049406c0",
   "metadata": {},
   "source": [
    "### Collect test activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4cabfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run forward passes\n",
    "\"\"\"\n",
    "model_module = importlib.import_module(f\"utils.pretrained_models.{model_architecture}\")\n",
    "run_model_return_topk = getattr(model_module, f\"run_{model_architecture}_return_topk\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_and_export_states(model, dl: ReconstructableTextDataset, layers_to_keep_acts: list[int]):\n",
    "    \"\"\"\n",
    "    Run forward passes on given model and store the decomposed sample_df plus hidden states\n",
    "\n",
    "    Params:\n",
    "        @model: The model to run forward passes on. Should return a dict with keys `logits`, `all_topk_experts`, `all_topk_weights`,\n",
    "          `all_pre_mlp_hidden_states`, and `all_expert_outputs`.\n",
    "        @dl: A ReconstructableTextDataset of which returns `input_ids`, `attention_mask`, and `original_tokens`.\n",
    "        @layers_to_keep_acts: A list of layer indices (0-indexed) for which to filter `all_pre_mlp_hidden_states` (see returned object description).\n",
    "\n",
    "    Returns:\n",
    "        A dict with keys:\n",
    "        - `sample_df`: A sample (token)-level dataframe with corresponding input token ID, output token ID, and input token text (removes masked tokens)\n",
    "        - `all_pre_mlp_hidden_states`: A tensor of size n_samples x layers_to_keep_acts x D return the hidden state for each retained layers. Each \n",
    "            n_sample corresponds to a row of sample_df.\n",
    "    \"\"\"\n",
    "    all_pre_mlp_hidden_states = []\n",
    "    sample_dfs = []\n",
    "\n",
    "    for batch_ix, batch in tqdm(enumerate(dl), total = len(dl)):\n",
    "\n",
    "        input_ids = batch['input_ids'].to(main_device)\n",
    "        attention_mask = batch['attention_mask'].to(main_device)\n",
    "        original_tokens = batch['original_tokens']\n",
    "        prompt_indices = batch['prompt_ix']\n",
    "\n",
    "        output = run_model_return_topk(model, input_ids, attention_mask, return_hidden_states = True)\n",
    "\n",
    "        # Check no bugs by validating output/perplexity\n",
    "        if batch == 0:\n",
    "            loss = ForCausalLMLoss(output['logits'], torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids), model.config.vocab_size).detach().cpu().item()\n",
    "            for i in range(min(20, input_ids.size(0))):\n",
    "                decoded_input = tokenizer.decode(input_ids[i, :], skip_special_tokens = False)\n",
    "                next_token_id = torch.argmax(output['logits'][i, -1, :]).item()\n",
    "                print('---------\\n' + decoded_input + colored(tokenizer.decode([next_token_id], skip_special_tokens = False).replace('\\n', '<lb>'), 'green'))\n",
    "            print(f\"PPL:\", torch.exp(torch.tensor(loss)).item())\n",
    "                \n",
    "        original_tokens_df = pd.DataFrame(\n",
    "            [(seq_i, tok_i, tok) for seq_i, tokens in enumerate(original_tokens) for tok_i, tok in enumerate(tokens)], \n",
    "            columns = ['sequence_ix', 'token_ix', 'token']\n",
    "        )\n",
    "                \n",
    "        prompt_indices_df = pd.DataFrame(\n",
    "            [(seq_i, seq_source) for seq_i, seq_source in enumerate(prompt_indices)], \n",
    "            columns = ['sequence_ix', 'prompt_ix']\n",
    "        )\n",
    "        \n",
    "        # Create sample (token) level dataframe\n",
    "        sample_df =\\\n",
    "            convert_outputs_to_df_fast(input_ids, attention_mask, output['logits'])\\\n",
    "            .merge(original_tokens_df, how = 'left', on = ['token_ix', 'sequence_ix'])\\\n",
    "            .merge(prompt_indices_df, how = 'left', on = ['sequence_ix'])\\\n",
    "            .assign(batch_ix = batch_ix)\n",
    "        \n",
    "        sample_dfs.append(sample_df)\n",
    "\n",
    "        # Store pre-MLP hidden states - the fwd pass as n_layers list as BN x D, collapse to BN x n_layers x D, with BN filtering out masked items\n",
    "        valid_pos = torch.where(attention_mask.cpu().view(-1) == 1) # Valid (BN, ) positions\n",
    "        all_pre_mlp_hidden_states.append(torch.stack(output['all_pre_mlp_hidden_states'], dim = 1)[valid_pos][:, layers_to_keep_acts, :])\n",
    "\n",
    "        batch_ix += 1\n",
    "\n",
    "    sample_df = pd.concat(sample_dfs, ignore_index = True)\n",
    "    all_pre_mlp_hidden_states = torch.cat(all_pre_mlp_hidden_states, dim = 0)\n",
    "\n",
    "    return {\n",
    "        'sample_df': sample_df,\n",
    "        'all_pre_mlp_hs': all_pre_mlp_hidden_states\n",
    "    }\n",
    "\n",
    "test_outputs = run_and_export_states(\n",
    "    model,\n",
    "    test_dl,\n",
    "    layers_to_keep_acts = list(range(model_n_layers))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56da9f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare sample df\n",
    "\"\"\"\n",
    "test_sample_df =\\\n",
    "    test_outputs['sample_df'].merge(test_input_df, on = 'prompt_ix', how = 'inner')\\\n",
    "    .assign(sample_ix = lambda df: df.groupby(['batch_ix', 'sequence_ix', 'token_ix']).ngroup())\\\n",
    "    .assign(token_in_prompt_ix = lambda df: df.groupby(['prompt_ix']).cumcount())\n",
    "\n",
    "display(test_sample_df)\n",
    "\n",
    "test_pre_mlp_hs = test_outputs['all_pre_mlp_hs'].to(torch.float16)\n",
    "test_pre_mlp_hs = {layer_ix: test_pre_mlp_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(act_map)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1e11d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Map each token base to its base_message (currently its mapped to its prompt key, but it should also be base-message mapped!)\n",
    "\"\"\"\n",
    "test_sample_df_labeled =\\\n",
    "    test_sample_df\\\n",
    "    .sort_values(['prompt_ix', 'token_ix'])\\\n",
    "    .assign(\n",
    "        _t1 = lambda d: d.groupby('prompt_ix')['token'].shift(-1),\n",
    "        _t2 = lambda d: d.groupby('prompt_ix')['token'].shift(-2),\n",
    "        _t3 = lambda d: d.groupby('prompt_ix')['token'].shift(-3),\n",
    "        _t4 = lambda d: d.groupby('prompt_ix')['token'].shift(-4),\n",
    "        _t5 = lambda d: d.groupby('prompt_ix')['token'].shift(-5),\n",
    "        _t6 = lambda d: d.groupby('prompt_ix')['token'].shift(-6),\n",
    "        _t7 = lambda d: d.groupby('prompt_ix')['token'].shift(-7),\n",
    "        _t8 = lambda d: d.groupby('prompt_ix')['token'].shift(-8),\n",
    "        _b1 = lambda d: d.groupby('prompt_ix')['token'].shift(1),\n",
    "        _b2 = lambda d: d.groupby('prompt_ix')['token'].shift(2),\n",
    "        _b3 = lambda d: d.groupby('prompt_ix')['token'].shift(3),\n",
    "        _b4 = lambda d: d.groupby('prompt_ix')['token'].shift(4),\n",
    "        _b5 = lambda d: d.groupby('prompt_ix')['token'].shift(5),\n",
    "        _b6 = lambda d: d.groupby('prompt_ix')['token'].shift(6),\n",
    "        _b7 = lambda d: d.groupby('prompt_ix')['token'].shift(7),\n",
    "        _b8 = lambda d: d.groupby('prompt_ix')['token'].shift(8)\n",
    "    )\\\n",
    "    .assign(\n",
    "        has_roll = lambda d: d[['_t1','_t2','_t3','_t4','_t5', '_t6', '_t7', '_t8']].notna().all(axis = 1),\n",
    "        has_back = lambda d: d[['_b1','_b2','_b3','_b4','_b5', '_b6', '_b7', '_b8']].notna().all(axis = 1),  \n",
    "    )\\\n",
    "    .assign(\n",
    "        tok_roll = lambda d: d['token'].fillna('') + d['_t1'].fillna('') + d['_t2'].fillna('') + d['_t3'].fillna('') + d['_t4'].fillna('') +\n",
    "            d['_t5'].fillna('')  + d['_t6'].fillna('')  + d['_t7'].fillna('')  + d['_t8'].fillna(''),\n",
    "        tok_back = lambda d: d['_b8'].fillna('') + d['_b7'].fillna('') + d['_b6'].fillna('') + d['_b5'].fillna('') + d['_b4'].fillna('') +\n",
    "            d['_b3'].fillna('') + d['_b2'].fillna('') + d['_b1'].fillna('') + d['token'].fillna('')\n",
    "    )\\\n",
    "    .drop(columns=['_t1','_t2','_t3','_t4', '_t5','_t6', '_t7', '_t8', '_b1','_b2','_b3','_b4','_b5', '_b6', '_b7', '_b8'])\\\n",
    "    .pipe(lambda d: d.join(\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    (d['has_roll'] & d['tok_roll'].apply(lambda s, t=t: s in t)) |\n",
    "                    (d['has_back'] & d['tok_back'].apply(lambda s, t=t: s in t))\n",
    "                    # d['tok_roll'].apply(lambda s, t=t: bool(s) and (s in t)) |\n",
    "                    # d['tok_back'].apply(lambda s, t=t: bool(s) and (s in t))\n",
    "                ).rename(f'hit_p{i}')\n",
    "                for i, t in enumerate(base_messages)\n",
    "            ],\n",
    "            axis = 1\n",
    "        )\n",
    "    ))\\\n",
    "    .assign(\n",
    "        base_message_ix = lambda d: np.select(\n",
    "            [d[f'hit_p{i}'] for i in range(len(base_messages))],\n",
    "            list(range(len(base_messages))),\n",
    "            default = None\n",
    "        ),\n",
    "        ambiguous = lambda d: d[[f'hit_p{i}' for i in range(len(base_messages))]].sum(axis=1) > 1,\n",
    "        # base_name = lambda d: d['base_ix'].map({i: f\"p{i}\" for i in range(len(base_messages))})\n",
    "    )\\\n",
    "    .drop(columns = [f'hit_p{i}' for i in range(len(base_messages))])\\\n",
    "    .drop(columns = ['tok_roll', 'tok_back', 'ambiguous', 'batch_ix', 'sequence_ix', 'token_ix'])\\\n",
    "    [[\n",
    "        'sample_ix',\n",
    "        'prompt_ix', 'prompt_key', 'prompt',\n",
    "        'token_in_prompt_ix',\n",
    "        'base_message_ix',\n",
    "        'token_id',\n",
    "        'token',\n",
    "        'output_id',\n",
    "        'output_prob' ,\n",
    "        # 'tok_roll',\n",
    "        # 'tok_back',\n",
    "        # 'ambiguous'\n",
    "    ]]\n",
    "\n",
    "display(test_sample_df_labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b35b9aa",
   "metadata": {},
   "source": [
    "### Project into rolespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07198ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now project these test samples into rolespace, then flatten - result should be len(test_sample_df_labeled) * # roles\n",
    "\"\"\"\n",
    "project_test_sample_df = test_sample_df_labeled\n",
    "project_hs_cp = cupy.asarray(test_pre_mlp_hs[test_layer][project_test_sample_df['sample_ix'].tolist(), :])\n",
    "project_probs = lr_model.predict_proba(project_hs_cp).round(6)\n",
    "\n",
    "# Merge seq probs withto get sampel_ix\n",
    "proj_results = pd.DataFrame(cupy.asnumpy(project_probs), columns = list(label_to_id.keys())).clip(1e-6)\n",
    "if len(proj_results) != len(project_test_sample_df):\n",
    "    raise Exception(\"Error!\")\n",
    "\n",
    "role_level_df = pd.concat([proj_results, project_test_sample_df[['sample_ix']]], axis = 1)\n",
    "\n",
    "role_level_df =\\\n",
    "    role_level_df.melt(id_vars = ['sample_ix'], var_name = 'role_space', value_name = 'prob')\\\n",
    "    .merge(\n",
    "        project_test_sample_df[['sample_ix', 'prompt_ix', 'prompt_key', 'prompt', 'token_in_prompt_ix', 'base_message_ix', 'token']],\n",
    "        on = 'sample_ix',\n",
    "        how = 'inner'\n",
    "    )\\\n",
    "    .merge(\n",
    "        pd.DataFrame({'base_message_ix': list(range(len(base_message_types))), 'base_message_type': base_message_types}),\n",
    "        on = 'base_message_ix',\n",
    "        how = 'left'\n",
    "    )\\\n",
    "    .assign(base_message_type = lambda df: df['base_message_type'].fillna('other'))\n",
    "\n",
    "role_level_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cfffc6",
   "metadata": {},
   "source": [
    "### Plot and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3da95ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot tests\n",
    "\"\"\"\n",
    "facet_order = list(label_to_id.keys())\n",
    "all_message_types = role_level_df['base_message_type'].unique()\n",
    "color_map = {msg_type: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] for i, msg_type in enumerate(all_message_types)}\n",
    "\n",
    "smooth_df =\\\n",
    "    role_level_df\\\n",
    "    .sort_values('sample_ix')\\\n",
    "    .assign(\n",
    "        prob_sma = lambda df: df.groupby(['prompt_ix', 'role_space'])['prob'].rolling(window = 2, min_periods = 1).mean().reset_index(level = [0, 1], drop = True),\n",
    "        prob_ewma = lambda df: df.groupby(['prompt_ix', 'role_space'])['prob'].ewm(alpha = .9, min_periods = 1).mean().reset_index(level = [0, 1], drop = True)\n",
    "    )\n",
    "\n",
    "display(smooth_df)\n",
    "\n",
    "for this_prompt_key in smooth_df['prompt_key'].unique().tolist():\n",
    "    this_df = smooth_df.pipe(lambda df: df[df['prompt_key'] == this_prompt_key])\n",
    "    fig = px.scatter(\n",
    "        this_df, x = 'token_in_prompt_ix', y = 'prob',\n",
    "        facet_row = 'role_space',\n",
    "        color = 'base_message_type',\n",
    "        color_discrete_map = color_map,\n",
    "        category_orders = {\n",
    "            'role_space': facet_order,\n",
    "            'base_message_type': ['system', 'user', 'cot', 'assistant-final', 'other']\n",
    "        },\n",
    "        hover_name = 'token',\n",
    "        hover_data = {\n",
    "            'prob': ':.3f'\n",
    "        },\n",
    "        # markers = True,\n",
    "        title = f'prompt = {this_prompt_key}',\n",
    "        labels = {\n",
    "            'token_in_prompt_ix': 'Token Index',\n",
    "            'prob': 'Prob',\n",
    "            'prob_smoothed': 'Smoothed Prob',\n",
    "            'role_space': 'role'\n",
    "        }\n",
    "    ).update_yaxes(\n",
    "        range = [0, 1],\n",
    "        side = 'left'\n",
    "    ).update_layout(height = 600, width = 800)\n",
    "\n",
    "    def pretty(a):\n",
    "        a.update(\n",
    "            text = a.text.split(\"=\")[-1],\n",
    "            x = 0.5, xanchor = \"center\",\n",
    "            y = a.y + 0.08,\n",
    "            textangle = 0,\n",
    "            font = dict(size = 12),\n",
    "            bgcolor = 'rgba(255, 255, 255, 0.9)',  # light strip look\n",
    "            showarrow = False\n",
    "        )\n",
    "\n",
    "    fig.for_each_annotation(pretty)\n",
    "    fig.update_yaxes(title_text = None)\n",
    "    fig.show()\n",
    "\n",
    "    fig.write_image(f\"{ws}/experiments/role-confusion/exports/{model_prefix}_{this_prompt_key}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f31df50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

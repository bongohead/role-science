{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009ac414",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate conversational datasets for each model\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1d05de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import yaml\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import duckdb\n",
    "import os\n",
    "import huggingface_hub\n",
    "\n",
    "from utils.openrouter import get_openrouter_responses\n",
    "\n",
    "seed = 1234\n",
    "\n",
    "load_dotenv('.env')\n",
    "std_params = {'temperature': 0, 'top_p': 1, 'top_k': 0}\n",
    "ws = '/workspace/deliberative-alignment-jailbreaks'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56052e9c",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082c4db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load WildChat data\n",
    "If it gets stuck, make sure you're logged in and the token is passed:\n",
    " - huggingface_hub.login(); huggingface_hub.whoami()\n",
    " - token = huggingface_hub.get_token(); duckdb.sql(f\"CREATE SECRET IF NOT EXISTS hf_token (TYPE HUGGINGFACE, TOKEN '{token}')\")\n",
    "\"\"\"\n",
    "\n",
    "def load_raw_ds(n_samples = 1000, n_shards = 4):\n",
    "    cache_path = f\"{ws}/experiments/role-analysis/wildchat_sample.parquet\"\n",
    "\n",
    "    if os.path.exists(cache_path):\n",
    "        return pd.read_parquet(cache_path)\n",
    "    \n",
    "    files = huggingface_hub.list_repo_files(\"allenai/WildChat-4.8M\", repo_type=\"dataset\")\n",
    "    shards = sorted([f\"hf://datasets/allenai/WildChat-4.8M/{f}\" for f in files if f.endswith('.parquet')])[:n_shards]\n",
    "    \n",
    "    duckdb.sql(\"SELECT setseed(0.1234)\") \n",
    "    df = duckdb.sql(f\"\"\"\n",
    "        SELECT conversation, model, turn\n",
    "        FROM read_parquet({shards})\n",
    "        WHERE country = 'United States' AND language = 'English' AND turn >= 1\n",
    "        ORDER BY random()\n",
    "        LIMIT {n_samples}\n",
    "    \"\"\").df()\n",
    "\n",
    "    os.makedirs(os.path.dirname(cache_path), exist_ok = True)\n",
    "    df.to_parquet(cache_path)\n",
    "    return df\n",
    "\n",
    "wildchat_ds = load_raw_ds(n_samples = 100)\n",
    "wildchat_ds = [\n",
    "    [msg['content'] for msg in conv if msg['role'] == 'user']\n",
    "    for conv in wildchat_ds['conversation']\n",
    "]\n",
    "wildchat_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370f5a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load Toxic-Chat\n",
    "\"\"\"\n",
    "def load_raw_ds(n_samples = 100):\n",
    "    def get_convs():\n",
    "        return load_dataset('lmsys/toxic-chat', 'toxicchat1123', split = 'train', streaming = True).shuffle(seed = seed, buffer_size = 50_000)\n",
    "    \n",
    "    def get_data(ds, n_samples):\n",
    "        raw_data = []\n",
    "        for sample in ds:\n",
    "            raw_data.append([sample['user_input']])\n",
    "            if len(raw_data) >= n_samples:\n",
    "                break\n",
    "        return raw_data\n",
    "    \n",
    "    return get_data(get_convs(), n_samples)    \n",
    "\n",
    "toxicchat_ds = load_raw_ds(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a49484",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Asssign conversation ids and user query indices within each conv; truncate convs to 5 queries\n",
    "\"\"\"\n",
    "user_queries_df =\\\n",
    "    pd.concat([\n",
    "        pd.DataFrame({'convs': wildchat_ds}).assign(dataset = 'wildchat'),\n",
    "        pd.DataFrame({'convs': toxicchat_ds}).assign(dataset = 'toxicchat')\n",
    "    ], ignore_index = True)\\\n",
    "    .assign(conv_id = lambda df: range(len(df)))\\\n",
    "    .explode('convs')\\\n",
    "    .assign(user_query_ix = lambda df: df.groupby('conv_id').cumcount())\\\n",
    "    .pipe(lambda df: df[df['user_query_ix'] < 5])\\\n",
    "    .rename(columns={'convs': 'user_query'})\\\n",
    "    [['conv_id', 'dataset', 'user_query_ix', 'user_query']]\n",
    "\n",
    "display(user_queries_df.groupby('conv_id').agg({'user_query_ix': 'max'}).value_counts().sort_index())\n",
    "\n",
    "user_queries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af0928c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define target models\n",
    "\"\"\"\n",
    "target_models = [\n",
    "    {'model': 'qwen/qwen3-30b-a3b-thinking-2507', 'model_provider': 'alibaba'}\n",
    "    # {'model': 'openai/gpt-oss-120b', 'model_provider': 'nebius/fp4', 'policy_prompt': 'openai'},\n",
    "    # {'model': 'qwen/qwen3-vl-30b-a3b-thinking', 'model_provider': 'novita/fp16', 'policy_prompt': 'qwen3-vl-30b-a3b'},\n",
    "]\n",
    "\n",
    "def _validate_and_extract_response(llm_response):\n",
    "    \"\"\"\n",
    "    Extract content/reasoning from response\n",
    "    \n",
    "    Params:\n",
    "        @llm_response: The LLM response object\n",
    "    \"\"\"\n",
    "    if 'choices' not in llm_response:\n",
    "        print(llm_response)\n",
    "        return {'reasoning': None, 'output': None}\n",
    "\n",
    "    choice = llm_response['choices'][0]\n",
    "    if choice['finish_reason'] == 'length':\n",
    "        print(f\"Warning - early stop: {choice['finish_reason']}\")\n",
    "        print(f\"  CONTENT: {choice['message']['content']}\")\n",
    "        print(f\"  REASONING: {choice['message']['reasoning']}\")\n",
    "        return {'reasoning': None, 'output': None}\n",
    "\n",
    "    return {\n",
    "        'reasoning': choice['message']['reasoning'],\n",
    "        'output': choice['message']['content'],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaf35a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate data\n",
    "\"\"\"\n",
    "async def generate_convs_data(user_queries_df, target_dir, target_models):\n",
    "    \"\"\"\n",
    "    Iterate over target models and run generations\n",
    "    \"\"\"\n",
    "    os.makedirs(target_dir, exist_ok = True)\n",
    "\n",
    "    all_model_results = []\n",
    "\n",
    "    max_rounds = user_queries_df['user_query_ix'].max() + 1  # +1 since 0-indexed\n",
    "    for target_model in target_models:\n",
    "        model_name = target_model['model']\n",
    "        filename = model_name.split('/')[-1] + \".csv\"\n",
    "        out_path = os.path.join(target_dir, filename)\n",
    "        \n",
    "        # Skip model if already generated\n",
    "        if os.path.exists(out_path):\n",
    "            print(f\"Skipping {model_name} - already exists at {out_path}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing model: {model_name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        # Store responses: (conv_id, user_query_ix) -> {'reasoning': ..., 'output': ...}\n",
    "        responses = {}\n",
    "\n",
    "        for round_ix in range(max_rounds):\n",
    "\n",
    "            print(f\"Processing round {round_ix}...\")\n",
    "            current_round_df = user_queries_df[user_queries_df['user_query_ix'] == round_ix]\n",
    "            # Build conversation histories for each query in this round\n",
    "            message_histories = []\n",
    "            conv_ids = []\n",
    "            skipped_count = 0\n",
    "\n",
    "            for _, row in current_round_df.iterrows():\n",
    "                # Skip conversations that had a failure in any previous round\n",
    "                has_prior_failure = any(responses[(row['conv_id'], prev_ix)]['output'] is None for prev_ix in range(round_ix))\n",
    "                if has_prior_failure:\n",
    "                    skipped_count += 1\n",
    "                    responses[(row['conv_id'], round_ix)] = {'output': None, 'reasoning': None}\n",
    "                    continue\n",
    "                \n",
    "                messages = []\n",
    "            \n",
    "                # Add all previous turns (user + assistant pairs)\n",
    "                for prev_ix in range(round_ix):\n",
    "                    # Get previous user query\n",
    "                    prev_query =\\\n",
    "                        user_queries_df\\\n",
    "                        .pipe(lambda df: df[(df['conv_id'] == row['conv_id']) & (df['user_query_ix'] == prev_ix)])\\\n",
    "                        ['user_query'].item()\n",
    "                    messages.append({'role': 'user', 'content': prev_query})\n",
    "                    # Get previous assistant response (output only, no CoT)\n",
    "                    prev_response = responses[(row['conv_id'], prev_ix)]\n",
    "                    messages.append({'role': 'assistant', 'content': prev_response['output']})\n",
    "            \n",
    "                # Add current round's user query\n",
    "                messages.append({'role': 'user', 'content': row['user_query']})\n",
    "                message_histories.append(messages)\n",
    "                conv_ids.append(row['conv_id'])\n",
    "        \n",
    "            if len(message_histories) == 0:\n",
    "                print(f\"  All {skipped_count} conversations skipped due to prior failures\")\n",
    "                continue\n",
    "        \n",
    "            # Call API\n",
    "            raw_llm_responses = await get_openrouter_responses(\n",
    "                message_histories,\n",
    "                {\n",
    "                    'model': target_model['model'],\n",
    "                    'provider': {'order': [target_model['model_provider']], 'allow_fallbacks': False},\n",
    "                    'reasoning': {'effort': 'medium', 'enabled': True},\n",
    "                    'temperature': 0,\n",
    "                    'max_tokens': 10_000,\n",
    "                    **std_params\n",
    "                },\n",
    "                batch_size = 20\n",
    "            )\n",
    "        \n",
    "            # Store responses for use in subsequent rounds\n",
    "            for conv_id, raw_response in zip(conv_ids, raw_llm_responses):\n",
    "                extracted = _validate_and_extract_response(raw_response)\n",
    "                responses[(conv_id, round_ix)] = extracted\n",
    "            \n",
    "            print(f\"  Completed: {len(conv_ids)} conversations, skipped: {skipped_count}\")\n",
    "\n",
    "        # Convert results to a dataframe for easier analysis\n",
    "        responses_df =\\\n",
    "            pd.DataFrame([\n",
    "                {'conv_id': conv_id, 'user_query_ix': user_query_ix, **response}\n",
    "                for (conv_id, user_query_ix), response in responses.items()\n",
    "            ])\\\n",
    "            .rename(columns = {'output': 'assistant', 'reasoning': 'cot'})\n",
    "        model_results_df = user_queries_df.merge(responses_df, on = ['conv_id', 'user_query_ix'], how = 'left').assign(model = model_name)\n",
    "        \n",
    "        model_results_df.to_csv(out_path, index = False)\n",
    "        all_model_results.append(model_results_df)\n",
    "\n",
    "    if not all_model_results:\n",
    "        return None\n",
    "    \n",
    "    return pd.concat(all_model_results, ignore_index = True)\n",
    "\n",
    "result_df = await generate_convs_data(\n",
    "    user_queries_df,\n",
    "    target_dir = f\"{ws}/experiments/role-analysis/convs\",\n",
    "    target_models = target_models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06689675",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3582848",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Probe testing\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Glm4vForConditionalGeneration, AutoModelForImageTextToText\n",
    "from transformers.loss.loss_utils import ForCausalLMLoss\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import textwrap\n",
    "from termcolor import colored\n",
    "import importlib\n",
    "import cupy\n",
    "import cuml\n",
    "import sklearn\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.store_outputs import convert_outputs_to_df_fast\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 123\n",
    "\n",
    "ws = '/workspace/deliberative-alignment-jailbreaks'\n",
    "\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "selected_model_index = 0\n",
    "\n",
    "def get_model(index):\n",
    "    # HF model ID, model prefix, model architecture,  attn implementation, whether to use hf lib implementation, is reasoning\n",
    "    models = {\n",
    "        0: ('openai/gpt-oss-20b', 'gptoss20', 'gptoss', 'kernels-community/vllm-flash-attn3', True, 24, True),\n",
    "        1: ('openai/gpt-oss-120b', 'gptoss120', 'gptoss', 'kernels-community/vllm-flash-attn3', True, 36, True), # Will load experts in MXFP4 if triton kernels installed\n",
    "        2: ('allenai/Olmo-3-7B-Think', 'olmo3-7', 'olmo3', None, True, 32, True), # OlMo-3\n",
    "        3: ('allenai/Olmo-3-32B-Think', 'olmo3-32', 'olmo3', None, True, 64, True),\n",
    "        4: ('zai-org/GLM-4.6V-Flash', 'glm-46v-flash', 'glm46v', None, True, 40, True),\n",
    "        5: ('ServiceNow-AI/Apriel-1.6-15b-Thinker', 'apriel-16', 'apriel', None, True, 48, True),\n",
    "        6: ('mistralai/Devstral-Small-2-24B-Instruct-2512', 'mistral3', 'devstral', None, True, 40, False)\n",
    "    }\n",
    "    return models[index]\n",
    "\n",
    "def load_model_and_tokenizer(model_id, model_prefix, model_attn, model_use_hf):\n",
    "    \"\"\"\n",
    "    Load the model and tokenizer from HF, or from file if already downloaded.\n",
    "    \"\"\"\n",
    "    cache_dir = '/workspace/hf'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir = cache_dir, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "    load_params = {'cache_dir': cache_dir, 'dtype': 'auto', 'trust_remote_code': not model_use_hf, 'device_map': None, 'attn_implementation': model_attn}    \n",
    "    if model_architecture == 'glm46v':\n",
    "        model = Glm4vForConditionalGeneration.from_pretrained(model_id, **load_params).to(main_device).eval()\n",
    "    elif model_architecture == 'apriel':\n",
    "        model = AutoModelForImageTextToText.from_pretrained(model_id, **load_params).to(main_device).eval()\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, **load_params).to(main_device).eval()\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "model_id, model_prefix, model_architecture, model_attn, model_use_hf, model_n_layers, model_is_reasoning = get_model(selected_model_index)\n",
    "tokenizer, model = load_model_and_tokenizer(model_id, model_prefix, model_attn, model_use_hf)\n",
    "\n",
    "# Check attention + exper precision for gpt-oss-*\n",
    "if model_architecture == 'gptoss':\n",
    "    print(model.model.layers[0].mlp.experts.down_proj)\n",
    "    print(model.model.config._attn_implementation)\n",
    "\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load reverse-engineered forward pass functions (note for other users - this can be replaced by simpler hooks if desired; we use custom functions due to intersection of other research projects)\n",
    "\"\"\"\n",
    "model_module = importlib.import_module(f\"utils.pretrained_models.{model_architecture}\")\n",
    "run_model_return_topk = getattr(model_module, f\"run_{model_architecture}_return_topk\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_custom_forward_pass(model, pad_token_id):\n",
    "    inputs = tokenizer(['Hi! I am a dog and I like to bark', 'Vegetables are good for'], return_tensors = 'pt', padding = 'max_length', truncation = True, max_length = 640).to(model.device)\n",
    "    original_results = model(**inputs, use_cache = False)\n",
    "    custom_results = run_model_return_topk(model, inputs['input_ids'], inputs['attention_mask'], return_hidden_states = True)\n",
    "    assert torch.equal(original_results.logits, custom_results['logits']), 'Error in custom forward'\n",
    "    loss = ForCausalLMLoss(custom_results['logits'], torch.where(inputs['input_ids'] == pad_token_id, torch.tensor(-100), inputs['input_ids']), custom_results['logits'].size(-1)).detach().cpu().item()\n",
    "    print(f\"LM loss: {loss}\")\n",
    "    print(f\"Hidden states layers (pre-mlp | post-layer): {len(custom_results['all_pre_mlp_hidden_states'])} | {len(custom_results['all_hidden_states'])}\")\n",
    "    print(f\"Hidden state size (pre-mlp | post-layer): {(custom_results['all_pre_mlp_hidden_states'][0].shape)} | {(custom_results['all_hidden_states'][0].shape)}\")\n",
    "\n",
    "test_custom_forward_pass(model, tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train probes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset - C4 + Dolma3\n",
    "\"\"\"\n",
    "n_sample_size = 250 # 500 default + tested\n",
    "\n",
    "def load_raw_ds():\n",
    "\n",
    "    def get_c4():\n",
    "        return load_dataset('allenai/c4', 'en', split = 'validation', streaming = True).shuffle(seed = seed, buffer_size = 50_000)\n",
    "    \n",
    "    def get_dolma3():\n",
    "        return load_dataset('allenai/dolma3_mix-150B-1025', split = 'train', streaming = True).shuffle(seed = seed, buffer_size = 10_000)\n",
    "    \n",
    "    # def get_hplt2():\n",
    "    #     return load_dataset('HPLT/HPLT2.0_cleaned', 'eng_Latn', split = 'train', streaming = True).shuffle(seed = seed, buffer_size = 50_000)\n",
    "\n",
    "    def get_data(ds, n_samples, data_source): # en\n",
    "        raw_data = []\n",
    "        ds_iter = iter(ds)\n",
    "        for _ in range(n_samples):\n",
    "            sample = next(ds_iter, None)\n",
    "            if sample is None:\n",
    "                break\n",
    "            raw_data.append({'text': sample['text'], 'source': data_source})\n",
    "        return raw_data\n",
    "    \n",
    "    return get_data(get_c4(), int(n_sample_size * .2), 'c4-en') + get_data(get_dolma3(), int(n_sample_size * .8), 'dolma3')\n",
    "\n",
    "raw_data = load_raw_ds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test our formattting functions\n",
    "\"\"\"\n",
    "import utils.role_templates\n",
    "importlib.reload(utils.role_templates)\n",
    "from utils.role_templates import render_single_message, render_mixed_cot\n",
    "\n",
    "# Test\n",
    "print(tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {'role': 'user', 'content': 'Hi! I am a dog and I like to bark'},\n",
    "        {'role': 'assistant', 'content': 'Hello! What a lovely dog you are!'}\n",
    "    ],\n",
    "    tokenize = False, padding = 'max_length', truncation = True, max_length = 512, add_generation_prompt = False\n",
    "))\n",
    "\n",
    "print('--')\n",
    "print(render_single_message(model_architecture, role = 'assistant', content ='Hi'))\n",
    "print('--')\n",
    "print(render_mixed_cot(model_architecture, 'The user...', 'Yes!'))\n",
    "print('--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define opening text to append at the start of each role\n",
    "\"\"\"\n",
    "if model_architecture == 'gptoss':\n",
    "    train_prefixes = [\"\"]\n",
    "\n",
    "elif model_architecture == 'olmo3':\n",
    "    train_prefixes = [\n",
    "        tokenizer.bos_token + render_single_message(\n",
    "            model_architecture,\n",
    "            'system', \n",
    "            textwrap.dedent(\n",
    "                \"\"\"\n",
    "                You are OLMo, a helpful function-calling AI assistant built by Ai2. Your date cutoff is November 2024, and your model weights are available at https://huggingface.co/allenai. You do not currently have access to any functions. <functions></functions>\n",
    "                \"\"\"\n",
    "            ).strip()\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "elif model_architecture in ['glm46v', 'glm4vmoe']:\n",
    "    train_prefixes = [\"\"\"[gMASK]<sop>\"\"\"]\n",
    "\n",
    "elif model_architecture == 'apriel':\n",
    "    train_prefixes = [\n",
    "        tokenizer.bos_token + render_single_message(\n",
    "            model_architecture,\n",
    "            'system',\n",
    "            textwrap.dedent(\n",
    "                \"\"\"\n",
    "                You are a thoughtful, systematic AI assistant from ServiceNow Language Models (SLAM) lab. Analyze each question carefully, present your reasoning step-by-step, then provide the final response after the marker [BEGIN FINAL RESPONSE].\n",
    "                \"\"\"\n",
    "            ).strip()\n",
    "        )\n",
    "    ]\n",
    "\n",
    "else:\n",
    "    raise ValueError('Missing model architecture')\n",
    "\n",
    "for p in train_prefixes:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run some IFT tests\n",
    "\"\"\"\n",
    "import utils.role_templates\n",
    "importlib.reload(utils.role_templates)\n",
    "from utils.role_templates import render_single_message, render_mixed_cot\n",
    "\n",
    "# Test\n",
    "print(tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {'role': 'user', 'content': 'Hi! I am a dog and I like to bark'},\n",
    "        {'role': 'assistant', 'content': 'Hello! What a lovely dog you are!'}\n",
    "    ],\n",
    "    tokenize = False, padding = 'max_length', truncation = True, max_length = 512, add_generation_prompt = False\n",
    "))\n",
    "\n",
    "print('--')\n",
    "print(render_single_message(model_architecture, role = 'assistant', content ='Hi'))\n",
    "print('--')\n",
    "print(render_mixed_cot(model_architecture, 'The user...', 'Yes!'))\n",
    "print('--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create sample sequences\n",
    "\"\"\"\n",
    "SEQLEN = 1536\n",
    "IS_MERGE = False # Whether to prefix random CoT before assistant text - do for models with nested <think></think>\n",
    "IS_REASONING = model_is_reasoning\n",
    "\n",
    "def get_sample_seqs(probe_text, partner_text, prefix = ''):\n",
    "    \"\"\"\n",
    "    Params\n",
    "        @probe_text: The text we're extracting states from (appears in all roles)\n",
    "        @partner_text: Random paired text (only used in merged sample's assistant position)\n",
    "    \"\"\"\n",
    "    seqs = []\n",
    "    for role in ['system', 'user', 'tool']:\n",
    "        seqs.append({\n",
    "            'role': role,\n",
    "            'prompt': prefix + render_single_message(model_architecture, role = role, content = probe_text)\n",
    "        })\n",
    "\n",
    "    if IS_REASONING:\n",
    "        seqs.append({\n",
    "            'role': 'cot',\n",
    "            'prompt': prefix + render_single_message(model_architecture, role = 'cot', content = probe_text)\n",
    "        })\n",
    "\n",
    "    # If merge + LRM, render with CoT prefix\n",
    "    if IS_REASONING and IS_MERGE:\n",
    "        seqs.append({\n",
    "            'role': 'assistant',\n",
    "            'prompt': prefix + render_mixed_cot(model_architecture, cot = partner_text, assistant = probe_text)\n",
    "        })\n",
    "    # Else render standalone\n",
    "    else:\n",
    "        seqs.append({\n",
    "            'role': 'assistant',\n",
    "            'prompt': prefix + render_single_message(model_architecture, role = 'assistant', content = probe_text)\n",
    "        })\n",
    "\n",
    "    return seqs\n",
    "\n",
    "\n",
    "truncated_texts = tokenizer.batch_decode(tokenizer([t['text'] for t in raw_data], padding = False, truncation = True, max_length = SEQLEN).input_ids)\n",
    "n_seqs = len(truncated_texts)\n",
    "\n",
    "np.random.seed(seed)\n",
    "partner_lengths = np.rint(np.random.lognormal(mean = 1.5, sigma = 2, size = n_seqs)).astype(int).tolist() # Reasonable seq length sampling\n",
    "partner_texts = [\n",
    "    tokenizer.decode(tokenizer(text['text'], padding = False, truncation = True, max_length = int(partner_lengths[i])).input_ids)\n",
    "    for i, text in enumerate(raw_data)\n",
    "]\n",
    "perm = np.random.permutation(n_seqs)\n",
    "while np.any(perm == np.arange(n_seqs)):\n",
    "    perm = np.random.permutation(n_seqs)\n",
    "\n",
    "sampled_prefixes = np.random.choice(train_prefixes, size = n_seqs)\n",
    "\n",
    "input_list = []\n",
    "for base_ix, base_text in enumerate(truncated_texts):\n",
    "    partner_text = partner_texts[int(perm[base_ix])]\n",
    "    prefix = sampled_prefixes[base_ix]\n",
    "\n",
    "    for seq in get_sample_seqs(base_text, partner_text, prefix):\n",
    "        row = {'question_ix': base_ix, 'question': base_text, **seq}\n",
    "        input_list.append(row)\n",
    "\n",
    "input_df = pd.DataFrame(input_list).assign(prompt_ix = lambda df: list(range(len(df))))\n",
    "display(input_df)\n",
    "\n",
    "# Print examples for simple ex\n",
    "for p in [row['prompt'] for row in input_df.pipe(lambda df: df[df['question_ix'] == 1]).to_dict('records')]:\n",
    "    print(p)\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Load dataset into a dataloader. The dataloader returns the original tokens - this is important for BPE tokenizers as otherwise it's difficult to reconstruct the correct string later!\n",
    "\"\"\"\n",
    "from utils.dataset import ReconstructableTextDataset, stack_collate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "max_seqlen = int(tokenizer(input_df['prompt'].tolist(), padding = True, truncation = False, return_tensors = 'pt')['attention_mask'].sum(dim = 1).max().item())\n",
    "train_dl = DataLoader(\n",
    "    ReconstructableTextDataset(input_df['prompt'].tolist(), tokenizer, max_length = max_seqlen, prompt_ix = input_df['prompt_ix'].tolist()),\n",
    "    batch_size = 16,\n",
    "    shuffle = False,\n",
    "    collate_fn = stack_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run forward passes\n",
    "\"\"\"\n",
    "@torch.no_grad()\n",
    "def run_and_export_states(model, dl: ReconstructableTextDataset, layers_to_keep_acts: list[int]):\n",
    "    \"\"\"\n",
    "    Run forward passes on given model and store the decomposed sample_df plus hidden states\n",
    "\n",
    "    Params:\n",
    "        @model: The model to run forward passes on. Should return a dict with keys `logits` and `all_hidden_states`.\n",
    "        @dl: A ReconstructableTextDataset of which returns `input_ids`, `attention_mask`, and `original_tokens`.\n",
    "        @layers_to_keep_acts: A list of layer indices (0-indexed) for which to filter `all_hidden_states` (see returned object description).\n",
    "\n",
    "    Returns:\n",
    "        A dict with keys:\n",
    "        - `sample_df`: A sample (token)-level dataframe with corresponding input token ID, output token ID, and input token text (removes masked tokens)\n",
    "        - `all_hidden_states`: A tensor of size n_samples x layers_to_keep_acts x D return the hidden state for each retained layers. Each \n",
    "            n_sample corresponds to a row of sample_df.\n",
    "\n",
    "    Example:\n",
    "        test_outputs = run_and_export_states(model, train_dl, layers_to_keep_acts = list(range(model_n_layers)))\n",
    "    \"\"\"\n",
    "    all_hidden_states = []\n",
    "    sample_dfs = []\n",
    "\n",
    "    for batch_ix, batch in tqdm(enumerate(dl), total = len(dl)):\n",
    "\n",
    "        input_ids = batch['input_ids'].to(main_device)\n",
    "        attention_mask = batch['attention_mask'].to(main_device)\n",
    "        original_tokens = batch['original_tokens']\n",
    "        prompt_indices = batch['prompt_ix']\n",
    "\n",
    "        output = run_model_return_topk(model, input_ids, attention_mask, return_hidden_states = True)\n",
    "\n",
    "        # Check no bugs by validating output/perplexity\n",
    "        if batch_ix == 0:\n",
    "            loss = ForCausalLMLoss(output['logits'], torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids), output['logits'].size(-1)).detach().cpu().item()\n",
    "            for i in range(min(20, input_ids.size(0))):\n",
    "                decoded_input = tokenizer.decode(input_ids[i, :], skip_special_tokens = False)\n",
    "                next_token_id = torch.argmax(output['logits'][i, -1, :]).item()\n",
    "                print('---------\\n' + decoded_input + colored(tokenizer.decode([next_token_id], skip_special_tokens = False).replace('\\n', '<lb>'), 'green'))\n",
    "            print(f\"PPL:\", torch.exp(torch.tensor(loss)).item())\n",
    "                \n",
    "        original_tokens_df = pd.DataFrame(\n",
    "            [(seq_i, tok_i, tok) for seq_i, tokens in enumerate(original_tokens) for tok_i, tok in enumerate(tokens)], \n",
    "            columns = ['sequence_ix', 'token_ix', 'token']\n",
    "        )\n",
    "                \n",
    "        prompt_indices_df = pd.DataFrame(\n",
    "            [(seq_i, seq_source) for seq_i, seq_source in enumerate(prompt_indices)], \n",
    "            columns = ['sequence_ix', 'prompt_ix']\n",
    "        )\n",
    "        \n",
    "        # Create sample (token) level dataframe\n",
    "        sample_df =\\\n",
    "            convert_outputs_to_df_fast(input_ids, attention_mask, output['logits'])\\\n",
    "            .merge(original_tokens_df, how = 'left', on = ['token_ix', 'sequence_ix'])\\\n",
    "            .merge(prompt_indices_df, how = 'left', on = ['sequence_ix'])\\\n",
    "            .assign(batch_ix = batch_ix)\n",
    "        \n",
    "        sample_dfs.append(sample_df)\n",
    "\n",
    "        # Store pre-MLP hidden states - the fwd pass as n_layers list as BN x D, collapse to BN x n_layers x D, with BN filtering out masked items\n",
    "        valid_pos = torch.where(attention_mask.cpu().view(-1) == 1) # Valid (BN, ) positions\n",
    "        all_hidden_states.append(torch.stack(output['all_pre_mlp_hidden_states'], dim = 1)[valid_pos][:, layers_to_keep_acts, :])\n",
    "\n",
    "    sample_df = pd.concat(sample_dfs, ignore_index = True).drop(columns = ['batch_ix', 'sequence_ix']) # Drop batch/seq_ix, since prompt_ix identifies\n",
    "    all_hidden_states = torch.cat(all_hidden_states, dim = 0)\n",
    "\n",
    "    return {\n",
    "        'sample_df': sample_df,\n",
    "        'all_hs': all_hidden_states\n",
    "    }\n",
    "\n",
    "layers_to_probe = list(range(0, model_n_layers, 4 if model_n_layers >= 40 else 4))\n",
    "res = run_and_export_states(model, train_dl, layers_to_keep_acts = layers_to_probe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label roles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Label roles\n",
    "\"\"\"\n",
    "import utils.role_assignments\n",
    "importlib.reload(utils.role_assignments)\n",
    "from utils.role_assignments import label_content_roles\n",
    "from utils.substring_assignments import flag_message_types\n",
    "\n",
    "probe_sample_df = (\n",
    "    # Flag all roles\n",
    "    label_content_roles(model_architecture, res['sample_df'])    \n",
    "    .assign(sample_ix = lambda df: range(0, len(df)))\n",
    "    # Flag those that match target role (for retain later)\n",
    "    .merge(input_df[['prompt_ix', 'role']].rename(columns = {'role': 'target_role'}), how = 'inner', on = 'prompt_ix')\n",
    "    .assign(match_target_role = lambda df: np.where(df['role'] == df['target_role'], True, False))\n",
    "    # Flag those that match the prepend (for drop later)\n",
    "    .pipe(lambda df: flag_message_types(df, train_prefixes))\n",
    "    .drop(columns = 'base_message')\n",
    "    # Drop prepend text + non-content tags\n",
    "    .pipe(lambda df: df[(df['base_message_ix'].isna()) & (df['is_content'] == True) & (df['role'].notna()) & (df['match_target_role']) ])\n",
    ")\n",
    "\n",
    "# Verify role counts are accurate (should be exactly equal for most models, slight discrepancy for Olmo3 since <think> isn't a single tok)\n",
    "display(\n",
    "    probe_sample_df\\\n",
    "    .groupby('role', as_index = False)\\\n",
    "    .agg(count = ('sample_ix', 'count'))\n",
    ")\n",
    "\n",
    "# Validate roles are flagged correctly\n",
    "display(\n",
    "    probe_sample_df\\\n",
    "    .pipe(lambda df: df[df['prompt_ix'] <= 14])\\\n",
    "    .groupby(['prompt_ix', 'seg_ix', 'role'], as_index = False)\\\n",
    "    .agg(combined_text = ('token', ''.join))\\\n",
    "    .assign(eot = lambda df: df['combined_text'].str[-30:])\n",
    ")\n",
    "\n",
    "# probe_sample_df.pipe(lambda df: df[df['prompt_ix'] == 3]).to_csv('dump.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert activations to fp16 (for compatibility with cupy later) + layer-wise dict\n",
    "\"\"\"\n",
    "all_probe_hs = res['all_hs'].to(torch.float16)\n",
    "del res['all_hs']\n",
    "all_probe_hs = {layer_ix: all_probe_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(layers_to_probe)}\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_all_cuda_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run a quick grid search over hyperparmeters\n",
    "\"\"\"\n",
    "SKIP_FIRST_N = 8 # Skip first N tokens in each sample\n",
    "\n",
    "def fit_lr(x_train, y_train, x_test, y_test, **lr_params):\n",
    "    \"\"\"\n",
    "    Fit a probe\n",
    "    \"\"\"\n",
    "    lr_model = sklearn.pipeline.Pipeline([\n",
    "        # ('scaler', cuml.preprocessing.StandardScaler()),\n",
    "        ('clf', cuml.linear_model.LogisticRegression(\n",
    "            penalty = 'l2', # 1e-3 if model_prefix == 'gptoss20' else 1.0,\n",
    "            max_iter = 5_000,\n",
    "            fit_intercept = True,\n",
    "            **lr_params\n",
    "        ))\n",
    "    ])\n",
    "    lr_model.fit(x_train, y_train)\n",
    "    accuracy = lr_model.score(x_test, y_test)\n",
    "    y_test_pred = lr_model.predict(x_test)\n",
    "    return lr_model, accuracy, y_test_pred\n",
    "\n",
    "def get_probe_result(sample_df, layer_hs, roles_map, **lr_params):\n",
    "    \"\"\"\n",
    "    Get probe results for a single layer and label combination\n",
    "\n",
    "    Params:\n",
    "        @sample_df: The sample-level df; with a column `sample_ix` indicating the token order of 0...T-1;\n",
    "            the actual df may be shorter due to pre-filters\n",
    "        @layer_hs: A tensor of probe hidden states for a layer, of T x D\n",
    "        @roles_map: The mapping order of the roles; a dict {}\n",
    "\n",
    "    Description:\n",
    "        Trains only on content space for given roles\n",
    "    \"\"\"\n",
    "    # Train/test split\n",
    "    prompt_ix_train, prompt_ix_test = cuml.train_test_split(sample_df['prompt_ix'].unique(), test_size = 0.1, random_state = seed)\n",
    "    train_df = sample_df[sample_df['prompt_ix'].isin(prompt_ix_train)]\n",
    "    test_df = sample_df[sample_df['prompt_ix'].isin(prompt_ix_test)]\n",
    "\n",
    "    # Get y labels\n",
    "    role_labels_train_cp = cupy.asarray([roles_map[r] for r in train_df['role']])\n",
    "    role_labels_test_cp = cupy.asarray([roles_map[r] for r in test_df['role']])\n",
    "\n",
    "    # Get x labels\n",
    "    x_train_cp = cupy.asarray(layer_hs[train_df['sample_ix'].tolist(), :].to(torch.float32).detach().cpu())\n",
    "    x_test_cp = cupy.asarray(layer_hs[test_df['sample_ix'].tolist(), :].to(torch.float32).detach().cpu())\n",
    "    \n",
    "    uniq_train = np.unique(role_labels_train_cp.get())\n",
    "    if len(uniq_train) < len(roles_map):\n",
    "        raise Exception(f\"Skipping mapping {roles_map}: missing roles in train\", uniq_train)\n",
    "\n",
    "    lr_model, test_acc, y_test_pred = fit_lr(x_train_cp, role_labels_train_cp, x_test_cp, role_labels_test_cp, **lr_params)\n",
    "\n",
    "    # Classification metrics\n",
    "    results_df =\\\n",
    "        test_df\\\n",
    "        .assign(pred = y_test_pred.tolist())\\\n",
    "        .assign(pred = lambda df: df['pred'].map({v: k for k, v in roles_map.items()}))\\\n",
    "        .assign(is_acc = lambda df: df['role'] == df['pred'])\n",
    "\n",
    "    acc_by_role =\\\n",
    "        results_df\\\n",
    "        .groupby(['role', 'pred'], as_index = False)\\\n",
    "        .agg(count = ('sample_ix', 'count'))\n",
    "\n",
    "    acc_by_pos =\\\n",
    "        results_df\\\n",
    "        .groupby('token_in_seg_ix', as_index = False)\\\n",
    "        .agg(count = ('sample_ix', 'count'), acc = ('is_acc', 'mean'))\n",
    "\n",
    "    return {\n",
    "        'probe': lr_model, 'acc': test_acc,\n",
    "        'acc_by_role': acc_by_role, 'acc_by_pos': acc_by_pos\n",
    "    }\n",
    "\n",
    "def test_c(C):   \n",
    "    test_roles = ['user', 'cot', 'assistant'] if model_is_reasoning else ['user', 'assistant']\n",
    "    test_layer_ix = layers_to_probe[(len(layers_to_probe) - 1) // 2] \n",
    "    acc = get_probe_result(\n",
    "        sample_df = probe_sample_df[(probe_sample_df['role'].isin(test_roles)) & (probe_sample_df['token_in_seg_ix'] >= SKIP_FIRST_N)].reset_index(drop = True),\n",
    "        layer_hs = all_probe_hs[test_layer_ix],\n",
    "        roles_map = {x: i for i, x in enumerate(test_roles)},\n",
    "        C = C\n",
    "    )['acc']\n",
    "    return {'C': C, 'val_acc': acc}\n",
    "\n",
    "hp_vals = [test_c(c_val) for c_val in tqdm([0.0001, 0.001, 0.01, 0.1, 1.0, 10.0])]\n",
    "hp_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run LRs\n",
    "\"\"\"\n",
    "all_role_combinations = [\n",
    "    ('user', 'cot', 'assistant'),\n",
    "    ('user', 'assistant', 'tool'),\n",
    "    ('system', 'user', 'cot', 'assistant'),\n",
    "    ('system', 'user', 'assistant', 'tool'),\n",
    "    ('user', 'cot', 'assistant', 'tool'),\n",
    "    ('system', 'user', 'cot', 'assistant', 'tool')\n",
    "]\n",
    "if not model_is_reasoning:\n",
    "    all_role_combinations = [roles for roles in all_role_combinations if 'cot' not in roles]\n",
    "    \n",
    "all_role_combinations = [\n",
    "    {\n",
    "        'roles': list(roles),\n",
    "        'roles_map': {x: i for i, x in enumerate(roles)},\n",
    "        # Sample df for only those roles\n",
    "        'sample_df': probe_sample_df.pipe(lambda df: df[(df['role'].isin(roles)) & (df['token_in_seg_ix'] >= SKIP_FIRST_N)]).reset_index(drop = True),\n",
    "    }\n",
    "    for roles in all_role_combinations\n",
    "]\n",
    "\n",
    "# clear_all_cuda_memory()\n",
    "\n",
    "all_probes = []\n",
    "for roles_dict in all_role_combinations:\n",
    "    print(f\"Training {roles_dict['roles']}\")\n",
    "    for layer_ix in layers_to_probe:\n",
    "        probe_res = get_probe_result(\n",
    "            sample_df = roles_dict['sample_df'],\n",
    "            layer_hs = all_probe_hs[layer_ix],\n",
    "            roles_map = roles_dict['roles_map'],\n",
    "            C = 1e-3 if model_architecture == 'gptoss' else 1e-1\n",
    "        )\n",
    "        print(f\"  Layer [{layer_ix}]: {probe_res['acc']:.2f}\")\n",
    "\n",
    "        all_probes.append({\n",
    "            **probe_res,\n",
    "            'layer_ix': layer_ix,\n",
    "            'roles': roles_dict['roles'],\n",
    "            'roles_map': roles_dict['roles_map'],\n",
    "            'n_inputs': len(roles_dict['sample_df'])\n",
    "        })\n",
    "\n",
    "print(f\"Num probes: {str(len(all_probes))}\")\n",
    "print(f\"Probe layers:\\n  {', '.join([str(x) for x in layers_to_probe])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save probes + metrics\n",
    "\"\"\"\n",
    "def validate_accuracy_and_save(all_probes):\n",
    "    print('Val accuracy by layer:')\n",
    "    display(\n",
    "        pd.DataFrame(all_probes)[['layer_ix', 'roles', 'acc']]\\\n",
    "            .assign(acc = lambda df: df['acc'].round(2), roles = lambda df: df['roles'].apply(lambda x: ','.join(([r[0] for r in x]))))\\\n",
    "            .pivot(index = 'layer_ix', columns = 'roles', values = 'acc')\n",
    "    )\n",
    "\n",
    "    print('Concatenating across layers and saving...')\n",
    "    acc_by_role = pd.concat([p['acc_by_role'].assign(model = model_prefix, layer_ix = p['layer_ix'], roles = ','.join(p['roles'])) for p in all_probes], ignore_index = True)\n",
    "    acc_by_pos =\\\n",
    "        pd.concat([p['acc_by_pos'].assign(model = model_prefix, layer_ix = p['layer_ix'], roles = ','.join(p['roles'])) for p in all_probes], ignore_index = True)\\\n",
    "        .assign(acc = lambda df: df['acc'].round(4))\n",
    "\n",
    "    acc_by_role.to_csv(f'{ws}/experiments/role-analysis/probes/acc_by_role_{model_prefix}.csv', index = False)\n",
    "    acc_by_pos.to_csv(f'{ws}/experiments/role-analysis/probes/acc_by_pos_{model_prefix}.csv', index = False)\n",
    "\n",
    "    with open(f'{ws}/experiments/role-analysis/probes/{model_prefix}.pkl', 'wb') as f:\n",
    "        pickle.dump(all_probes, f)\n",
    "\n",
    "    print('Accuracy by role:')\n",
    "    base_sums =\\\n",
    "        acc_by_role\\\n",
    "        .groupby(['role', 'layer_ix', 'roles'], as_index = False)\\\n",
    "        .agg(base_sum = ('count', 'sum'))\n",
    "\n",
    "    acc_by_role =\\\n",
    "        acc_by_role\\\n",
    "        .pipe(lambda df: df[df['role'] == df['pred']])\\\n",
    "        .groupby(['role', 'layer_ix', 'roles'], as_index = False)\\\n",
    "        .agg(sum = ('count', 'sum'))\\\n",
    "        .merge(base_sums, on = ['layer_ix', 'roles', 'role'], how = 'inner')\\\n",
    "        .assign(acc = lambda df: df['sum']/df['base_sum'])\\\n",
    "        .pivot(index = ['roles', 'layer_ix'], columns = 'role', values = 'acc')\\\n",
    "        .round(2)\n",
    "\n",
    "    display(acc_by_role)\n",
    "\n",
    "    return True\n",
    "\n",
    "validate_accuracy_and_save(all_probes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalization to conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Validate IFT on the full chat template, which tokenizes the whole thing e2e. This should match the format from original messages.:\n",
    "- `load_chat_template`: Overwrites the existing chat template in the tokenizer with one that better handles discrepancies.\n",
    "    See docstring for the function for details. \n",
    "- `label_content_roles`: Takes an instruct-formatted text, then assigns each token to the correct roles.\n",
    "\"\"\"\n",
    "import importlib\n",
    "import utils.role_assignments\n",
    "import utils.role_templates\n",
    "importlib.reload(utils.role_assignments)\n",
    "label_content_roles = utils.role_assignments.label_content_roles\n",
    "importlib.reload(utils.role_templates)\n",
    "load_chat_template = utils.role_templates.load_chat_template\n",
    "\n",
    "# Load custom chat templater\n",
    "old_chat_template = tokenizer.chat_template\n",
    "new_chat_template = load_chat_template(f'{ws}/utils/chat_templates', model_architecture)\n",
    "\n",
    "def test_chat_template(tokenizer):\n",
    "    s = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {'role': 'system', 'content': 'Test.'},\n",
    "            {'role': 'user', 'content': 'Hi! I\\'m a dog.'},\n",
    "            {'role': 'assistant', 'content': '<think>The user is a dog!</think>Congrats!'},\n",
    "            {'role': 'user', 'content': 'Thanks!'},\n",
    "            {'role': 'assistant', 'content': '<think>Hmm, the user said thanks!</think>Anything else?'}\n",
    "        ],\n",
    "        tokenize = False,\n",
    "        padding = 'max_length',\n",
    "        truncation = True,\n",
    "        max_length = 512,\n",
    "        add_generation_prompt = False\n",
    "    )\n",
    "    print(s)\n",
    "    return s\n",
    "\n",
    "tokenizer.chat_template = old_chat_template\n",
    "s = test_chat_template(tokenizer)\n",
    "tokenizer.chat_template = new_chat_template\n",
    "s = test_chat_template(tokenizer)\n",
    "\n",
    "z =\\\n",
    "    pd.DataFrame({'input_ids': tokenizer(s)['input_ids']})\\\n",
    "    .assign(token = lambda df: tokenizer.convert_ids_to_tokens(df['input_ids']))\\\n",
    "    .assign(prompt_ix = 0, batch_ix = 0, sequence_ix = 0, token_ix = lambda df: df.index)\n",
    "\n",
    "label_content_roles(model_architecture, z).tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define opening text to append at the start of each role\n",
    "\"\"\"\n",
    "if model_architecture == 'gptoss':\n",
    "    test_prefix = tokenizer.bos_token + render_single_message(\n",
    "        model_architecture,\n",
    "        'system', \n",
    "        textwrap.dedent(\n",
    "        \"\"\"\n",
    "        You are ChatGPT, a large language model trained by OpenAI.\n",
    "        Knowledge cutoff: 2024-06\n",
    "        Current date: 2026-01-01\n",
    "\n",
    "        Reasoning: medium\n",
    "\n",
    "        # Valid channels: analysis, commentary, final. Channel must be included for every message.\n",
    "        \"\"\").strip()\n",
    "    )\n",
    "\n",
    "elif model_architecture == 'olmo3':\n",
    "    test_prefix = tokenizer.bos_token + render_single_message(\n",
    "            model_architecture,\n",
    "            'system', \n",
    "            textwrap.dedent(\n",
    "            \"\"\"\n",
    "            You are OLMo, a helpful function-calling AI assistant built by Ai2. Your date cutoff is November 2024, and your model weights are available at https://huggingface.co/allenai. You do not currently have access to any functions. <functions></functions>\n",
    "            \"\"\").strip()\n",
    "        )\n",
    "    \n",
    "elif model_architecture in ['glm46v', 'glm4vmoe']:\n",
    "    test_prefix = \"\"\"[gMASK]<sop>\"\"\"\n",
    "        \n",
    "elif model_architecture == 'apriel':\n",
    "    test_prefix = tokenizer.bos_token + render_single_message(\n",
    "        model_architecture,\n",
    "        'system',\n",
    "        textwrap.dedent(\n",
    "        \"\"\"\n",
    "        You are a thoughtful, systematic AI assistant from ServiceNow Language Models (SLAM) lab. Analyze each question carefully, present your reasoning step-by-step, then provide the final response after the marker [BEGIN FINAL RESPONSE].\n",
    "        \"\"\"\n",
    "        ).strip()\n",
    "    )\n",
    "\n",
    "else:\n",
    "    raise ValueError('Missing model architecture')\n",
    "\n",
    "print(test_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Validate with prefix + generation ability\n",
    "\"\"\"\n",
    "def test():\n",
    "    conv = test_prefix + tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {'role': 'user', 'content': 'Hi stinky pupper'}    \n",
    "        ],\n",
    "        tokenize = False,\n",
    "        add_generation_prompt = True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(conv, return_tensors = 'pt')\n",
    "    gen_ids = model.generate(inputs['input_ids'].to(main_device), attention_mask = inputs['attention_mask'].to(main_device), max_new_tokens = 100, do_sample = False)\n",
    "    print(tokenizer.decode(gen_ids)[0])\n",
    "\n",
    "    df = pd.DataFrame({'token_id': gen_ids[0].tolist(), 'token': tokenizer.convert_ids_to_tokens(gen_ids[0].tolist()),})\n",
    "    return df\n",
    "\n",
    "display(test())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import conversations\n",
    "\"\"\"\n",
    "def filter_convs(user_queries_df, max_samples):\n",
    "    print(f\"Starting length: {len(user_queries_df['conv_id'].unique())} convs\")\n",
    "\n",
    "    # 1. Remove convs with any missing responses\n",
    "    convs_with_missing = user_queries_df[user_queries_df[['user_query', 'cot', 'assistant']].isna().any(axis = 1)]['conv_id'].unique()\n",
    "    user_queries_df = user_queries_df[~user_queries_df['conv_id'].isin(convs_with_missing)]\n",
    "    print(f\"After dropping missing: {len(user_queries_df['conv_id'].unique())} convs\")\n",
    "\n",
    "    # 2. Filter out super long convs + convs with any message <= conv_min_length (to avoid issues with flag_message_type fn)\n",
    "    user_queries_df =\\\n",
    "        user_queries_df\\\n",
    "        .assign(total_len = lambda df:\n",
    "            df.groupby('conv_id')['user_query'].transform(lambda x: x.str.len().sum()) +\n",
    "            df.groupby('conv_id')['cot'].transform(lambda x: x.str.len().sum()) +\n",
    "            df.groupby('conv_id')['assistant'].transform(lambda x: x.str.len().sum())\n",
    "        )\\\n",
    "        .assign(min_len = lambda df: df[['user_query', 'cot', 'assistant']].apply(lambda x: x.str.len()).min(axis=1))\\\n",
    "        .assign(conv_min_len = lambda df: df.groupby('conv_id')['min_len'].transform('min'))\\\n",
    "        .assign(conv_min_user_len = lambda df: df.groupby('conv_id')['user_query'].transform(lambda x: x.str.len().min()))\\\n",
    "        .query('total_len < 16000 and conv_min_len >= 20 and conv_min_user_len >= 100')\\\n",
    "        .drop(columns = ['total_len', 'min_len', 'conv_min_len', 'conv_min_user_len'])\n",
    "\n",
    "    print(f\"After dropping long convs: {len(user_queries_df['conv_id'].unique())} convs\")\n",
    "\n",
    "    # 3. Filter out convs where any message is a substring of another message\n",
    "    #    (this causes dupe issues with the flag_message_type fn, which does not handle tokens in multiple string contexts)\n",
    "    def has_substring_messages(group):\n",
    "        contents = group['user_query'].tolist() + group['cot'].tolist() + group['assistant'].tolist()\n",
    "        for i, a in enumerate(contents):\n",
    "            for j, b in enumerate(contents):\n",
    "                if i != j and a in b:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    bad_convs = user_queries_df.groupby('conv_id').filter(has_substring_messages)['conv_id'].unique()\n",
    "    user_queries_df = user_queries_df.pipe(lambda df: df[~df['conv_id'].isin(bad_convs)])\n",
    "    print(f\"After dropping substr: {len(user_queries_df['conv_id'].unique())} convs\")\n",
    "\n",
    "    # Combine into conv-level df\n",
    "    convs_df =\\\n",
    "        user_queries_df\\\n",
    "        .sort_values(['conv_id', 'user_query_ix'])\\\n",
    "        .assign(conv_id = lambda df: df.groupby('conv_id', sort = False).ngroup())\\\n",
    "        .groupby('conv_id')\\\n",
    "        .apply(lambda g: pd.Series({\n",
    "            'dataset': g['dataset'].values[0],\n",
    "            'messages': [\n",
    "                msg\n",
    "                for _, row in g.iterrows()\n",
    "                for msg in [\n",
    "                    {'role': 'user', 'content': row['user_query']},\n",
    "                    {'role': 'cot', 'content': row['cot']},\n",
    "                    {'role': 'assistant', 'content': row['assistant']}\n",
    "                ]\n",
    "            ]\n",
    "        }))\\\n",
    "        .reset_index()\\\n",
    "        [['conv_id', 'dataset', 'messages']]\\\n",
    "        .sample(n = max_samples, random_state = seed)\\\n",
    "        .assign(conv_id = lambda df: range(len(df)))\n",
    "\n",
    "    print(f\"Final: {len(convs_df['conv_id'].unique())} convs\")\n",
    "\n",
    "    # messages_df =\\\n",
    "    #     convs_df\\\n",
    "    #     .explode('messages')\\\n",
    "    #     .assign(message_ix = lambda df: df.groupby('conv_id').cumcount())\\\n",
    "    #     .assign(\n",
    "    #         role = lambda df: df['messages'].apply(lambda x: x['role']),\n",
    "    #         content = lambda df: df['messages'].apply(lambda x: x['content'])\n",
    "    #     )\\\n",
    "    #     .drop(columns = 'messages')\\\n",
    "    #     .reset_index(drop = True)\n",
    "\n",
    "    convs = convs_df['messages'].tolist()\n",
    "    return convs, convs_df\n",
    "\n",
    "convs, convs_df = filter_convs(\n",
    "    user_queries_df = pd.read_csv(f'{ws}/experiments/role-analysis/convs/{model_prefix}.csv'),\n",
    "    max_samples = 40\n",
    ")\n",
    "\n",
    "display(convs_df.head(5))\n",
    "print(f\"Total convs: {len(convs_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prep untagged conversations\n",
    "\"\"\"\n",
    "all_convs_by_type = {}\n",
    "\n",
    "# Prep conversations\n",
    "def prep_conv(conv):\n",
    "    return test_prefix + '\\n' + '\\n'.join([x['content'] for x in conv])\n",
    "\n",
    "all_convs_by_type['untagged'] = [prep_conv(conv) for conv in convs]\n",
    "print(all_convs_by_type['untagged'][0])\n",
    "\n",
    "# Find max input length\n",
    "max_input_length =\\\n",
    "    tokenizer(all_convs_by_type['untagged'], padding = True, truncation = True, max_length = 1024 * 6, return_tensors = 'pt')\\\n",
    "    ['attention_mask'].sum(dim = 1).max().item()\n",
    "\n",
    "print(f\"Max input length: {max_input_length}\")\n",
    "print(f\"Untagged convs: {len(all_convs_by_type['untagged'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prep tagged conversations\n",
    "\"\"\"\n",
    "def fold_cot_into_final(convs):\n",
    "    \"\"\"Fold CoT into the following assistant message as a <think></think> tag.\"\"\"\n",
    "    result = []\n",
    "    for conv in convs:\n",
    "        new_conv = []\n",
    "        it = iter(enumerate(conv))\n",
    "        for i, msg in it:\n",
    "            if msg['role'] == 'cot':\n",
    "                if i + 1 >= len(conv) or conv[i + 1]['role'] != 'assistant':\n",
    "                    raise ValueError(\"cot must be followed by assistant\")\n",
    "                _, next_msg = next(it)\n",
    "                new_conv.append({'role': 'assistant', 'content': f\"<think>{msg['content']}</think>{next_msg['content']}\"})\n",
    "            else:\n",
    "                new_conv.append({'role': msg['role'], 'content': msg['content']})\n",
    "        result.append(new_conv)\n",
    "    return result\n",
    "\n",
    "final_convs = fold_cot_into_final(convs)\n",
    "all_convs_by_type['tagged'] = [\n",
    "    test_prefix + x\n",
    "    for x in tokenizer.apply_chat_template(final_convs, tokenize = False, add_generation_prompt = False)\n",
    "]\n",
    "print(all_convs_by_type['tagged'][0])\n",
    "\n",
    "# Find max input length\n",
    "max_input_length =\\\n",
    "    tokenizer(all_convs_by_type['tagged'], padding = True, truncation = True, max_length = 1024 * 8, return_tensors = 'pt')\\\n",
    "    ['attention_mask'].sum(dim = 1).max().item()\n",
    "\n",
    "print(f\"Max input length: {max_input_length}\")\n",
    "print(f\"Tagged convs: {len(all_convs_by_type['tagged'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prep mistagged conversations\n",
    "\"\"\"\n",
    "all_convs_by_type['user_tagged'] = [render_single_message(model_architecture, 'user', prep_conv(conv)) for conv in convs]\n",
    "print(all_convs_by_type['user_tagged'][0])\n",
    "\n",
    "all_convs_by_type['tool_tagged'] = [render_single_message(model_architecture, 'tool', prep_conv(conv)) for conv in convs]\n",
    "print(all_convs_by_type['tool_tagged'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run forward passes\n",
    "\"\"\"\n",
    "# Get input df\n",
    "input_convs_df =\\\n",
    "    pd.concat([\n",
    "        pd.DataFrame({'convs': v, 'conv_type': conv_type})\\\n",
    "            .assign(conv_id = lambda df: range(0, len(df)))\\\n",
    "            .merge(convs_df, how = 'inner', on = 'conv_id')\n",
    "        for conv_type, v in all_convs_by_type.items()\n",
    "    ], ignore_index = True)\\\n",
    "    .assign(prompt_ix = lambda df: list(range(0, len(df))))\n",
    "    \n",
    "display(input_convs_df)\n",
    "\n",
    "convs_dl = DataLoader(\n",
    "    ReconstructableTextDataset(input_convs_df['convs'].tolist(), tokenizer, max_length = max_input_length, prompt_ix = list(range(0, len(input_convs_df)))),\n",
    "    batch_size = 8,\n",
    "    shuffle = False,\n",
    "    collate_fn = stack_collate\n",
    ")\n",
    "\n",
    "convs_outputs = run_and_export_states(model, convs_dl, layers_to_keep_acts = layers_to_probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Label messages\n",
    "\"\"\"\n",
    "import utils.substring_assignments\n",
    "importlib.reload(utils.substring_assignments)\n",
    "from utils.substring_assignments import flag_message_types\n",
    "\n",
    "sample_dfs_by_conv = [group for _, group in convs_outputs['sample_df'].groupby('prompt_ix', sort = True)]\n",
    "metadata_by_conv = input_convs_df.to_dict('records')\n",
    "\n",
    "all_res = []\n",
    "\n",
    "# Iterate through (input metadata, token-level sample df) pairs\n",
    "for conv_metadata, sample_df_for_conv in tqdm(zip(metadata_by_conv, sample_dfs_by_conv)):\n",
    "\n",
    "    content_spans = [msg['content'] for msg in conv_metadata['messages']]\n",
    "    content_roles = [msg['role'] for msg in conv_metadata['messages']]\n",
    "    \n",
    "    try:\n",
    "        # Note: set the last arg of flag_message_types for Olmo3/other models whose CoTs quote the FULL user message (one base_message is a subset of another)\n",
    "        # Setting the flag causes it to default to the first matching BMT (which should be user)\n",
    "        res = \\\n",
    "            flag_message_types(sample_df_for_conv, content_spans, any(x in model_prefix for x in ['olmo', 'glm']))\\\n",
    "            .merge(\n",
    "                pd.DataFrame({'role': content_roles, 'base_message_ix': range(len(content_spans))}),\n",
    "                on = 'base_message_ix',\n",
    "                how = 'left'\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    \n",
    "    all_res.append(res)\n",
    "\n",
    "display(pd.concat(all_res).pipe(lambda df: df[df['role'].isna()]))\n",
    "        \n",
    "sample_df_labeled =\\\n",
    "    pd.concat(all_res).reset_index(drop = True)\\\n",
    "    .assign(sample_ix = lambda df: range(0, len(df)))\\\n",
    "    .assign(token_in_prompt_ix = lambda df: df.groupby(['prompt_ix']).cumcount())\n",
    "\n",
    "display(sample_df_labeled.pipe(lambda df: df[~df['role'].isna()]))\n",
    "\n",
    "display(\n",
    "    sample_df_labeled\\\n",
    "    .pipe(lambda df: df[df['prompt_ix'] <= 5])\\\n",
    "    .groupby(['prompt_ix', 'base_message', 'role'], as_index = False)\\\n",
    "    .agg(combined_text = ('token', ''.join))\\\n",
    "    .assign(eot = lambda df: df['combined_text'].str[-30:])\n",
    ")\n",
    "\n",
    "display(\n",
    "    sample_df_labeled\\\n",
    "    .pipe(lambda df: df[~df['role'].isna()])\\\n",
    "    .groupby(['prompt_ix', 'role'], as_index = False)\\\n",
    "    .agg(count = ('sample_ix', 'count'))\\\n",
    "    .pivot(index = ['prompt_ix'], columns = 'role', values = 'count')\\\n",
    "    .head(20)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Clean up conversation pre_mlp_hs\n",
    "\"\"\"\n",
    "convs_hs = convs_outputs['all_hs'].to(torch.float16)\n",
    "convs_hs = {layer_ix: convs_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(layers_to_probe)} # Match layers_to_keep_act\n",
    "print(convs_hs[0].shape)\n",
    "print(len(sample_df_labeled))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run test projections for single layer\n",
    "\"\"\"\n",
    "def run_projections(valid_sample_df, layer_hs, probe):\n",
    "    \"\"\"\n",
    "    Run probe-level projections\n",
    "    \n",
    "    Params:\n",
    "        @valid_sample_df: A sample-level df with columns `sample_ix` (1... T - 1), `sample_ix`.\n",
    "            Can be shorter than full T - 1 due to pre-filters, as long as sample_ix corresponds to the full length.\n",
    "        @layer_hs: A tensor of size T x D for the layer to project.\n",
    "        @probe: The probe dict with keys `probe` (the trained model) and `roles` (the roles list)\n",
    "    \n",
    "    Returns:\n",
    "        A df at (sample_ix, role_space) level with cols `sample_ix`, `role_space`, `prob`\n",
    "    \"\"\"\n",
    "    x_cp = cupy.asarray(layer_hs[valid_sample_df['sample_ix'].tolist(), :])\n",
    "    y_cp = probe['probe'].predict_proba(x_cp).round(8)\n",
    "\n",
    "    proj_results = pd.DataFrame(cupy.asnumpy(y_cp), columns = probe['roles'])\n",
    "    if len(proj_results) != len(valid_sample_df):\n",
    "        raise Exception(\"Error!\")\n",
    "\n",
    "    role_df =\\\n",
    "        pd.concat([\n",
    "            proj_results.reset_index(drop = True),\n",
    "            valid_sample_df[['sample_ix']].reset_index(drop = True)\n",
    "        ], axis = 1)\\\n",
    "        .melt(id_vars = ['sample_ix'], var_name = 'role_space', value_name = 'prob')\\\n",
    "        .reset_index(drop = True)\n",
    "\n",
    "    return role_df\n",
    "\n",
    "# top_train_layer =\\\n",
    "#     pd.DataFrame(all_probes)\\\n",
    "#     .assign(probe_ix = lambda df: range(0, len(df)))\\\n",
    "#     .pipe(lambda df: df[df['roles'].apply(lambda r: sorted(r) == sorted(test_roles))])\\\n",
    "#     .pipe(lambda df: df.nlargest(1, 'acc')).to_dict('records')[0]\n",
    "# print(f\"Top layer: {top_train_layer['layer_ix']}\")\n",
    "# test_layer = top_train_layer['layer_ix']\n",
    "\n",
    "test_roles = ['system', 'user', 'cot', 'assistant'] if model_is_reasoning else ['system', 'user', 'assistant']\n",
    "test_layer_ix = 16\n",
    "\n",
    "test_projections =\\\n",
    "    run_projections(\n",
    "        valid_sample_df = sample_df_labeled.pipe(lambda df: df[(~df['role'].isna()) & (df['role'].isin(test_roles))]),\n",
    "        layer_hs = convs_hs[test_layer_ix],\n",
    "        probe = [x for x in all_probes if sorted(x['roles']) == sorted(test_roles) and x['layer_ix'] == test_layer_ix][0]\n",
    "    )\\\n",
    "    .merge(sample_df_labeled[['prompt_ix', 'sample_ix', 'token_in_prompt_ix', 'token', 'role']], how = 'inner', on = ['sample_ix'])\\\n",
    "    .merge(input_convs_df[['prompt_ix', 'dataset', 'conv_id', 'conv_type']], how = 'inner', on = 'prompt_ix', validate = 'many_to_one')\n",
    "\n",
    "\n",
    "# Now aggregate up to (prompt, role, role_space) level\n",
    "prompt_x_role_x_role_space_projections =\\\n",
    "    test_projections\\\n",
    "    .groupby(['conv_type', 'prompt_ix', 'role', 'role_space'], as_index = False)\\\n",
    "    .agg(\n",
    "        combined_text = ('token', ''.join),\n",
    "        mean_prob = ('prob', lambda x: np.mean(x).round(3))    \n",
    "    )\\\n",
    "    .assign(eot = lambda df: df['combined_text'].str[-30:])\\\n",
    "\n",
    "display(prompt_x_role_x_role_space_projections.head(12))\n",
    "\n",
    "display(\n",
    "    prompt_x_role_x_role_space_projections\\\n",
    "    .groupby(['conv_type', 'role_space', 'role'], as_index = False)\\\n",
    "    .agg(mean_prob = ('mean_prob', 'mean'))\\\n",
    "    .pivot(index = ['conv_type', 'role'], columns = 'role_space', values = 'mean_prob')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_projections\\\n",
    "    .pipe(lambda df: df[df['prompt_ix'] == 30])\\\n",
    "    .pipe(lambda df: df[df['role'] == 'cot'])\\\n",
    "    .groupby('role_space')\\\n",
    "    .agg(combined_text = ('token', ''.join))\n",
    "    ['combined_text'].tolist()[0]\n",
    ")\n",
    "print('-----------')\n",
    "print(test_projections\\\n",
    "    # .pipe(lambda df: df[df['conv_type'] == 'tagged'])\\\n",
    "    .pipe(lambda df: df[df['prompt_ix'] == len(convs) + 30])\\\n",
    "    .pipe(lambda df: df[df['role'] == 'cot'])\\\n",
    "    .groupby('role_space')\\\n",
    "    .agg(combined_text = ('token', ''.join))\\\n",
    "    ['combined_text'].tolist()[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "All-layer projections\n",
    "\"\"\"\n",
    "clear_all_cuda_memory()\n",
    "gc.collect()\n",
    "\n",
    "all_projs = [\n",
    "    run_projections(\n",
    "        valid_sample_df = sample_df_labeled.pipe(lambda df: df[(~df['role'].isna()) & (df['role'].isin(probe['roles']))]),\n",
    "        layer_hs = convs_hs[probe['layer_ix']],\n",
    "        probe = probe\n",
    "    ).assign(layer_ix = probe['layer_ix'], roles = ''.join([x[0] for x in probe['roles']]))\n",
    "\n",
    "    for probe in tqdm(all_probes)\n",
    "]\n",
    "\n",
    "all_projs_df =\\\n",
    "    pd.concat(all_projs, ignore_index = True)\\\n",
    "    .merge(sample_df_labeled[['prompt_ix', 'sample_ix', 'token_in_prompt_ix', 'token', 'role']], how = 'inner', on = ['sample_ix'])\\\n",
    "    .merge(input_convs_df[['prompt_ix', 'dataset', 'conv_id', 'conv_type']], how = 'inner', on = 'prompt_ix', validate = 'many_to_one')\n",
    "\n",
    "all_projs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Validation\n",
    "\"\"\"\n",
    "out =\\\n",
    "    all_projs_df\\\n",
    "    .pipe(lambda df: df[df['role_space'] == df['role']])\\\n",
    "    .drop(columns = 'role_space')\\\n",
    "    .rename(columns = {'prob': 'acc'})\\\n",
    "    .groupby(['conv_type', 'roles', 'layer_ix', 'role', 'prompt_ix'], as_index = False)\\\n",
    "    .agg(mean_acc = ('acc', 'mean'))\\\n",
    "    .groupby(['conv_type', 'roles', 'layer_ix', 'role'], as_index = False)\\\n",
    "    .agg(mean_acc = ('mean_acc', 'mean'))\\\n",
    "    .assign(\n",
    "        role = lambda df: np.where(df['role'] == 'assistant', 'asst', df['role'])\n",
    "    )\\\n",
    "    .pivot(index = ['conv_type', 'layer_ix'], columns = ['roles', 'role'], values = 'mean_acc')\n",
    "\n",
    "def color_thresholds(v):\n",
    "    if pd.isna(v):\n",
    "        return \"\"\n",
    "    if v > 0.9:\n",
    "        return \"background-color: #00c853; color: black;\" # green\n",
    "    elif v > 0.75:\n",
    "        return \"background-color: #aeea00; color: black;\" # green-yellow\n",
    "    elif v > 0.6:\n",
    "        return \"background-color: #ffeb3b; color: black;\" # yellow\n",
    "    return \"\"\n",
    "\n",
    "col_groups = out.columns.get_level_values(0)\n",
    "group_starts = [0] + [i for i in range(1, len(col_groups)) if col_groups[i] != col_groups[i-1]]\n",
    "divider_styles = []\n",
    "for i in group_starts:\n",
    "    divider_styles += [\n",
    "        {\"selector\": f\"td.col{i}\", \"props\": [(\"border-left\", \"3px solid #ddd\")]},\n",
    "        {\"selector\": f\"th.col_heading.col{i}\", \"props\": [(\"border-left\", \"3px solid #ddd\")]},\n",
    "    ]\n",
    "\n",
    "display(out.style.format(\"{:.2f}\").map(color_thresholds).set_table_styles(divider_styles, overwrite = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Actual accuracy (instead of averaged roleness)\n",
    "\"\"\"\n",
    "# Take argmax role_space per token for each (roles, layer_ix)\n",
    "max_role_space_idxs =\\\n",
    "    all_projs_df\\\n",
    "    .groupby(['conv_type', 'roles', 'layer_ix', 'sample_ix'], sort = False)['prob']\\\n",
    "    .idxmax()\n",
    "\n",
    "pred_df =\\\n",
    "    all_projs_df.loc[max_role_space_idxs, ['conv_type', 'roles', 'layer_ix', 'sample_ix', 'prompt_ix', 'role', 'role_space']]\\\n",
    "    .rename(columns = {'role_space': 'pred_role'})\\\n",
    "    .assign(is_correct = lambda df: (df['pred_role'] == df['role']).astype(float))\n",
    "\n",
    "# Only score tokens whose true role is IN the probe label set for that 'roles' group. (ow roles like 'tool' will be forced-incorrect when not in labelset.)\n",
    "roles_padded = ',' + pred_df['roles'] + ','\n",
    "possible_roles = ['system', 'user', 'cot', 'assistant', 'tool']\n",
    "\n",
    "mask = np.zeros(len(pred_df), dtype=bool)\n",
    "for r in possible_roles:\n",
    "    mask |= (pred_df['role'].eq(r) & roles_padded.str.contains(f',{r},', regex = False))\n",
    "\n",
    "pred_df_scored = pred_df[mask].copy()\n",
    "\n",
    "out_argmax = (\n",
    "    pred_df_scored\n",
    "    .groupby(['conv_type', 'roles', 'layer_ix', 'role', 'prompt_ix'], as_index=False)\n",
    "    .agg(acc = ('is_correct', 'mean'))\n",
    "    .groupby(['conv_type', 'roles', 'layer_ix', 'role'], as_index=False)\n",
    "    .agg(mean_acc = ('acc', 'mean'))\n",
    "    .assign(\n",
    "        roles = lambda df: df['roles'].apply(lambda x: ''.join([c[0] for c in x.split(',')])),\n",
    "        role = lambda df: np.where(df['role'] == 'assistant', 'asst', df['role'])\n",
    "    )\n",
    "    .pivot(index = ['conv_type', 'layer_ix'], columns = ['roles', 'role'], values = 'mean_acc')\n",
    ")\n",
    "\n",
    "display(out_argmax.style.format(\"{:.2f}\").map(color_thresholds).set_table_styles(divider_styles, overwrite = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save and export\n",
    "\"\"\"\n",
    "all_projs_df.to_csv(f'{ws}/experiments/role-analysis/probes/all_conv_projs_{model_prefix}.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrong convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prep wrong-convs\n",
    "\"\"\"\n",
    "all_model_files = [\n",
    "    'kimik2'\n",
    "]\n",
    "\n",
    "for m in all_model_files:\n",
    "    m_df = pd.read_csv(f'{ws}/experiments/role-analysis/convs/{m}.csv')\n",
    "\n",
    "alt_convs, alt_convs_df = filter_convs(m_df, max_samples = 20)\n",
    "\n",
    "final_alt_convs = fold_cot_into_final(alt_convs)\n",
    "all_wrong_convs_tagged = [\n",
    "    test_prefix + x\n",
    "    for x in tokenizer.apply_chat_template(final_alt_convs, tokenize = False, add_generation_prompt = False)\n",
    "]\n",
    "print(all_wrong_convs_tagged[0])\n",
    "\n",
    "# Find max input length\n",
    "max_alt_input_length =\\\n",
    "    tokenizer(all_wrong_convs_tagged, padding = True, truncation = True, max_length = 1024 * 8, return_tensors = 'pt')\\\n",
    "    ['attention_mask'].sum(dim = 1).max().item()\n",
    "\n",
    "print(f\"Max input length: {max_alt_input_length}\")\n",
    "print(f\"Tagged convs: {len(all_wrong_convs_tagged)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run forward passes\n",
    "\"\"\"\n",
    "# Get input df\n",
    "input_alt_convs_df =\\\n",
    "    pd.concat([\n",
    "        pd.DataFrame({'convs': v, 'conv_type': conv_type})\\\n",
    "            .assign(conv_id = lambda df: range(0, len(df)))\\\n",
    "            .merge(alt_convs_df, how = 'inner', on = 'conv_id')\n",
    "        for conv_type, v in {'alt_tagged': all_wrong_convs_tagged}.items()\n",
    "    ], ignore_index = True)\\\n",
    "    .assign(prompt_ix = lambda df: list(range(0, len(df))))\n",
    "    \n",
    "display(input_alt_convs_df)\n",
    "\n",
    "alt_convs_dl = DataLoader(\n",
    "    ReconstructableTextDataset(input_alt_convs_df['convs'].tolist(), tokenizer, max_length = max_input_length, prompt_ix = list(range(0, len(input_alt_convs_df)))),\n",
    "    batch_size = 8,\n",
    "    shuffle = False,\n",
    "    collate_fn = stack_collate\n",
    ")\n",
    "\n",
    "alt_convs_outputs = run_and_export_states(model, alt_convs_dl, layers_to_keep_acts = layers_to_probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Label messages\n",
    "\"\"\"\n",
    "import utils.substring_assignments\n",
    "importlib.reload(utils.substring_assignments)\n",
    "from utils.substring_assignments import flag_message_types\n",
    "\n",
    "alt_sample_dfs_by_conv = [group for _, group in alt_convs_outputs['sample_df'].groupby('prompt_ix', sort = True)]\n",
    "alt_metadata_by_conv = input_alt_convs_df.to_dict('records')\n",
    "\n",
    "all_res = []\n",
    "\n",
    "# Iterate through (input metadata, token-level sample df) pairs\n",
    "for conv_metadata, sample_df_for_conv in tqdm(zip(alt_metadata_by_conv, alt_sample_dfs_by_conv)):\n",
    "\n",
    "    content_spans = [msg['content'] for msg in conv_metadata['messages']]\n",
    "    content_roles = [msg['role'] for msg in conv_metadata['messages']]\n",
    "    \n",
    "    try:\n",
    "        # Note: set the last arg of flag_message_types for Olmo3/other models whose CoTs quote the FULL user message (one base_message is a subset of another)\n",
    "        # Setting the flag causes it to default to the first matching BMT (which should be user)\n",
    "        res = \\\n",
    "            flag_message_types(sample_df_for_conv, content_spans, any(x in model_prefix for x in ['olmo', 'glm']))\\\n",
    "            .merge(\n",
    "                pd.DataFrame({'role': content_roles, 'base_message_ix': range(len(content_spans))}),\n",
    "                on = 'base_message_ix',\n",
    "                how = 'left'\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    \n",
    "    all_res.append(res)\n",
    "\n",
    "display(pd.concat(all_res).pipe(lambda df: df[df['role'].isna()]))\n",
    "        \n",
    "alt_sample_df_labeled =\\\n",
    "    pd.concat(all_res).reset_index(drop = True)\\\n",
    "    .assign(sample_ix = lambda df: range(0, len(df)))\\\n",
    "    .assign(token_in_prompt_ix = lambda df: df.groupby(['prompt_ix']).cumcount())\n",
    "\n",
    "display(alt_sample_df_labeled.pipe(lambda df: df[~df['role'].isna()]))\n",
    "\n",
    "display(\n",
    "    alt_sample_df_labeled\\\n",
    "    .pipe(lambda df: df[df['prompt_ix'] <= 5])\\\n",
    "    .groupby(['prompt_ix', 'base_message', 'role'], as_index = False)\\\n",
    "    .agg(combined_text = ('token', ''.join))\\\n",
    "    .assign(eot = lambda df: df['combined_text'].str[-30:])\n",
    ")\n",
    "\n",
    "display(\n",
    "    alt_sample_df_labeled\\\n",
    "    .pipe(lambda df: df[~df['role'].isna()])\\\n",
    "    .groupby(['prompt_ix', 'role'], as_index = False)\\\n",
    "    .agg(count = ('sample_ix', 'count'))\\\n",
    "    .pivot(index = ['prompt_ix'], columns = 'role', values = 'count')\\\n",
    "    .head(20)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Clean up conversation pre_mlp_hs\n",
    "\"\"\"\n",
    "alt_convs_hs = alt_convs_outputs['all_hs'].to(torch.float16)\n",
    "alt_convs_hs = {layer_ix: alt_convs_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(layers_to_probe)} # Match layers_to_keep_act\n",
    "print(alt_convs_hs[0].shape)\n",
    "print(len(alt_sample_df_labeled))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "All-layer projections\n",
    "\"\"\"\n",
    "clear_all_cuda_memory()\n",
    "gc.collect()\n",
    "\n",
    "alt_projs = [\n",
    "    run_projections(\n",
    "        valid_sample_df = alt_sample_df_labeled.pipe(lambda df: df[(~df['role'].isna()) & (df['role'].isin(probe['roles']))]),\n",
    "        layer_hs = alt_convs_hs[probe['layer_ix']],\n",
    "        probe = probe\n",
    "    ).assign(layer_ix = probe['layer_ix'], roles = ''.join([x[0] for x in probe['roles']]))\n",
    "\n",
    "    for probe in tqdm(all_probes)\n",
    "]\n",
    "\n",
    "alt_projs_df =\\\n",
    "    pd.concat(all_projs, ignore_index = True)\\\n",
    "    .merge(alt_sample_df_labeled[['prompt_ix', 'sample_ix', 'token_in_prompt_ix', 'token', 'role']], how = 'inner', on = ['sample_ix'])\\\n",
    "    .merge(input_alt_convs_df[['prompt_ix', 'dataset', 'conv_id', 'conv_type']], how = 'inner', on = 'prompt_ix', validate = 'many_to_one')\n",
    "\n",
    "alt_projs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Validation\n",
    "\"\"\"\n",
    "out =\\\n",
    "    alt_projs_df\\\n",
    "    .pipe(lambda df: df[df['role_space'] == df['role']])\\\n",
    "    .drop(columns = 'role_space')\\\n",
    "    .rename(columns = {'prob': 'acc'})\\\n",
    "    .groupby(['conv_type', 'roles', 'layer_ix', 'role', 'prompt_ix'], as_index = False)\\\n",
    "    .agg(mean_acc = ('acc', 'mean'))\\\n",
    "    .groupby(['conv_type', 'roles', 'layer_ix', 'role'], as_index = False)\\\n",
    "    .agg(mean_acc = ('mean_acc', 'mean'))\\\n",
    "    .assign(\n",
    "        role = lambda df: np.where(df['role'] == 'assistant', 'asst', df['role'])\n",
    "    )\\\n",
    "    .pivot(index = ['conv_type', 'layer_ix'], columns = ['roles', 'role'], values = 'mean_acc')\n",
    "\n",
    "def color_thresholds(v):\n",
    "    if pd.isna(v):\n",
    "        return \"\"\n",
    "    if v > 0.9:\n",
    "        return \"background-color: #00c853; color: black;\" # green\n",
    "    elif v > 0.75:\n",
    "        return \"background-color: #aeea00; color: black;\" # green-yellow\n",
    "    elif v > 0.6:\n",
    "        return \"background-color: #ffeb3b; color: black;\" # yellow\n",
    "    return \"\"\n",
    "\n",
    "col_groups = out.columns.get_level_values(0)\n",
    "group_starts = [0] + [i for i in range(1, len(col_groups)) if col_groups[i] != col_groups[i-1]]\n",
    "divider_styles = []\n",
    "for i in group_starts:\n",
    "    divider_styles += [\n",
    "        {\"selector\": f\"td.col{i}\", \"props\": [(\"border-left\", \"3px solid #ddd\")]},\n",
    "        {\"selector\": f\"th.col_heading.col{i}\", \"props\": [(\"border-left\", \"3px solid #ddd\")]},\n",
    "    ]\n",
    "\n",
    "display(out.style.format(\"{:.2f}\").map(color_thresholds).set_table_styles(divider_styles, overwrite = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tomato example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define append prefix\n",
    "\"\"\"\n",
    "tomato_prefix = ''# tokenizer.bos_token\n",
    "\n",
    "tomato_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Format tomato-style conversations\n",
    "\"\"\"\n",
    "import yaml\n",
    "\n",
    "def load_messages(yaml_path, model_key):\n",
    "    \"\"\"Loads system-like, assistant-like, user-like, etc. messages\"\"\"\n",
    "    with open(yaml_path, 'r', encoding = 'utf-8') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    return [x['content'] for x in data[model_key]]\n",
    "\n",
    "base_message_types = ['system', 'user', 'cot', 'assistant', 'user', 'cot', 'assistant']\n",
    "base_messages = load_messages(f\"{ws}/experiments/role-analysis/prompts/standard-conversations.yaml\", 'gptoss20')\n",
    "\n",
    "tomato_prompts = {}\n",
    "\n",
    "tomato_prompts['basic_no_format'] = tomato_prefix + '\\n'.join(base_messages)\n",
    "\n",
    "tomato_prompts['everything_in_user_tags'] = tomato_prefix + tokenizer.apply_chat_template(\n",
    "    [{'role': 'user', 'content': '\\n'.join(base_messages)}],\n",
    "    tokenize = False, add_generation_prompt = False\n",
    ")\n",
    "\n",
    "tomato_prompts['proper_tags'] = tomato_prefix + tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {'role': 'system', 'content': base_messages[0]},\n",
    "        {'role': 'user', 'content': base_messages[1]},\n",
    "        {'role': 'assistant', 'content': f\"<think>{base_messages[2]}</think>{base_messages[3]}\"},\n",
    "        {'role': 'user', 'content': base_messages[4]},\n",
    "        {'role': 'assistant', 'content': f\"<tool_call>{base_messages[5]}<tool_call>{base_messages[6]}\"}\n",
    "    ],\n",
    "    tokenize = False, add_generation_prompt = False\n",
    ")\n",
    "\n",
    "print(tomato_prompts['proper_tags'])\n",
    "\n",
    "tomato_input_df =\\\n",
    "    pd.DataFrame({\n",
    "        'prompt': [p for _, p in tomato_prompts.items()],\n",
    "        'prompt_key': [pk for pk, _ in tomato_prompts.items()]\n",
    "    })\\\n",
    "    .assign(prompt_ix = lambda df: list(range(0, len(df))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run!!\n",
    "\"\"\"\n",
    "tomato_dl = DataLoader(\n",
    "    ReconstructableTextDataset(tomato_input_df['prompt'].tolist(), tokenizer, max_length = 512 * 4, prompt_ix = tomato_input_df['prompt_ix'].tolist()),\n",
    "    batch_size = 16,\n",
    "    shuffle = False,\n",
    "    collate_fn = stack_collate\n",
    ")\n",
    "\n",
    "tomato_outputs = run_and_export_states(model, tomato_dl, layers_to_keep_acts = layers_to_probe)\n",
    "\n",
    "tomato_hs = tomato_outputs['all_hs'].to(torch.float16)\n",
    "tomato_hs = {layer_ix: tomato_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(layers_to_probe)} # Match layers_to_keep_act\n",
    "print(tomato_hs[0].shape)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ensure tokens/section is reasonable; map to base_message_type\n",
    "\"\"\"\n",
    "tomato_sample_df_labeled =\\\n",
    "    flag_message_types(tomato_outputs['sample_df'], base_messages)\\\n",
    "    .assign(sample_ix = lambda df: range(0, len(df)))\\\n",
    "    .assign(token_in_prompt_ix = lambda df: df.groupby(['prompt_ix']).cumcount())\\\n",
    "    .merge(tomato_input_df, on = 'prompt_ix', how = 'inner')\\\n",
    "    .merge(\n",
    "        pd.DataFrame({'base_message_ix': list(range(len(base_message_types))), 'base_message_type': base_message_types}),\n",
    "        on = 'base_message_ix',\n",
    "        how = 'inner'\n",
    "    )\n",
    "\n",
    "tomato_sample_df_labeled\\\n",
    "    .groupby(['prompt_ix', 'prompt_key', 'base_message_ix', 'base_message_type', 'base_message'], as_index = False)\\\n",
    "    .agg(\n",
    "        count = ('token', 'count'),\n",
    "        combined_text = ('token', ''.join)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "tomato_projs = [\n",
    "    run_projections(\n",
    "        valid_sample_df = tomato_sample_df_labeled.pipe(lambda df: df[(~df['base_message_type'].isna()) & (df['base_message_type'].isin(probe['roles']))]),\n",
    "        layer_hs = tomato_hs[probe['layer_ix']],\n",
    "        probe = probe\n",
    "    ).assign(layer_ix = probe['layer_ix'], roles = ''.join([x[0] for x in probe['roles']]))\n",
    "\n",
    "    for probe in tqdm(all_probes)\n",
    "]\n",
    "\n",
    "tomato_projs_df =\\\n",
    "    pd.concat(tomato_projs, ignore_index = True)\\\n",
    "    .merge(\n",
    "        tomato_sample_df_labeled[['sample_ix', 'prompt_ix', 'prompt_key', 'base_message_ix', 'base_message_type', 'token_in_prompt_ix', 'token']],\n",
    "        how = 'inner',\n",
    "        on = ['sample_ix']\n",
    "    )\n",
    "\n",
    "tomato_projs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomato_projs_df\\\n",
    "    .groupby(['prompt_ix', 'prompt_key', 'role_space', 'layer_ix', 'roles', 'sample_ix'], as_index = False)\\\n",
    "    .agg(count = ('sample_ix', 'count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "facet_order = ['system', 'user', 'cot', 'assistant']\n",
    "tomato_projs_df_single = tomato_projs_df.pipe(lambda df: df[(df['roles'] == 'suca') & (df['layer_ix'] == 12)])\n",
    "\n",
    "all_message_types = tomato_projs_df['base_message_type'].unique()\n",
    "color_map = {msg_type: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] for i, msg_type in enumerate(all_message_types)}\n",
    "\n",
    "group_cols = ['prompt_ix','prompt_key','base_message_type','role_space']\n",
    "\n",
    "smooth_df = (\n",
    "    tomato_projs_df_single\n",
    "    .sort_values(group_cols + ['sample_ix'])\n",
    "    .assign(\n",
    "        prob_sma = lambda df: (\n",
    "            df.groupby(group_cols)['prob']\n",
    "              .rolling(window=10, min_periods=1).mean()\n",
    "              .reset_index(level=list(range(len(group_cols))), drop=True)\n",
    "        ),\n",
    "        prob_ewma = lambda df: (\n",
    "            df.groupby(group_cols)['prob']\n",
    "              .ewm(alpha=.5, min_periods=1).mean()\n",
    "              .reset_index(level=list(range(len(group_cols))), drop=True)\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "display(smooth_df)\n",
    "\n",
    "for this_prompt_key in smooth_df['prompt_key'].unique().tolist():\n",
    "    this_df = smooth_df.pipe(lambda df: df[df['prompt_key'] == this_prompt_key])\n",
    "    fig = px.scatter(\n",
    "        this_df, x = 'token_in_prompt_ix', y = 'prob',\n",
    "        facet_row = 'role_space',\n",
    "        color = 'base_message_type',\n",
    "        color_discrete_map = color_map,\n",
    "        category_orders = {\n",
    "            'role_space': facet_order,\n",
    "            'base_message_type': ['system', 'user', 'cot', 'assistant', 'other']\n",
    "        },\n",
    "        hover_name = 'token',\n",
    "        hover_data = {\n",
    "            'prob': ':.3f'\n",
    "        },\n",
    "        # markers = True,\n",
    "        title = f'prompt = {this_prompt_key}',\n",
    "        labels = {\n",
    "            'token_in_prompt_ix': 'Token Index',\n",
    "            'prob': 'Prob',\n",
    "            'prob_smoothed': 'Smoothed Prob',\n",
    "            'role_space': 'role'\n",
    "        }\n",
    "    ).update_yaxes(\n",
    "        range = [0, 1],\n",
    "        side = 'left'\n",
    "    ).update_layout(height = 500, width = 800)\n",
    "\n",
    "    def pretty(a):\n",
    "        a.update(\n",
    "            text = a.text.split('=')[-1],\n",
    "            x = 0.5, xanchor = \"center\",\n",
    "            y = a.y + 0.08,\n",
    "            textangle = 0,\n",
    "            font = dict(size = 12),\n",
    "            bgcolor = 'rgba(255, 255, 255, 0.9)',\n",
    "            showarrow = False\n",
    "        )\n",
    "\n",
    "    fig.for_each_annotation(pretty)\n",
    "    fig.update_yaxes(title_text = None)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomato_projs_df.to_csv(f\"{ws}/experiments/role-analysis/projections/tomato-role-projections-{model_prefix}.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

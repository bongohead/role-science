{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beade33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create role probes and tests them\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb75c7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cupy\n",
    "import cuml\n",
    "import importlib\n",
    "import gc\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.role_assignments import label_content_roles\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "\n",
    "clear_all_cuda_memory()\n",
    "check_memory()\n",
    "\n",
    "ws = '/workspace/deliberative-alignment-jailbreaks'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed26db3f",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75666ea5",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80af64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "selected_model_index = 0\n",
    "\n",
    "def get_model(index):\n",
    "    # HF model ID, model prefix, model architecture,  attn implementation, whether to use hf lib implementation\n",
    "    models = {\n",
    "        0: ('openai/gpt-oss-20b', 'gptoss20', 'gptoss', 'kernels-community/vllm-flash-attn3', True, 24),\n",
    "        1: ('openai/gpt-oss-120b', 'gptoss120', 'gptoss', 'kernels-community/vllm-flash-attn3', True, 36),\n",
    "        2: ('Qwen/Qwen3-30B-A3B-Thinking-2507', 'qwen3-30b-a3b', 'qwen3moe', None, True, 48),\n",
    "        3: ('zai-org/GLM-4.5-Air-FP8', 'glm4moe', 'glm4moe', None, True, 45) # MoE layers\n",
    "    }\n",
    "    return models[index]\n",
    "\n",
    "def load_model_and_tokenizer(model_id, model_prefix, model_attn, model_use_hf):\n",
    "    \"\"\"\n",
    "    Load the model and tokenizer from HF, or from file if already downloaded.\n",
    "    \"\"\"\n",
    "    cache_dir = '/workspace/hf'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir = cache_dir, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, cache_dir = cache_dir, dtype = torch.bfloat16, trust_remote_code = not model_use_hf, device_map = 'auto', attn_implementation = model_attn).eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "model_id, model_prefix, model_architecture, model_attn, model_use_hf, model_n_layers = get_model(selected_model_index)\n",
    "tokenizer, model = load_model_and_tokenizer(model_id, model_prefix, model_attn, model_use_hf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02219c6",
   "metadata": {},
   "source": [
    "# Train probe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e11e85b",
   "metadata": {},
   "source": [
    "## Load probe training activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0533d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load SFT dataset (probe training data)\n",
    "\"\"\"\n",
    "def load_data(folder_path, model_prefix, max_data_files):\n",
    "    \"\"\"\n",
    "    Load data saved by `export-c4-activations.ipynb`\n",
    "    \"\"\"\n",
    "    folders = [f'{ws}/experiments/role-analysis/{folder_path}/{model_prefix}/{i:02d}' for i in range(max_data_files)]\n",
    "    folders = [f for f in folders if os.path.isdir(f)]\n",
    "\n",
    "    all_pre_mlp_hs = []\n",
    "    sample_df = []\n",
    "    topk_df = []\n",
    "\n",
    "    for f in tqdm(folders):\n",
    "        sample_df.append(pd.read_pickle(f'{f}/samples.pkl'))\n",
    "        topk_df.append(pd.read_pickle(f'{f}/topks.pkl'))\n",
    "        all_pre_mlp_hs.append(torch.load(f'{f}/all-pre-mlp-hidden-states.pt'))\n",
    "\n",
    "    sample_df = pd.concat(sample_df)    \n",
    "    topk_df = pd.concat(topk_df)\n",
    "    all_pre_mlp_hs = torch.concat(all_pre_mlp_hs)    \n",
    "\n",
    "    with open(f'{ws}/experiments/role-analysis/{folder_path}/{model_prefix}/metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    gc.collect()\n",
    "    return sample_df, topk_df, all_pre_mlp_hs, metadata['all_pre_mlp_hidden_states_layers']\n",
    "\n",
    "sample_df_import, topk_df_import, all_pre_mlp_hs_import, act_map = load_data('activations', model_prefix, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845a89a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's clean up the mappings here. We'll get everything to a sample_ix level first. We'll also flag the role wrapper tokens. Drop nothing. \n",
    "\"\"\"\n",
    "def clean_mappings(model_architecture, sample_df_import, topk_df_import):\n",
    "    \"\"\"\n",
    "    We'll read the role mappings the raw way, instead of just using the input_mappings file. Then we can re-use the same logic for later on.\n",
    "    \n",
    "    Note:\n",
    "        Technically this could be done easily with the input_mappings file with \n",
    "        .merge(input_mappings[['prompt_ix', 'question_ix', 'role']], how = 'left', on = ['prompt_ix']), but this is more robust, \n",
    "        and doesn't assume one role per sequence/prompt_ix.\n",
    "    \"\"\"\n",
    "    # input_mappings = pd.read_csv(f'{ws}/experiments/role-analysis/{folder_path}/{model_prefix}/input_mappings.csv')\n",
    "\n",
    "    sample_df_raw =\\\n",
    "        label_content_roles(model_architecture, sample_df_import)\\\n",
    "        .assign(sample_ix = lambda df: df.groupby(['batch_ix', 'sequence_ix', 'token_ix']).ngroup())\\\n",
    "        .assign(token_in_seq_ix = lambda df: df.groupby(['prompt_ix']).cumcount())\\\n",
    "        .assign(token_in_role_ix = lambda df: df.groupby(['prompt_ix', 'role']).cumcount())\n",
    "\n",
    "    c4_topk_df =\\\n",
    "        topk_df_import\\\n",
    "        .merge(\n",
    "            sample_df_raw[['sample_ix', 'prompt_ix', 'batch_ix', 'sequence_ix', 'token_ix']],\n",
    "            how = 'inner',\n",
    "            on = ['sequence_ix', 'token_ix', 'batch_ix']\n",
    "        )\\\n",
    "        .drop(columns = ['sequence_ix', 'token_ix', 'batch_ix'])\n",
    "\n",
    "    c4_sample_df =\\\n",
    "        sample_df_raw\\\n",
    "        .drop(columns = ['batch_ix', 'sequence_ix'])\n",
    "\n",
    "    return c4_sample_df, c4_topk_df\n",
    "\n",
    "sample_df, topk_df = clean_mappings(model_architecture, sample_df_import, topk_df_import)\n",
    "del sample_df_import, topk_df_import\n",
    "\n",
    "gc.collect()\n",
    "display(sample_df)\n",
    "display(topk_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81cc749",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert activations to fp16 (for compatibility with cupy later) + layer-wise dict\n",
    "\"\"\"\n",
    "all_pre_mlp_hs = all_pre_mlp_hs_import.to(torch.float16)\n",
    "# compare_bf16_fp16_batched(all_pre_mlp_hs_import, all_pre_mlp_hs)\n",
    "del all_pre_mlp_hs_import\n",
    "all_pre_mlp_hs = {layer_ix: all_pre_mlp_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(act_map)}\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fa9433",
   "metadata": {},
   "source": [
    "## Run LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac139eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run logistic regression probes to get role space models\n",
    "\"\"\"\n",
    "def fit_lr(x_cp, y_cp):\n",
    "    \"\"\"\n",
    "    Fit a probe with a standard 80/20 split\n",
    "    \"\"\"\n",
    "    x_train, x_test, y_train, y_test = cuml.train_test_split(x_cp, y_cp, test_size = 0.2, random_state = 123)\n",
    "    lr_model = cuml.linear_model.LogisticRegression(penalty = 'l2', C = 0.0001, max_iter = 1000, fit_intercept = True)\n",
    "    lr_model.fit(x_train, y_train)\n",
    "    accuracy = lr_model.score(x_test, y_test)\n",
    "    return lr_model, accuracy\n",
    "\n",
    "def get_probe_result(layer_ix, label2id):\n",
    "    \"\"\"\n",
    "    Get probe results for a single layer and label combination\n",
    "\n",
    "    Params:\n",
    "        @layer_ix: The layer index to train the probe on\n",
    "        @label2id: The label-to-id mapping\n",
    "    \n",
    "    Description:\n",
    "        Trains only on content space for given roles\n",
    "    \"\"\"\n",
    "    # id2label = {v: k for k, v in label2id.items()}\n",
    "    roles = list(label2id.keys())\n",
    "\n",
    "    valid_sample_ix =\\\n",
    "        sample_df\\\n",
    "        .pipe(lambda df: df[(df['in_content_span'] == True) & (df['role'].notna()) & (df['role'].isin(roles))])\\\n",
    "        ['sample_ix'].tolist()\n",
    "\n",
    "    role_labels = [\n",
    "        label2id[role]\n",
    "        for role in sample_df[sample_df['sample_ix'].isin(valid_sample_ix)]['role'].tolist()\n",
    "    ]\n",
    "\n",
    "    role_labels_cp = cupy.asarray(role_labels)\n",
    "    x_cp = cupy.asarray(all_pre_mlp_hs[layer_ix][valid_sample_ix, :].to(torch.float16).detach().cpu())\n",
    "    lr_model, test_acc = fit_lr(x_cp, role_labels_cp)\n",
    "\n",
    "    print(test_acc)\n",
    "    \n",
    "    return {\n",
    "        'layer_ix': layer_ix, 'label2id': label2id,\n",
    "        'roles': roles, 'probe': lr_model, 'accuracy': test_acc\n",
    "    }\n",
    "\n",
    "layers_to_test = list(all_pre_mlp_hs.keys()) # [i for i in list(all_pre_mlp_hs.keys()) if i % 4 == 0]\n",
    "mappings_to_test = [\n",
    "    {'user': 0, 'assistant-cot': 1, 'assistant-final': 2},\n",
    "    {'system': 0, 'user': 1, 'assistant-cot': 2, 'assistant-final': 3},\n",
    "    {'system': 0, 'user': 1, 'assistant-cot': 2, 'assistant-final': 3, 'tool': 4},\n",
    "    {'user': 0, 'assistant-cot': 1, 'assistant-final': 2, 'tool': 3}\n",
    "]\n",
    "\n",
    "probes = []\n",
    "for layer_ix in tqdm(layers_to_test):\n",
    "    for mapping in mappings_to_test:\n",
    "        probes.append(get_probe_result(layer_ix, mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bcadc3",
   "metadata": {},
   "source": [
    "## Save probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9782f7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save\n",
    "\"\"\"\n",
    "with open(f'{ws}/experiments/role-analysis/probes/{model_prefix}.pkl', 'wb') as f:\n",
    "    pickle.dump(probes, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train probes\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Glm4vForConditionalGeneration, AutoModelForImageTextToText\n",
    "from transformers.loss.loss_utils import ForCausalLMLoss\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "from termcolor import colored\n",
    "import importlib\n",
    "import cupy\n",
    "import cuml\n",
    "import sklearn\n",
    "import yaml\n",
    "from packaging import version\n",
    "import transformers\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.store_outputs import convert_outputs_to_df_fast\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 123\n",
    "\n",
    "ws = '/workspace/deliberative-alignment-jailbreaks'\n",
    "version = version.parse(transformers.__version__).major\n",
    "\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "selected_model_index = 0\n",
    "\n",
    "def get_model(index):\n",
    "    \"\"\"\n",
    "    - HF model id, model prefix (short model identifier), model arch\n",
    "    - Attn implementation, whether to use the HF default implementation, # hidden layers\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        0: ('openai/gpt-oss-20b', 'gptoss-20b', 'gptoss', 'kernels-community/vllm-flash-attn3', True, 24),\n",
    "        1: ('openai/gpt-oss-120b', 'gptoss-120b', 'gptoss', 'kernels-community/vllm-flash-attn3', True, 36),\n",
    "        2: ('nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16', 'nemotron-3-nano', 'nemotron3', None, False, 52),\n",
    "        3: ('Qwen/Qwen3-30B-A3B-Thinking-2507', 'qwen3-30b-a3b', 'qwen3moe', None, True, 48),\n",
    "        4: ('ai21labs/AI21-Jamba-Reasoning- 3B', 'jamba-reasoning', 'jamba', None, True, 28),\n",
    "        5: ('ServiceNow-AI/Apriel-1.6-15b-Thinker', 'apriel-1.6-15b-thinker', 'apriel', None, True, 48),\n",
    "        6: ('allenai/Olmo-3-7B-Think', 'olmo3-7b-think', 'olmo3', None, True, 32),\n",
    "        7: ('zai-org/GLM-4.6V-Flash', 'glm-4.6v-flash', 'glm46v', None, True, 40),\n",
    "        8: ('zai-org/GLM-4.7-Flash', 'glm-4.7-flash', 'glm4moelite', None, True, 46)\n",
    "    }\n",
    "    return models[index]\n",
    "\n",
    "def load_model_and_tokenizer(model_id, model_architecture, model_attn, model_use_hf):\n",
    "    \"\"\"\n",
    "    Load the model and tokenizer from HF, or from file if already downloaded.\n",
    "    \"\"\"\n",
    "    cache_dir = '/workspace/hf'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir = cache_dir, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "    load_params = {'cache_dir': cache_dir, 'dtype': 'auto', 'trust_remote_code': not model_use_hf, 'device_map': None, 'attn_implementation': model_attn}    \n",
    "    if model_architecture == 'glm46v':\n",
    "        model = Glm4vForConditionalGeneration.from_pretrained(model_id, **load_params).to(main_device).eval()\n",
    "    elif model_architecture == 'apriel':\n",
    "        model = AutoModelForImageTextToText.from_pretrained(model_id, **load_params).to(main_device).eval()\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, **load_params).to(main_device).eval()\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "model_id, model_prefix, model_architecture, model_attn, model_use_hf, model_n_layers = get_model(selected_model_index)\n",
    "tokenizer, model = load_model_and_tokenizer(model_id, model_architecture, model_attn, model_use_hf)\n",
    "\n",
    "if model_architecture == 'gptoss':\n",
    "    print(model.model.layers[0].mlp.experts.down_proj) # Precision should be MXFP4\n",
    "    print(model.model.config._attn_implementation) # Attn should be FA3\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    print('Setting pad token automatically')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "if version == 5 and hasattr(model, 'set_experts_implementation'): # In transformers v5, avoid non-deterministic implss\n",
    "    model.set_experts_implementation('eager')\n",
    "\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load reverse-engineered forward pass functions (usage note - this can be replaced by simpler hooks if desired)\n",
    "\"\"\"\n",
    "model_module = importlib.import_module(f\"utils.pretrained_models.{model_architecture}\")\n",
    "run_model_return_topk = getattr(model_module, f\"run_{model_architecture}_return_topk\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_custom_forward_pass(model, pad_token_id):\n",
    "    inputs = tokenizer(['Hi! I am a dog and I like to bark', 'Vegetables are good for'], return_tensors = 'pt', padding = 'max_length', truncation = True, max_length = 640).to(model.device)\n",
    "    original_results = model(**inputs, use_cache = False)\n",
    "    custom_results = run_model_return_topk(model, inputs['input_ids'], inputs['attention_mask'], return_hidden_states = True)\n",
    "    assert torch.equal(original_results.logits, custom_results['logits']), 'Error in custom forward'\n",
    "    loss = ForCausalLMLoss(custom_results['logits'], torch.where(inputs['input_ids'] == pad_token_id, torch.tensor(-100), inputs['input_ids']), custom_results['logits'].size(-1)).detach().cpu().item()\n",
    "    print(f\"LM loss: {loss}\")\n",
    "    print(f\"Hidden states layers (pre-mlp | post-layer): {len(custom_results['all_pre_mlp_hidden_states'])} | {len(custom_results['all_hidden_states'])}\")\n",
    "    print(f\"Hidden state size (pre-mlp | post-layer): {(custom_results['all_pre_mlp_hidden_states'][0].shape)} | {(custom_results['all_hidden_states'][0].shape)}\")\n",
    "\n",
    "test_custom_forward_pass(model, tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test generation is sensible\n",
    "\"\"\"\n",
    "def test_generation():\n",
    "    conv = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": \"Write a haiku about GPUs\"},],\n",
    "        tokenize = False,\n",
    "        enable_thinking = True,\n",
    "        add_generation_prompt = True\n",
    "    )\n",
    "    inputs = tokenizer(conv, return_tensors = 'pt')\n",
    "    gen_ids = model.generate(inputs['input_ids'].to(main_device), max_new_tokens = 100, do_sample = False)\n",
    "    print(tokenizer.batch_decode(gen_ids, skip_special_tokens = False)[0])    \n",
    "\n",
    "test_generation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train probes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset - Dolma3 / C4\n",
    "\"\"\"\n",
    "n_sample_size = yaml.safe_load(open('experiment-config/probe.yaml'))[model_prefix]['n_sample_size']\n",
    "\n",
    "def load_raw_ds():\n",
    "\n",
    "    def get_c4():\n",
    "        return load_dataset('allenai/c4', 'en', split = 'validation', streaming = True).shuffle(seed = seed, buffer_size = 50_000)\n",
    "    \n",
    "    def get_dolma3():\n",
    "        return load_dataset('allenai/dolma3_mix-150B-1025', split = 'train', revision = '3a8349c', streaming = True).shuffle(seed = seed, buffer_size = 50_000)\n",
    "    \n",
    "    def get_data(ds, n_samples, data_source): # en\n",
    "        raw_data = []\n",
    "        ds_iter = iter(ds)\n",
    "        for _ in range(n_samples):\n",
    "            sample = next(ds_iter, None)\n",
    "            if sample is None:\n",
    "                break\n",
    "            raw_data.append({'text': sample['text'], 'source': data_source})\n",
    "        return raw_data\n",
    "    \n",
    "    return get_data(get_c4(), int(n_sample_size * .25), 'c4-en') + get_data(get_dolma3(), int(n_sample_size * .75), 'dolma3')\n",
    "\n",
    "raw_data = load_raw_ds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test rendering\n",
    "\"\"\"\n",
    "import utils.role_templates\n",
    "importlib.reload(utils.role_templates)\n",
    "from utils.role_templates import render_single_message, render_mixed_cot\n",
    "\n",
    "def test_render():\n",
    "    print(tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {'role': 'user', 'content': 'Hi! I am a dog and I like to bark'},\n",
    "            {'role': 'assistant', 'content': 'Hello! What a lovely dog you are!'}\n",
    "        ],\n",
    "        tokenize = False, padding = 'max_length', truncation = True, max_length = 512, add_generation_prompt = True,\n",
    "        enable_thinking = True\n",
    "    ))\n",
    "    print('--')\n",
    "    print(render_single_message(model_prefix, role = 'user', content ='Hi'))\n",
    "    print('--')\n",
    "    print(render_mixed_cot(model_prefix, 'The user...', 'Yes!'))\n",
    "    return True\n",
    "\n",
    "test_render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Validate forward passes work\n",
    "\"\"\"\n",
    "# Define opening texts to append at the start of each role for training. If multiple, will randomize equally.\n",
    "train_prefixes = yaml.safe_load(open('experiment-config/probe.yaml'))[model_prefix]['train_prefixes']\n",
    "for p in train_prefixes:\n",
    "    print(p)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test1():\n",
    "    conv = tokenizer.apply_chat_template(\n",
    "        [{'role': 'user', 'content': 'Hi stinky'}],\n",
    "        tokenize = False,\n",
    "        add_generation_prompt = True\n",
    "    )\n",
    "    inputs = tokenizer(conv, add_special_tokens = True, return_tensors = 'pt')\n",
    "    gen_ids = model.generate(inputs['input_ids'].to(main_device), attention_mask = inputs['attention_mask'].to(main_device), max_new_tokens = 500, do_sample = False)\n",
    "    df = pd.DataFrame({'token_id': gen_ids[0].tolist(), 'token': tokenizer.convert_ids_to_tokens(gen_ids[0].tolist()),})\n",
    "    print(tokenizer.decode(df['token_id'].tolist(), skip_special_tokens = False))\n",
    "    return df\n",
    "\n",
    "@torch.no_grad()\n",
    "def test2():\n",
    "    \"\"\"Quick test - can the model run forward passes correctly?\"\"\"\n",
    "    for tr_prefix in train_prefixes:\n",
    "        conv = tr_prefix + render_single_message(model_prefix, 'user', 'Where is Atlanta?') +\\\n",
    "            (\n",
    "            '<|start|>assistant<|channel|>analysis<|message|>' # gpt-oss-*\n",
    "            # '<|im_start|>assistant\\n<think></think>' # Nemotron\n",
    "            # '<|assistant|>\\n<think>' # GLM-4.6V-Flash\n",
    "            # '<|im_start|>assistant\\n<think>\\n' # Qwen3\n",
    "            # '\\n<|begin_assistant|>\\nHere are my reasoning steps:\\n' # Apriel-1.6-15B-Thinker\n",
    "            # '<|assistant|><think>' # GLM-4.7-Flash\n",
    "            )\n",
    "        inputs = tokenizer(conv, add_special_tokens = False, return_tensors = 'pt')\n",
    "        gen_ids = model.generate(inputs['input_ids'].to(main_device), attention_mask = inputs['attention_mask'].to(main_device), max_new_tokens = 500, do_sample = False)\n",
    "        print(tokenizer.decode(gen_ids[0], skip_special_tokens = False))\n",
    "        print('\\n')\n",
    "\n",
    "test1()\n",
    "test2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create sample sequences\n",
    "\"\"\"\n",
    "SEQLEN = yaml.safe_load(open('experiment-config/probe.yaml'))[model_prefix]['seq_len']\n",
    "NESTED_REASONING = yaml.safe_load(open('experiment-config/probe.yaml'))[model_prefix]['nested_reasoning']\n",
    "\n",
    "def get_sample_seqs_for_input_seq(probe_text, partner_text, prefix = ''):\n",
    "    \"\"\"\n",
    "    Helper to convert a single input sequence into all examples\n",
    "\n",
    "    Params\n",
    "        @probe_text: The text we're extracting states from (appears in all roles)\n",
    "        @partner_text: Random paired text (only used in merged sample's assistant position)\n",
    "    \"\"\"\n",
    "    seqs = []\n",
    "    for role in ['system', 'user', 'tool', 'cot']:\n",
    "        seqs.append({\n",
    "            'role': role,\n",
    "            'prompt': prefix + render_single_message(model_prefix, role = role, content = probe_text)\n",
    "        })\n",
    "\n",
    "    # If merge AND LRM, render with CoT prefix\n",
    "    if NESTED_REASONING:\n",
    "        seqs.append({\n",
    "            'role': 'assistant',\n",
    "            'prompt': prefix + render_mixed_cot(model_prefix, cot = partner_text, assistant = probe_text)\n",
    "        })\n",
    "    # Else render standalone\n",
    "    else:\n",
    "        seqs.append({\n",
    "            'role': 'assistant',\n",
    "            'prompt': prefix + render_single_message(model_prefix, role = 'assistant', content = probe_text)\n",
    "        })\n",
    "\n",
    "    return seqs\n",
    "\n",
    "def build_sample_seqs(train_prefixes):\n",
    "    \"\"\"\n",
    "    Build all sample sequences\n",
    "    \"\"\"\n",
    "    truncated_texts = tokenizer.batch_decode(tokenizer([t['text'] for t in raw_data], add_special_tokens = False, padding = False, truncation = True, max_length = SEQLEN).input_ids)\n",
    "    n_seqs = len(truncated_texts)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    partner_lengths = ((np.random.beta(0.5, 4.0, size = n_seqs) * (SEQLEN/2 + 1)).astype(int)).tolist()\n",
    "    partner_texts = [\n",
    "        tokenizer.decode(tokenizer(text['text'], add_special_tokens = False, padding = False, truncation = True, max_length = int(partner_lengths[i])).input_ids)\n",
    "        for i, text in enumerate(raw_data)\n",
    "    ]\n",
    "    perm = np.random.permutation(n_seqs)\n",
    "    while np.any(perm == np.arange(n_seqs)):\n",
    "        perm = np.random.permutation(n_seqs)\n",
    "\n",
    "    sampled_prefixes = np.random.choice(train_prefixes, size = n_seqs)\n",
    "\n",
    "    input_list = []\n",
    "    for base_ix, base_text in enumerate(truncated_texts):\n",
    "        partner_text = partner_texts[int(perm[base_ix])].strip()\n",
    "        prefix = sampled_prefixes[base_ix]\n",
    "\n",
    "        for seq in get_sample_seqs_for_input_seq(base_text, partner_text, prefix):\n",
    "            row = {'question_ix': base_ix, 'question': base_text, **seq}\n",
    "            input_list.append(row)\n",
    "\n",
    "    input_df = pd.DataFrame(input_list).assign(prompt_ix = lambda df: list(range(len(df))))\n",
    "    return input_df\n",
    "\n",
    "input_df = build_sample_seqs(train_prefixes = train_prefixes)\n",
    "display(input_df)\n",
    "\n",
    "for p in [row['prompt'] for row in input_df.pipe(lambda df: df[df['question_ix'] == 2]).to_dict('records')]:\n",
    "    print(p)\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Load dataset into a dataloader which returns original tokens - important for BPE tokenizers to reconstruct the correct string later\n",
    "\"\"\"\n",
    "from utils.dataset import ReconstructableTextDataset, stack_collate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "max_seqlen = int(tokenizer(input_df['prompt'].tolist(), padding = True, truncation = False, return_tensors = 'pt')['attention_mask'].sum(dim = 1).max().item())\n",
    "train_dl = DataLoader(\n",
    "    ReconstructableTextDataset(input_df['prompt'].tolist(), tokenizer, max_length = max_seqlen, prompt_ix = input_df['prompt_ix'].tolist()),\n",
    "    batch_size = 32,\n",
    "    shuffle = False,\n",
    "    collate_fn = stack_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run forward passes\n",
    "\"\"\"\n",
    "@torch.no_grad()\n",
    "def run_and_export_states(model, dl: ReconstructableTextDataset, layers_to_keep_acts: list[int]):\n",
    "    \"\"\"\n",
    "    Run forward passes on given model and store the decomposed sample_df plus hidden states\n",
    "\n",
    "    Params:\n",
    "        @model: The model to run forward passes on via `run_model_return_topk`. Should return a dict with keys `logits` and `all_hidden_states`.\n",
    "        @dl: A ReconstructableTextDataset of which returns `input_ids`, `attention_mask`, `original_tokens`, and `prompt_ix`.\n",
    "        @layers_to_keep_acts: A list of layer indices (0-indexed) for which to filter `all_hidden_states` (see returned object description).\n",
    "\n",
    "    Returns:\n",
    "        A dict with keys:\n",
    "        - `sample_df`: A sample (token)-level dataframe with corresponding input token ID, output token ID, and input token text (removes masked tokens)\n",
    "        - `all_hidden_states`: A tensor of size n_samples x layers_to_keep_acts x D return the hidden state for each retained layers. Each \n",
    "            n_sample corresponds to a row of sample_df.\n",
    "\n",
    "    Example:\n",
    "        test_outputs = run_and_export_states(model, train_dl, layers_to_keep_acts = list(range(model_n_layers)))\n",
    "    \"\"\"\n",
    "    all_hidden_states = []\n",
    "    sample_dfs = []\n",
    "\n",
    "    for batch_ix, batch in tqdm(enumerate(dl), total = len(dl)):\n",
    "\n",
    "        input_ids = batch['input_ids'].to(main_device)\n",
    "        attention_mask = batch['attention_mask'].to(main_device)\n",
    "        original_tokens = batch['original_tokens']\n",
    "        prompt_indices = batch['prompt_ix']\n",
    "\n",
    "        output = run_model_return_topk(model, input_ids, attention_mask, return_hidden_states = True)\n",
    "\n",
    "        # Check no bugs by validating output/perplexity\n",
    "        if batch_ix == 0:\n",
    "            loss = ForCausalLMLoss(output['logits'], torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids), output['logits'].size(-1)).detach().cpu().item()\n",
    "            for i in range(min(20, input_ids.size(0))):\n",
    "                decoded_input = tokenizer.decode(input_ids[i, :], skip_special_tokens = False)\n",
    "                next_token_id = torch.argmax(output['logits'][i, -1, :]).item()\n",
    "                print('---------\\n' + decoded_input + colored(tokenizer.decode([next_token_id], skip_special_tokens = False).replace('\\n', '<lb>'), 'green'))\n",
    "            print(f\"PPL:\", torch.exp(torch.tensor(loss)).item())\n",
    "                \n",
    "        original_tokens_df = pd.DataFrame(\n",
    "            [(seq_i, tok_i, tok) for seq_i, tokens in enumerate(original_tokens) for tok_i, tok in enumerate(tokens)], \n",
    "            columns = ['sequence_ix', 'token_ix', 'token']\n",
    "        )\n",
    "                \n",
    "        prompt_indices_df = pd.DataFrame(\n",
    "            [(seq_i, seq_source) for seq_i, seq_source in enumerate(prompt_indices)], \n",
    "            columns = ['sequence_ix', 'prompt_ix']\n",
    "        )\n",
    "        \n",
    "        # Create sample (token) level dataframe\n",
    "        sample_df =\\\n",
    "            convert_outputs_to_df_fast(input_ids, attention_mask, output['logits'])\\\n",
    "            .merge(original_tokens_df, how = 'left', on = ['token_ix', 'sequence_ix'])\\\n",
    "            .merge(prompt_indices_df, how = 'left', on = ['sequence_ix'])\\\n",
    "            .assign(batch_ix = batch_ix)\n",
    "        \n",
    "        sample_dfs.append(sample_df)\n",
    "\n",
    "        # Store pre-MLP hidden states - the fwd pass as n_layers list as BN x D, collapse to BN x n_layers x D, with BN filtering out masked items\n",
    "        valid_pos = torch.where(attention_mask.cpu().view(-1) == 1) # Valid (BN, ) positions\n",
    "        all_hidden_states.append(torch.stack(output['all_pre_mlp_hidden_states'], dim = 1)[valid_pos][:, layers_to_keep_acts, :])\n",
    "\n",
    "    sample_df = pd.concat(sample_dfs, ignore_index = True).drop(columns = ['batch_ix', 'sequence_ix']) # Drop batch/seq_ix, since prompt_ix identifies\n",
    "    all_hidden_states = torch.cat(all_hidden_states, dim = 0)\n",
    "\n",
    "    return {\n",
    "        'sample_df': sample_df,\n",
    "        'all_hs': all_hidden_states\n",
    "    }\n",
    "\n",
    "layers_to_probe = list(range(0, model_n_layers, 4)) if model_n_layers >= 30 else list(range(0, model_n_layers, 2))\n",
    "res = run_and_export_states(model, train_dl, layers_to_keep_acts = layers_to_probe)\n",
    "\n",
    "# Convert to f16 for cupy compatability\n",
    "all_probe_hs = res['all_hs'].to(torch.float16)\n",
    "all_probe_hs = {layer_ix: all_probe_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(layers_to_probe)}\n",
    "del res['all_hs']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label roles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Take each token and label it with its correct role (user/assistant/system/tool/cot); check the counts are as expected\n",
    "\"\"\"\n",
    "import utils.role_assignments\n",
    "importlib.reload(utils.role_assignments)\n",
    "from utils.role_assignments import label_content_roles\n",
    "from utils.substring_assignments import flag_message_types\n",
    "\n",
    "probe_sample_df = (\n",
    "    # Flag all roles\n",
    "    label_content_roles(model_prefix, res['sample_df'])    \n",
    "    .assign(sample_ix = lambda df: range(0, len(df)))\n",
    "    # Flag those that match target role (for retain later)\n",
    "    .merge(input_df[['prompt_ix', 'role']].rename(columns = {'role': 'target_role'}), how = 'inner', on = 'prompt_ix')\n",
    "    .assign(match_target_role = lambda df: np.where(df['role'] == df['target_role'], True, False))\n",
    "    # Flag those that match the prepend (for drop later)\n",
    "    .pipe(lambda df: flag_message_types(df, train_prefixes, allow_ambiguous = True))\n",
    "    .drop(columns = 'base_message')\n",
    "    # Drop prepend text + non-content tags\n",
    "    .pipe(lambda df: df[(df['base_message_ix'].isna()) & (df['is_content'] == True) & (df['role'].notna()) & (df['match_target_role'])])\n",
    ")\n",
    "\n",
    "# Verify role counts are accurate (should be exactly equal for most models, except when models where role tags can be merged with content into single toks)\n",
    "display(probe_sample_df.groupby('role', as_index = False).agg(count = ('sample_ix', 'count')))\n",
    "\n",
    "# Validate roles are flagged correctly\n",
    "display(\n",
    "    probe_sample_df\\\n",
    "    .pipe(lambda df: df[df['prompt_ix'] <= 14])\\\n",
    "    .groupby(['prompt_ix', 'seg_ix', 'role'], as_index = False)\\\n",
    "    .agg(combined_text = ('token', ''.join))\\\n",
    "    .assign(eot = lambda df: df['combined_text'].str[-30:])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "First run a quick grid search over hyperparmeters\n",
    "\"\"\"\n",
    "SKIP_FIRST_N = 32 if NESTED_REASONING else 0\n",
    "\n",
    "def fit_lr(x_train, y_train, x_test, y_test, add_scaling = False, **lr_params):\n",
    "    \"\"\"\n",
    "    Fit a probe\n",
    "    \"\"\"\n",
    "    steps = []\n",
    "    if add_scaling:\n",
    "        steps.append(('scaler', cuml.preprocessing.StandardScaler()))\n",
    "    steps.append(('clf', cuml.linear_model.LogisticRegression(\n",
    "        penalty = 'l2',\n",
    "        max_iter = 5_000,\n",
    "        fit_intercept = True,\n",
    "        **lr_params\n",
    "    )))\n",
    "    lr_model = sklearn.pipeline.Pipeline(steps)\n",
    "    lr_model.fit(x_train, y_train)\n",
    "    accuracy = lr_model.score(x_test, y_test)\n",
    "    y_test_pred = lr_model.predict(x_test)\n",
    "    y_test_prob = lr_model.predict_proba(x_test)\n",
    "    nll = cuml.metrics.log_loss(y_test, y_test_prob,)\n",
    "    return lr_model, accuracy, nll, y_test_pred\n",
    "\n",
    "def get_probe_result(sample_df, layer_hs, roles_map, add_scaling = False, **lr_params):\n",
    "    \"\"\"\n",
    "    Get probe results for a single layer and label combination\n",
    "\n",
    "    Params:\n",
    "        @sample_df: The sample-level df; with a column `sample_ix` indicating the token order of 0...T-1;\n",
    "            the actual df may be shorter due to pre-filters\n",
    "        @layer_hs: A tensor of probe hidden states for a layer, of T x D\n",
    "        @roles_map: The mapping order of the roles; a dict {}\n",
    "\n",
    "    Description:\n",
    "        Trains only on content space for given roles\n",
    "    \"\"\"\n",
    "    # Train/test split\n",
    "    prompt_ix_train, prompt_ix_test = cuml.train_test_split(sample_df['prompt_ix'].unique(), test_size = 0.1, random_state = seed)\n",
    "    train_df = sample_df[sample_df['prompt_ix'].isin(prompt_ix_train)]\n",
    "    test_df = sample_df[sample_df['prompt_ix'].isin(prompt_ix_test)]\n",
    "\n",
    "    # Get y labels\n",
    "    role_labels_train_cp = cupy.asarray([roles_map[r] for r in train_df['role']])\n",
    "    role_labels_test_cp = cupy.asarray([roles_map[r] for r in test_df['role']])\n",
    "\n",
    "    # Get x labels\n",
    "    x_train_cp = cupy.asarray(layer_hs[train_df['sample_ix'].tolist(), :].to(torch.float32).detach().cpu())\n",
    "    x_test_cp = cupy.asarray(layer_hs[test_df['sample_ix'].tolist(), :].to(torch.float32).detach().cpu())\n",
    "    \n",
    "    if (len(train_df) != x_train_cp.shape[0]):\n",
    "        raise Exception(f\"Shape mismatch!\")\n",
    "\n",
    "    uniq_train = np.unique(role_labels_train_cp.get())\n",
    "    if len(uniq_train) < len(roles_map):\n",
    "        raise Exception(f\"Skipping mapping {roles_map}: missing roles in train\", uniq_train)\n",
    "\n",
    "    lr_model, test_acc, test_nll, y_test_pred = fit_lr(\n",
    "        x_train_cp, role_labels_train_cp, x_test_cp, role_labels_test_cp,\n",
    "        add_scaling = add_scaling,\n",
    "        **lr_params\n",
    "    )\n",
    "\n",
    "    # Classification metrics\n",
    "    results_df =\\\n",
    "        test_df\\\n",
    "        .assign(pred = y_test_pred.tolist())\\\n",
    "        .assign(pred = lambda df: df['pred'].map({v: k for k, v in roles_map.items()}))\\\n",
    "        .assign(is_acc = lambda df: df['role'] == df['pred'])\n",
    "\n",
    "    acc_by_role =\\\n",
    "        results_df\\\n",
    "        .groupby(['role', 'pred'], as_index = False)\\\n",
    "        .agg(count = ('sample_ix', 'count'))\n",
    "\n",
    "    acc_by_pos =\\\n",
    "        results_df\\\n",
    "        .groupby('token_in_seg_ix', as_index = False)\\\n",
    "        .agg(count = ('sample_ix', 'count'), acc = ('is_acc', 'mean'))\n",
    "\n",
    "    return {\n",
    "        'probe': lr_model, 'acc': test_acc, 'nll': test_nll,\n",
    "        'acc_by_role': acc_by_role, 'acc_by_pos': acc_by_pos\n",
    "    }\n",
    "\n",
    "def test_c(C, add_scaling):\n",
    "    \"\"\"\n",
    "    Test hyperparams: mid-layer, roles for UCAT rolespace\n",
    "    \"\"\"\n",
    "    test_roles = ['user', 'assistant', 'tool']\n",
    "    test_layer_ix = layers_to_probe[(len(layers_to_probe) - 1) // 2] \n",
    "    probe_res = get_probe_result(\n",
    "        sample_df = probe_sample_df[(probe_sample_df['role'].isin(test_roles)) & (probe_sample_df['token_in_seg_ix'] >= SKIP_FIRST_N)].reset_index(drop = True),\n",
    "        layer_hs = all_probe_hs[test_layer_ix],\n",
    "        roles_map = {x: i for i, x in enumerate(test_roles)},\n",
    "        add_scaling = add_scaling,\n",
    "        C = C\n",
    "    )\n",
    "    return {'C': C, 'add_scaling': add_scaling, 'val_acc': probe_res['acc'], 'val_nll': probe_res['nll']}\n",
    "\n",
    "clear_all_cuda_memory()\n",
    "gc.collect()\n",
    "\n",
    "for scale_val in [True, False]:\n",
    "    print(f\"Scaling: {scale_val}\")\n",
    "    display(pd.DataFrame([test_c(c_val, add_scaling = scale_val) for c_val in tqdm([1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run role probes at every fourth layer for role combinations specified in all_role_combinations (excl CoT role for non-reasoning models)\n",
    "\"\"\"\n",
    "train_params = yaml.safe_load(open('experiment-config/probe.yaml'))[model_prefix]['train_params']\n",
    "\n",
    "all_role_combinations = [\n",
    "    ('user', 'assistant'),\n",
    "    ('user', 'assistant', 'tool'),\n",
    "]\n",
    "\n",
    "if model_prefix in ['gptoss-20b']:\n",
    "    all_role_combinations += [\n",
    "        ('user', 'cot', 'assistant'),\n",
    "        ('user', 'cot', 'assistant', 'tool'),\n",
    "        \n",
    "        ('system', 'user', 'assistant'),\n",
    "        ('system', 'user', 'assistant', 'tool'),\n",
    "        ('system', 'user', 'cot', 'assistant'),\n",
    "        ('system', 'user', 'cot', 'assistant', 'tool'),\n",
    "    ]\n",
    "\n",
    "all_role_combinations = [\n",
    "    {\n",
    "        'roles': list(roles),\n",
    "        'roles_map': {x: i for i, x in enumerate(roles)},\n",
    "        # Sample df for only those roles - filtering here is fine since we retain sample_ix which get_probe_result() uses to trace the original token\n",
    "        'sample_df': probe_sample_df.pipe(lambda df: df[(df['role'].isin(roles)) & (df['token_in_seg_ix'] >= SKIP_FIRST_N)]).reset_index(drop = True),\n",
    "    }\n",
    "    for roles in all_role_combinations\n",
    "]\n",
    "\n",
    "clear_all_cuda_memory()\n",
    "gc.collect()\n",
    "\n",
    "all_probes = []\n",
    "for roles_dict in all_role_combinations:\n",
    "    print(f\"Training {roles_dict['roles']}\")\n",
    "    for layer_ix in layers_to_probe:\n",
    "        probe_res = get_probe_result(\n",
    "            sample_df = roles_dict['sample_df'],\n",
    "            layer_hs = all_probe_hs[layer_ix],\n",
    "            roles_map = roles_dict['roles_map'],\n",
    "            add_scaling = train_params['add_scaling'],\n",
    "            C = train_params['C']\n",
    "        )\n",
    "        print(f\"  Layer [{layer_ix}]: {probe_res['acc']:.2f}\")\n",
    "\n",
    "        all_probes.append({\n",
    "            **probe_res,\n",
    "            'layer_ix': layer_ix,\n",
    "            'role_space': roles_dict['roles'],\n",
    "            'roles_map': roles_dict['roles_map'],\n",
    "            'n_inputs': len(roles_dict['sample_df'])\n",
    "        })\n",
    "\n",
    "print(f\"Num probes: {str(len(all_probes))}\")\n",
    "print(f\"Probe layers:\\n  {', '.join([str(x) for x in layers_to_probe])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save probes + metrics\n",
    "\"\"\"\n",
    "def validate_accuracy_and_save(all_probes):\n",
    "    \"\"\"\n",
    "    Calculate accurcay by (rolespace, layer, role) and save\n",
    "    \"\"\"\n",
    "    print('Val accuracy by layer:')\n",
    "    display(\n",
    "        pd.DataFrame(all_probes)[['layer_ix', 'role_space', 'acc']]\\\n",
    "            .assign(acc = lambda df: df['acc'].round(2), role_space = lambda df: df['role_space'].apply(lambda x: ','.join(([r[0] for r in x]))))\\\n",
    "            .pivot(index = 'layer_ix', columns = 'role_space', values = 'acc')\n",
    "    )\n",
    "\n",
    "    print('Concatenating across layers and saving...')\n",
    "    acc_by_role = pd.concat([p['acc_by_role'].assign(model = model_prefix, layer_ix = p['layer_ix'], role_space = ','.join(p['role_space'])) for p in all_probes], ignore_index = True)\n",
    "    acc_by_pos =\\\n",
    "        pd.concat([p['acc_by_pos'].assign(model = model_prefix, layer_ix = p['layer_ix'], role_space = ','.join(p['role_space'])) for p in all_probes], ignore_index = True)\\\n",
    "        .assign(acc = lambda df: df['acc'].round(4))\n",
    "\n",
    "    acc_by_role.to_csv(f'{ws}/experiments/role-analysis/outputs/probe-training/acc_by_role_{model_prefix}.csv', index = False)\n",
    "    acc_by_pos.to_csv(f'{ws}/experiments/role-analysis/outputs/probe-training/acc_by_pos_{model_prefix}.csv', index = False)\n",
    "\n",
    "    with open(f'{ws}/experiments/role-analysis/outputs/probes/{model_prefix}.pkl', 'wb') as f:\n",
    "        pickle.dump(all_probes, f)\n",
    "\n",
    "    print('Accuracy by role:')\n",
    "    base_sums = acc_by_role.groupby(['role', 'layer_ix', 'role_space'], as_index = False).agg(base_sum = ('count', 'sum'))\n",
    "\n",
    "    acc_by_role =\\\n",
    "        acc_by_role\\\n",
    "        .pipe(lambda df: df[df['role'] == df['pred']])\\\n",
    "        .groupby(['role', 'layer_ix', 'role_space'], as_index = False)\\\n",
    "        .agg(sum = ('count', 'sum'))\\\n",
    "        .merge(base_sums, on = ['layer_ix', 'role_space', 'role'], how = 'inner')\\\n",
    "        .assign(acc = lambda df: df['sum']/df['base_sum'])\\\n",
    "        .pivot(index = ['role_space', 'layer_ix'], columns = 'role', values = 'acc')\\\n",
    "        .round(2)\n",
    "\n",
    "    display(acc_by_role)\n",
    "\n",
    "    return True\n",
    "\n",
    "validate_accuracy_and_save(all_probes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate on real conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Validate IFT on the full chat template, which tokenizes the whole thing e2e. This should match the format from original messages.:\n",
    "- `load_chat_template`: Overwrites the existing chat template in the tokenizer with one that better handles discrepancies.\n",
    "    See docstring for the function for details. \n",
    "- `label_content_roles`: Takes an instruct-formatted text, then assigns each token to the correct roles.\n",
    "\"\"\"\n",
    "import importlib\n",
    "import utils.role_assignments\n",
    "import utils.role_templates\n",
    "importlib.reload(utils.role_assignments)\n",
    "importlib.reload(utils.role_templates)\n",
    "load_chat_template = utils.role_templates.load_chat_template\n",
    "label_content_roles = utils.role_assignments.label_content_roles\n",
    "\n",
    "def test_chat_template(tokenizer):\n",
    "    s = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {'role': 'system', 'content': 'Test.'},\n",
    "            {'role': 'user', 'content': 'Hi! I\\'m a dog.'},\n",
    "            {'role': 'assistant', 'content': '<think>The user is a dog!</think>Congrats!'},\n",
    "            {'role': 'user', 'content': 'Thanks!'},\n",
    "            {'role': 'assistant', 'content': '<think>Hmm, the user said thanks!</think>Anything else?'}\n",
    "        ],\n",
    "        tokenize = False,\n",
    "        padding = 'max_length',\n",
    "        truncation = True,\n",
    "        max_length = 512,\n",
    "        add_generation_prompt = True\n",
    "    )\n",
    "    print(s)\n",
    "    return s\n",
    "\n",
    "# Original chat template\n",
    "if 'base_chat_template' not in globals():\n",
    "    base_chat_template = tokenizer.chat_template\n",
    "tokenizer.chat_template = base_chat_template\n",
    "s = test_chat_template(tokenizer)\n",
    "\n",
    "# New chat template\n",
    "tokenizer.chat_template = load_chat_template(f'{ws}/utils/chat_templates', model_prefix)\n",
    "s = test_chat_template(tokenizer)\n",
    "\n",
    "# Label content roles\n",
    "tokens_df = (\n",
    "    pd.DataFrame({'input_ids': tokenizer(s, add_special_tokens = False)['input_ids']})\n",
    "    .assign(token = lambda df: tokenizer.convert_ids_to_tokens(df['input_ids']))\n",
    "    .assign(prompt_ix = 0, token_ix = lambda df: df.index)\n",
    ")\n",
    "label_content_roles(model_prefix, tokens_df).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define opening text to append at the start of each role\n",
    "\"\"\"\n",
    "test_prefix = yaml.safe_load(open('experiment-config/probe.yaml'))[model_prefix]['test_prefix']\n",
    "\n",
    "def validate_test_prefix():\n",
    "    conv = test_prefix + tokenizer.apply_chat_template(\n",
    "        [{'role': 'user', 'content': 'Hi stinky pupper! How are you?'}],\n",
    "        tokenize = False,\n",
    "        add_generation_prompt = True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(conv, add_special_tokens = False, return_tensors = 'pt')\n",
    "    gen_ids = model.generate(inputs['input_ids'].to(main_device), attention_mask = inputs['attention_mask'].to(main_device), max_new_tokens = 500, do_sample = False)\n",
    "    print(tokenizer.decode(gen_ids[0]))\n",
    "\n",
    "    df = pd.DataFrame({'token_id': gen_ids[0].tolist(), 'token': tokenizer.convert_ids_to_tokens(gen_ids[0].tolist()),})\n",
    "    return df\n",
    "\n",
    "display(validate_test_prefix().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import conversations\n",
    "\"\"\"\n",
    "def filter_convs(user_queries_df, max_samples, max_total_len = 10000, min_msg_len = 100, max_user_messages_per_conv = 2):\n",
    "    \"\"\"\n",
    "    Filters conversation rows and returns:\n",
    "      - convs: list[list[{'role': ..., 'content': ...}]]\n",
    "      - convs_df: conv-level dataframe with columns ['conv_id', 'dataset', 'messages']\n",
    "\n",
    "    Assumes input has columns:\n",
    "        conv_id, user_query_ix, dataset, user_query, assistant, and optionally cot.\n",
    "    \"\"\"\n",
    "    content_cols = ['user_query', 'assistant', 'cot']\n",
    "    print(f\"Starting length: {user_queries_df['conv_id'].nunique()} convs\")\n",
    "    \n",
    "    # 1. Remove convs with any missing responses\n",
    "    user_queries_df = user_queries_df.pipe(lambda df: df[~df['conv_id'].isin(df[df[content_cols].isna().any(axis=1)]['conv_id'].unique())])\n",
    "    print(f\"After dropping missing: {user_queries_df['conv_id'].nunique()} convs\")\n",
    "\n",
    "    # 2. Filter out (1) long convs, (2) convs with any message <= conv_min_length (to avoid stub messages), (3) convs with too many messages\n",
    "    user_queries_df = (\n",
    "        user_queries_df\n",
    "        .pipe(lambda df: df.assign(**{c: df[c].astype(str).str.strip() for c in content_cols}))\n",
    "        .assign(\n",
    "            row_total_len = lambda df: df[content_cols].apply(lambda r: r.str.len()).sum(axis=1),\n",
    "            row_min_len = lambda df: df[content_cols].apply(lambda r: r.str.len()).min(axis=1),\n",
    "        )\n",
    "        .assign(\n",
    "            total_len = lambda df: df.groupby(\"conv_id\")[\"row_total_len\"].transform(\"sum\"),\n",
    "            conv_min_len = lambda df: df.groupby(\"conv_id\")[\"row_min_len\"].transform(\"min\"),\n",
    "            user_turns=lambda df: df.groupby(\"conv_id\")[\"user_query_ix\"].transform(\"count\"),\n",
    "        )\n",
    "        .query(\"total_len < @max_total_len and conv_min_len >= @min_msg_len and user_turns <= @max_user_messages_per_conv\")\n",
    "        .drop(columns = [\"row_total_len\", \"row_min_len\", \"total_len\", \"conv_min_len\", \"user_turns\"])\n",
    "    )\n",
    "    print(f\"After dropping long convs: {user_queries_df['conv_id'].nunique()} convs\")\n",
    "\n",
    "    # 3. Drop convs with substring messages (causes dupe issues with the flag_message_type fn, which does not handle tokens in multiple string contexts)\n",
    "    def has_subtr_messages(g):\n",
    "        msgs = sum([g[c].tolist() for c in [\"user_query\", \"assistant\", \"cot\"]], [])\n",
    "        # sort by length so we only check short-in-long; reduces comparisons and removes symmetric i/j loop\n",
    "        msgs = sorted(msgs, key=len)\n",
    "        return any(\n",
    "            (i != j) and (a in b)\n",
    "            for i, a in enumerate(msgs)\n",
    "            for j, b in enumerate(msgs)\n",
    "            if len(a) <= len(b) and (i < j)  # only short->long checks\n",
    "        )            \n",
    "    \n",
    "    bad_conv_ids = user_queries_df.groupby(\"conv_id\", sort=False).apply(has_subtr_messages, include_groups=False).reset_index(name=\"bad\").query(\"bad\")[\"conv_id\"].unique()\n",
    "    \n",
    "    user_queries_df = user_queries_df.pipe(lambda df: df[~df[\"conv_id\"].isin(bad_conv_ids)])\n",
    "    print(f\"After dropping substr: {user_queries_df['conv_id'].nunique()} convs\")\n",
    "\n",
    "    # 4. Combine into conv-level df\n",
    "    def rows_to_messages(g):\n",
    "        msgs = []\n",
    "        for _, row in g.iterrows():\n",
    "            msgs.append({\"role\": \"user\", \"content\": row[\"user_query\"]})\n",
    "            msgs.append({\"role\": \"cot\", \"content\": row[\"cot\"]})\n",
    "            msgs.append({\"role\": \"assistant\", \"content\": row[\"assistant\"]})\n",
    "        return msgs\n",
    "\n",
    "    convs_df = (\n",
    "        user_queries_df\n",
    "        .sort_values([\"conv_id\", \"user_query_ix\"])\n",
    "        .assign(conv_id=lambda df: df.groupby(\"conv_id\", sort=False).ngroup())\n",
    "        .groupby(\"conv_id\", sort=False)\n",
    "        .apply(lambda g: pd.Series({\"dataset\": g[\"dataset\"].values[0], \"messages\": rows_to_messages(g)}), include_groups = False)\n",
    "        .reset_index()\n",
    "        [[\"conv_id\", \"dataset\", \"messages\"]]\n",
    "        .pipe(lambda df: df.sample(n = min(max_samples, len(df)), random_state = seed))\n",
    "        .reset_index(drop = True)\n",
    "        .assign(conv_id = lambda df: range(len(df)))\n",
    "    )\n",
    "    print(f\"Final: {convs_df['conv_id'].nunique()} convs\")\n",
    "    return convs_df['messages'].tolist(), convs_df\n",
    "\n",
    "\n",
    "convs, convs_df = filter_convs(\n",
    "    user_queries_df = pd.read_csv(f'{ws}/experiments/role-analysis/data/conversations/{model_prefix}.csv'),\n",
    "    max_samples = 30\n",
    ")\n",
    "\n",
    "display(convs_df.head(5))\n",
    "print(f\"Total convs: {len(convs_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prep untagged/tagged/mistagged conversations\n",
    "\"\"\"\n",
    "from utils.role_templates import fold_cot_into_final\n",
    "\n",
    "all_convs_by_type = {}\n",
    "seperators = yaml.safe_load(open('experiment-config/probe.yaml'))[model_prefix]['test_seperators']\n",
    "print(seperators)\n",
    "\n",
    "# 1. Prep untagged convs\n",
    "def prep_untagged_conv(conv, start_sep, role_seperator):\n",
    "    \"\"\"Create the untagged conversation, matching linebreak struct of model\"\"\"\n",
    "    return test_prefix + start_sep + role_seperator.join([x['content'] for x in conv])\n",
    "\n",
    "all_convs_by_type['untagged'] = [prep_untagged_conv(conv, seperators['untagged_start_sep'], seperators['untagged_role_sep']) for conv in convs]\n",
    "\n",
    "# 2. Prep tagged convs\n",
    "all_convs_by_type['tagged'] = [\n",
    "    test_prefix + seperators['tagged_start_sep'] + x\n",
    "    for x in tokenizer.apply_chat_template(fold_cot_into_final(convs), tokenize = False, add_generation_prompt = False)\n",
    "]\n",
    "\n",
    "# 3. Prep mistagged convs\n",
    "def prep_mistagged_conv(conv, start_sep, role_seperator, role, model_prefix):\n",
    "    \"\"\"\n",
    "    Create the mistagged conversation in <user> and <tool> tags\n",
    "    - Uses the tagged seperator between the start and the start of the new role, since the new role is tagged\n",
    "    - Uses the untagged seperator within the conv to seperate semantic roles, since there is no inner role seperation within the new role\n",
    "    \"\"\"\n",
    "    return test_prefix + start_sep + render_single_message(model_prefix, role, role_seperator.join([x['content'] for x in conv]))\n",
    "\n",
    "all_convs_by_type['user_tagged'] = [prep_mistagged_conv(conv, seperators['tagged_start_sep'], seperators['untagged_role_sep'], 'user', model_prefix) for conv in convs]\n",
    "all_convs_by_type['tool_tagged'] = [prep_mistagged_conv(conv, seperators['tagged_start_sep'], seperators['untagged_role_sep'], 'tool', model_prefix) for conv in convs]\n",
    "\n",
    "assert len(all_convs_by_type['untagged']) == len(all_convs_by_type['tagged'])\n",
    "assert len(all_convs_by_type['untagged']) == len(all_convs_by_type['user_tagged'])\n",
    "assert len(all_convs_by_type['untagged']) == len(all_convs_by_type['tool_tagged'])\n",
    "assert all(len(v) == len(convs) for v in all_convs_by_type.values())\n",
    "\n",
    "test_ix = 1\n",
    "print('------------------- UNTAGGED ------------------')\n",
    "print(all_convs_by_type['untagged'][test_ix])\n",
    "\n",
    "print('------------------- TAGGED ------------------')\n",
    "print(all_convs_by_type['tagged'][test_ix])\n",
    "\n",
    "print('------------------- MISROLED (AS USER) ------------------')\n",
    "print(all_convs_by_type['user_tagged'][test_ix])\n",
    "\n",
    "print('------------------- MISROLED (AS TOOL) ------------------')\n",
    "print(all_convs_by_type['tool_tagged'][test_ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del res\n",
    "gc.collect()\n",
    "clear_all_cuda_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run forward passes\n",
    "\"\"\"\n",
    "# Get input df\n",
    "input_convs_df =\\\n",
    "    pd.concat([\n",
    "        pd.DataFrame({'convs': v, 'conv_type': conv_type})\\\n",
    "            .assign(conv_id = lambda df: range(0, len(df)))\\\n",
    "            .merge(convs_df, how = 'inner', on = 'conv_id')\n",
    "        for conv_type, v in all_convs_by_type.items()\n",
    "    ], ignore_index = True)\\\n",
    "    .assign(prompt_ix = lambda df: list(range(0, len(df))))\n",
    "    \n",
    "display(input_convs_df)\n",
    "\n",
    "max_input_length =\\\n",
    "    tokenizer(input_convs_df['convs'].tolist(), padding = True, truncation = True, max_length = 1024 * 8, return_tensors = 'pt')\\\n",
    "    ['attention_mask'].sum(dim = 1).max().item()\n",
    "\n",
    "print(f\"Max input length: {max_input_length}\")\n",
    "\n",
    "convs_dl = DataLoader(\n",
    "    ReconstructableTextDataset(input_convs_df['convs'].tolist(), tokenizer, max_length = max_input_length, prompt_ix = list(range(0, len(input_convs_df)))),\n",
    "    batch_size = 8,\n",
    "    shuffle = False,\n",
    "    collate_fn = stack_collate\n",
    ")\n",
    "\n",
    "convs_outputs = run_and_export_states(model, convs_dl, layers_to_keep_acts = layers_to_probe)\n",
    "convs_hs = convs_outputs['all_hs'].to(torch.float16)\n",
    "convs_hs = {layer_ix: convs_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(layers_to_probe)}\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Label messages\n",
    "\"\"\"\n",
    "import utils.substring_assignments\n",
    "importlib.reload(utils.substring_assignments)\n",
    "from utils.substring_assignments import flag_message_types\n",
    "\n",
    "def label_convs(input_convs_df, sample_level_df):\n",
    "    \"\"\"\n",
    "    Return a sample-level dataframe labeled appropriately\n",
    "    \n",
    "    Params:\n",
    "        @input_convs_df: The conversation-level df\n",
    "        @convs_output: The sample-level df returns by run_and_export_states\n",
    "    \"\"\"\n",
    "    base_sample_df = sample_level_df.assign(sample_ix = lambda df: range(0, len(df)))\n",
    "    sample_dfs_by_conv = [g for _, g in base_sample_df.groupby('prompt_ix', sort = True)]\n",
    "    metadata_by_conv = input_convs_df.sort_values('prompt_ix').to_dict('records')\n",
    "\n",
    "    all_res = []\n",
    "\n",
    "    # Iterate through (input metadata, token-level sample df) pairs\n",
    "    for conv_metadata, sample_df_for_conv in tqdm(zip(metadata_by_conv, sample_dfs_by_conv, strict = True)):\n",
    "        content_spans = [m['content'].strip() for m in conv_metadata['messages']]\n",
    "        content_roles = [m['role'] for m in conv_metadata['messages']]\n",
    "        # Note: Set the last arg of flag_message_types for reasoning models whose CoTs quote the FULL user message\n",
    "        # Setting the flag causes it to default to the first matching base_message (which should be user)\n",
    "        try:\n",
    "            res = \\\n",
    "                flag_message_types(sample_df_for_conv, content_spans, False)\\\n",
    "                .merge(pd.DataFrame({'role': content_roles, 'base_message_ix': range(len(content_spans))}), on = 'base_message_ix', how = 'left')\n",
    "            assert len(res) == len(sample_df_for_conv)\n",
    "            assert (res['sample_ix'].values == sample_df_for_conv['sample_ix'].values).all() \n",
    "            all_res.append(res)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "        \n",
    "    print(len(all_res))\n",
    "    display(pd.concat(all_res).pipe(lambda df: df[df['role'].isna()]))\n",
    "\n",
    "    sample_df_labeled =\\\n",
    "        pd.concat(all_res).reset_index(drop = True)\\\n",
    "        .assign(token_in_prompt_ix = lambda df: df.groupby(['prompt_ix']).cumcount())\n",
    "\n",
    "    display(sample_df_labeled.pipe(lambda df: df[~df['role'].isna()])) # Validate all NA values are just tag toks\n",
    "\n",
    "    # Validate message assignment seems reasonable\n",
    "    display(\n",
    "        sample_df_labeled\\\n",
    "        .pipe(lambda df: df[df['prompt_ix'] <= 5])\\\n",
    "        .groupby(['prompt_ix', 'base_message', 'role'], as_index = False)\\\n",
    "        .agg(combined_text = ('token', ''.join))\\\n",
    "        .assign(eot = lambda df: df['combined_text'].str[-30:])\n",
    "    )\n",
    "\n",
    "    # Validate counts per message seem reasonable\n",
    "    display(\n",
    "        sample_df_labeled\\\n",
    "        .pipe(lambda df: df[~df['role'].isna()])\\\n",
    "        .groupby(['prompt_ix', 'role'], as_index = False)\\\n",
    "        .agg(count = ('sample_ix', 'count'))\\\n",
    "        .pivot(index = ['prompt_ix'], columns = 'role', values = 'count')\\\n",
    "        .head(20)\n",
    "    )\n",
    "    return sample_df_labeled\n",
    "\n",
    "sample_df_labeled = label_convs(input_convs_df, convs_outputs['sample_df'])\n",
    "assert convs_hs[0].shape[0] == len(sample_df_labeled)\n",
    "sample_df_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_all_cuda_memory()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run test projections for single layer\n",
    "\"\"\"\n",
    "TEST_ROLE_SPACE = ['user', 'cot', 'assistant', 'tool']\n",
    "TEST_LAYER_IX = sorted(layers_to_probe)[len(layers_to_probe) // 2]\n",
    "\n",
    "def run_projections(valid_sample_df, layer_hs, probe):\n",
    "    \"\"\"\n",
    "    Run probe-level projections\n",
    "    \n",
    "    Params:\n",
    "        @valid_sample_df: A sample-level df with columns `sample_ix` (1... T - 1), `sample_ix`.\n",
    "            Can be shorter than full T - 1 due to pre-filters, as long as sample_ix corresponds to the full length.\n",
    "        @layer_hs: A tensor of size T x D for the layer to project.\n",
    "        @probe: The probe dict with keys `probe` (the trained model) and `role_space` (the roles list)\n",
    "    \n",
    "    Returns:\n",
    "        A df at (sample_ix, target_role) level with cols `sample_ix`, `target_role`, `prob`\n",
    "    \"\"\"\n",
    "    x_cp = cupy.asarray(layer_hs[valid_sample_df['sample_ix'].tolist(), :])\n",
    "    y_cp = probe['probe'].predict_proba(x_cp).round(12)\n",
    "\n",
    "    proj_results = pd.DataFrame(cupy.asnumpy(y_cp), columns = probe['role_space'])\n",
    "    if len(proj_results) != len(valid_sample_df):\n",
    "        raise Exception(\"Error!\")\n",
    "\n",
    "    role_df =\\\n",
    "        pd.concat([\n",
    "            proj_results.reset_index(drop = True),\n",
    "            valid_sample_df[['sample_ix']].reset_index(drop = True)\n",
    "        ], axis = 1)\\\n",
    "        .melt(id_vars = ['sample_ix'], var_name = 'target_role', value_name = 'prob')\\\n",
    "        .reset_index(drop = True)\n",
    "\n",
    "    return role_df\n",
    "\n",
    "# top_train_layer =\\\n",
    "#     pd.DataFrame(all_probes)\\\n",
    "#     .assign(probe_ix = lambda df: range(0, len(df)))\\\n",
    "#     .pipe(lambda df: df[df['roles'].apply(lambda r: sorted(r) == sorted(test_roles))])\\\n",
    "#     .pipe(lambda df: df.nlargest(1, 'acc')).to_dict('records')[0]\n",
    "# print(f\"Top layer: {top_train_layer['layer_ix']}\")\n",
    "# test_layer = top_train_layer['layer_ix']\n",
    "\n",
    "test_projections =\\\n",
    "    run_projections(\n",
    "        valid_sample_df = sample_df_labeled.pipe(lambda df: df[(~df['role'].isna()) & (df['role'].isin(TEST_ROLE_SPACE))]),\n",
    "        layer_hs = convs_hs[TEST_LAYER_IX],\n",
    "        probe = [x for x in all_probes if sorted(x['role_space']) == sorted(TEST_ROLE_SPACE) and x['layer_ix'] == TEST_LAYER_IX][0]\n",
    "    )\\\n",
    "    .merge(sample_df_labeled[['prompt_ix', 'sample_ix', 'token_in_prompt_ix', 'token', 'role']], how = 'inner', on = ['sample_ix'])\\\n",
    "    .merge(input_convs_df[['prompt_ix', 'dataset', 'conv_id', 'conv_type']], how = 'inner', on = 'prompt_ix', validate = 'many_to_one')\n",
    "\n",
    "# Now aggregate up to (prompt, role, target_role) level\n",
    "prompt_x_role_x_target_role_projections =\\\n",
    "    test_projections\\\n",
    "    .groupby(['conv_type', 'prompt_ix', 'role', 'target_role'], as_index = False)\\\n",
    "    .agg(\n",
    "        combined_text = ('token', ''.join),\n",
    "        mean_prob = ('prob', lambda x: np.mean(x).round(3))    \n",
    "    )\\\n",
    "    .assign(eot = lambda df: df['combined_text'].str[-30:])\\\n",
    "\n",
    "display(prompt_x_role_x_target_role_projections.head(12))\n",
    "\n",
    "display(\n",
    "    prompt_x_role_x_target_role_projections\\\n",
    "    .groupby(['conv_type', 'target_role', 'role'], as_index = False)\\\n",
    "    .agg(mean_prob = ('mean_prob', 'mean'))\\\n",
    "    .pivot(index = ['conv_type', 'role'], columns = 'target_role', values = 'mean_prob')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_projections\\\n",
    "    .pipe(lambda df: df[df['prompt_ix'] == 10])\\\n",
    "    .pipe(lambda df: df[df['role'] == ('cot')])\\\n",
    "    .groupby('target_role')\\\n",
    "    .agg(combined_text = ('token', ''.join))\n",
    "    ['combined_text'].tolist()[0]\n",
    ")\n",
    "\n",
    "print('-----------')\n",
    "print(test_projections\\\n",
    "    # .pipe(lambda df: df[df['conv_type'] == 'tagged'])\\\n",
    "    .pipe(lambda df: df[df['prompt_ix'] == len(convs) + 10])\\\n",
    "    .pipe(lambda df: df[df['role'] == ('cot')])\\\n",
    "    .groupby('target_role')\\\n",
    "    .agg(combined_text = ('token', ''.join))\\\n",
    "    ['combined_text'].tolist()[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_projections\\\n",
    "    .pipe(lambda df: df[df['prompt_ix'] == 10])\\\n",
    "    .pipe(lambda df: df[df['role'] == ('user')])\\\n",
    "    .groupby('target_role')\\\n",
    "    .agg(combined_text = ('token', ''.join))\n",
    "    ['combined_text'].tolist()[0]\n",
    ")\n",
    "\n",
    "print('-----------')\n",
    "print(test_projections\\\n",
    "    # .pipe(lambda df: df[df['conv_type'] == 'tagged'])\\\n",
    "    .pipe(lambda df: df[df['prompt_ix'] == len(convs) + 10])\\\n",
    "    .pipe(lambda df: df[df['role'] == ('user')])\\\n",
    "    .groupby('target_role')\\\n",
    "    .agg(combined_text = ('token', ''.join))\\\n",
    "    ['combined_text'].tolist()[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "All-layer projections\n",
    "\"\"\"\n",
    "clear_all_cuda_memory()\n",
    "gc.collect()\n",
    "\n",
    "all_projs = [\n",
    "    run_projections(\n",
    "        valid_sample_df = sample_df_labeled.pipe(lambda df: df[(~df['role'].isna()) & (df['role'].isin(probe['role_space']))]),\n",
    "        layer_hs = convs_hs[probe['layer_ix']],\n",
    "        probe = probe\n",
    "    ).assign(layer_ix = probe['layer_ix'], role_space = ''.join([x[0] for x in probe['role_space']]))\n",
    "\n",
    "    for probe in tqdm(all_probes)\n",
    "]\n",
    "\n",
    "all_projs_df =\\\n",
    "    pd.concat(all_projs, ignore_index = True)\\\n",
    "    .merge(sample_df_labeled[['prompt_ix', 'sample_ix', 'token_in_prompt_ix', 'token', 'role']], how = 'inner', on = ['sample_ix'])\\\n",
    "    .merge(input_convs_df[['prompt_ix', 'dataset', 'conv_id', 'conv_type']], how = 'inner', on = 'prompt_ix', validate = 'many_to_one')\n",
    "\n",
    "all_projs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Validation\n",
    "\"\"\"\n",
    "out =\\\n",
    "    all_projs_df\\\n",
    "    .pipe(lambda df: df[df['target_role'] == df['role']])\\\n",
    "    .drop(columns = 'target_role')\\\n",
    "    .rename(columns = {'prob': 'acc'})\\\n",
    "    .groupby(['conv_type', 'role_space', 'layer_ix', 'role', 'prompt_ix'], as_index = False)\\\n",
    "    .agg(mean_acc = ('acc', 'mean'))\\\n",
    "    .groupby(['conv_type', 'role_space', 'layer_ix', 'role'], as_index = False)\\\n",
    "    .agg(mean_acc = ('mean_acc', 'mean'))\n",
    "\n",
    "def draw_proj_val_table(input_df):\n",
    "\n",
    "    def color_thresholds(v):\n",
    "        if pd.isna(v):\n",
    "            return \"\"\n",
    "        if v > 0.9:\n",
    "            return \"background-color: #00c853; color: black;\" # green\n",
    "        elif v > 0.75:\n",
    "            return \"background-color: #aeea00; color: black;\" # green-yellow\n",
    "        elif v > 0.6:\n",
    "            return \"background-color: #ffeb3b; color: black;\" # yellow\n",
    "        return \"\"\n",
    "    \n",
    "    wide =\\\n",
    "        input_df\\\n",
    "        .assign(role = lambda df: np.where(df['role'] == 'assistant', 'asst', df['role']))\\\n",
    "        .pivot(index = ['conv_type', 'layer_ix'], columns = ['role_space', 'role'], values = 'mean_acc')\n",
    "\n",
    "    col_groups = wide.columns.get_level_values(0)\n",
    "    group_starts = [0] + [i for i in range(1, len(col_groups)) if col_groups[i] != col_groups[i-1]]\n",
    "    divider_styles = []\n",
    "    for i in group_starts:\n",
    "        divider_styles += [\n",
    "            {\"selector\": f\"td.col{i}\", \"props\": [(\"border-left\", \"3px solid #ddd\")]},\n",
    "            {\"selector\": f\"th.col_heading.col{i}\", \"props\": [(\"border-left\", \"3px solid #ddd\")]},\n",
    "        ]\n",
    "\n",
    "    return wide.style.format(\"{:.2f}\").map(color_thresholds).set_table_styles(divider_styles, overwrite = False)\n",
    "\n",
    "display(draw_proj_val_table(out))\n",
    "out.to_csv(f'{ws}/experiments/role-analysis/outputs/probe-projections/all_conv_projs_{model_prefix}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Validation: real accuracy (argmax) instead of mean prob on true class\n",
    "\"\"\"\n",
    "hard_preds = (\n",
    "    all_projs_df\n",
    "    .sort_values(\n",
    "        [\"conv_type\", \"role_space\", \"layer_ix\", \"sample_ix\", \"prob\"],\n",
    "        ascending = [True, True, True, True, False],\n",
    "    )\n",
    "    .drop_duplicates([\"conv_type\", \"role_space\", \"layer_ix\", \"sample_ix\"])  # keep max-prob row\n",
    "    .rename(columns = {\"target_role\": \"pred_role\", \"prob\": \"pred_prob\"})\n",
    "    .assign(is_acc = lambda df: df[\"pred_role\"] == df[\"role\"])\n",
    ")\n",
    "\n",
    "out = (\n",
    "    hard_preds\n",
    "    .groupby([\"conv_type\", \"role_space\", \"layer_ix\", \"role\", \"prompt_ix\"], as_index = False)\n",
    "    .agg(mean_acc = (\"is_acc\", \"mean\"))\n",
    "    .groupby([\"conv_type\", \"role_space\", \"layer_ix\", \"role\"], as_index = False)\n",
    "    .agg(mean_acc = (\"mean_acc\", \"mean\"))\n",
    ")\n",
    "\n",
    "display(draw_proj_val_table(out))\n",
    "out.to_csv(f\"{ws}/experiments/role-analysis/outputs/probe-projections/all_conv_acc_{model_prefix}.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alt-model convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prep wrong-convs\n",
    "\"\"\"\n",
    "all_model_files = [\n",
    "    'haiku-4.5',\n",
    "    'minimax-m2.1',\n",
    "    'kimi-k2-thinking'\n",
    "]\n",
    "\n",
    "def get_multiple_model_convs(comparison_models):\n",
    "    \"\"\"\n",
    "    Get multiple model conversations\n",
    "    \"\"\"\n",
    "    m_dfs = [pd.read_csv(f'{ws}/experiments/role-analysis/data/conversations/{m}.csv').assign(model = m) for m in comparison_models]\n",
    "    m_df = (\n",
    "        pd.concat(m_dfs, ignore_index = True)\n",
    "        .assign(conv_id = lambda df: df.groupby(['model', 'conv_id']).ngroup())\n",
    "    )\n",
    "    return m_df\n",
    "\n",
    "alt_model_df = get_multiple_model_convs(all_model_files)\n",
    "alt_convs, alt_convs_df = filter_convs(alt_model_df, max_samples = 30)\n",
    "\n",
    "final_alt_convs = fold_cot_into_final(alt_convs)\n",
    "all_wrong_convs_tagged = [\n",
    "    test_prefix + seperators['tagged_start_sep'] + x\n",
    "    for x in tokenizer.apply_chat_template(final_alt_convs, tokenize = False, add_generation_prompt = False)\n",
    "]\n",
    "print(all_wrong_convs_tagged[0])\n",
    "\n",
    "# Find max input length\n",
    "max_alt_input_length =\\\n",
    "    tokenizer(all_wrong_convs_tagged, padding = True, truncation = True, max_length = 1024 * 8, return_tensors = 'pt')\\\n",
    "    ['attention_mask'].sum(dim = 1).max().item()\n",
    "\n",
    "print(f\"Max input length: {max_alt_input_length}\")\n",
    "print(f\"Tagged convs: {len(all_wrong_convs_tagged)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "clear_all_cuda_memory()\n",
    "# del res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run forward passes\n",
    "\"\"\"\n",
    "# Get input df\n",
    "input_alt_convs_df =\\\n",
    "    pd.concat([\n",
    "        pd.DataFrame({'convs': v, 'conv_type': conv_type})\\\n",
    "            .assign(conv_id = lambda df: range(0, len(df)))\\\n",
    "            .merge(alt_convs_df, how = 'inner', on = 'conv_id')\n",
    "        for conv_type, v in {'alt_tagged': all_wrong_convs_tagged}.items()\n",
    "    ], ignore_index = True)\\\n",
    "    .assign(prompt_ix = lambda df: list(range(0, len(df))))\n",
    "    \n",
    "display(input_alt_convs_df)\n",
    "\n",
    "alt_convs_dl = DataLoader(\n",
    "    ReconstructableTextDataset(input_alt_convs_df['convs'].tolist(), tokenizer, max_length = max_alt_input_length, prompt_ix = list(range(0, len(input_alt_convs_df)))),\n",
    "    batch_size = 4,\n",
    "    shuffle = False,\n",
    "    collate_fn = stack_collate\n",
    ")\n",
    "\n",
    "alt_convs_outputs = run_and_export_states(model, alt_convs_dl, layers_to_keep_acts = layers_to_probe)\n",
    "alt_convs_hs = alt_convs_outputs['all_hs'].to(torch.float16)\n",
    "alt_convs_hs = {layer_ix: alt_convs_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(layers_to_probe)} # Match layers_to_keep_act\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Label messages\n",
    "\"\"\"\n",
    "alt_sample_df_labeled = label_convs(input_alt_convs_df, alt_convs_outputs['sample_df'])\n",
    "assert alt_convs_hs[0].shape[0] == len(alt_sample_df_labeled)\n",
    "alt_sample_df_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "All-layer projections\n",
    "\"\"\"\n",
    "clear_all_cuda_memory()\n",
    "gc.collect()\n",
    "\n",
    "alt_projs = [\n",
    "    run_projections(\n",
    "        valid_sample_df = alt_sample_df_labeled.pipe(lambda df: df[(~df['role'].isna()) & (df['role'].isin(probe['role_space']))]),\n",
    "        layer_hs = alt_convs_hs[probe['layer_ix']],\n",
    "        probe = probe\n",
    "    ).assign(layer_ix = probe['layer_ix'], role_space = ''.join([x[0] for x in probe['role_space']]))\n",
    "\n",
    "    for probe in tqdm(all_probes)\n",
    "]\n",
    "\n",
    "alt_projs_df =\\\n",
    "    pd.concat(alt_projs, ignore_index = True)\\\n",
    "    .merge(alt_sample_df_labeled[['prompt_ix', 'sample_ix', 'token_in_prompt_ix', 'token', 'role']], how = 'inner', on = ['sample_ix'])\\\n",
    "    .merge(input_alt_convs_df[['prompt_ix', 'dataset', 'conv_id', 'conv_type']], how = 'inner', on = 'prompt_ix', validate = 'many_to_one')\n",
    "\n",
    "alt_projs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Validation\n",
    "\"\"\"\n",
    "out2 =\\\n",
    "    alt_projs_df\\\n",
    "    .pipe(lambda df: df[df['target_role'] == df['role']])\\\n",
    "    .drop(columns = 'target_role')\\\n",
    "    .rename(columns = {'prob': 'acc'})\\\n",
    "    .groupby(['conv_type', 'role_space', 'layer_ix', 'role', 'prompt_ix'], as_index = False)\\\n",
    "    .agg(mean_acc = ('acc', 'mean'))\\\n",
    "    .groupby(['conv_type', 'role_space', 'layer_ix', 'role'], as_index = False)\\\n",
    "    .agg(mean_acc = ('mean_acc', 'mean'))\n",
    "\n",
    "display(draw_proj_val_table(out2)) \n",
    "out2.to_csv(f'{ws}/experiments/role-analysis/outputs/probe-projections/alt_conv_projs_{model_prefix}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Validation: real accuracy (argmax) instead of mean prob on true class\n",
    "\"\"\"\n",
    "hard_preds = (\n",
    "    alt_projs_df\n",
    "    .sort_values(\n",
    "        [\"conv_type\", \"role_space\", \"layer_ix\", \"sample_ix\", \"prob\"],\n",
    "        ascending = [True, True, True, True, False],\n",
    "    )\n",
    "    .drop_duplicates([\"conv_type\", \"role_space\", \"layer_ix\", \"sample_ix\"])  # keep max-prob row\n",
    "    .rename(columns = {\"target_role\": \"pred_role\", \"prob\": \"pred_prob\"})\n",
    "    .assign(is_acc = lambda df: df[\"pred_role\"] == df[\"role\"])\n",
    ")\n",
    "\n",
    "out = (\n",
    "    hard_preds\n",
    "    .groupby([\"conv_type\", \"role_space\", \"layer_ix\", \"role\", \"prompt_ix\"], as_index = False)\n",
    "    .agg(mean_acc = (\"is_acc\", \"mean\"))\n",
    "    .groupby([\"conv_type\", \"role_space\", \"layer_ix\", \"role\"], as_index = False)\n",
    "    .agg(mean_acc = (\"mean_acc\", \"mean\"))\n",
    ")\n",
    "\n",
    "display(draw_proj_val_table(out))\n",
    "out.to_csv(f\"{ws}/experiments/role-analysis/outputs/probe-projections/alt_conv_acc_{model_prefix}.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tomato example (run for gpt-oss-20b only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define append prefix\n",
    "\"\"\"\n",
    "tomato_prefix = tokenizer.bos_token or ''\n",
    "tomato_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Format tomato-style conversations\n",
    "\"\"\"\n",
    "def load_messages(yaml_path, model_key):\n",
    "    \"\"\"Loads system-like, assistant-like, user-like, etc. messages\"\"\"\n",
    "    with open(yaml_path, 'r', encoding = 'utf-8') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    return [x['content'] for x in data[model_key]]\n",
    "\n",
    "base_message_types = ['system', 'user', 'cot', 'assistant', 'user', 'cot', 'assistant']\n",
    "base_messages = load_messages(f\"{ws}/experiments/role-analysis/experiment-config/tomato.yaml\", 'gptoss20')\n",
    "\n",
    "tomato_prompts = {}\n",
    " \n",
    "tomato_prompts['basic_no_format'] = tomato_prefix + ' ' + '\\n\\n'.join(base_messages)\n",
    "\n",
    "tomato_prompts['everything_in_user_tags'] = tomato_prefix + tokenizer.apply_chat_template(\n",
    "    [{'role': 'user', 'content': '\\n\\n'.join(base_messages)}],\n",
    "    tokenize = False, add_generation_prompt = False\n",
    ")\n",
    "\n",
    "tomato_prompts['proper_tags'] = tomato_prefix + tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {'role': 'system', 'content': base_messages[0]},\n",
    "        {'role': 'user', 'content': base_messages[1]},\n",
    "        {'role': 'assistant', 'content': f\"<think>{base_messages[2]}</think>{base_messages[3]}\"},\n",
    "        {'role': 'user', 'content': base_messages[4]},\n",
    "        {'role': 'assistant', 'content': f\"<tool_call>{base_messages[5]}<tool_call>{base_messages[6]}\"}\n",
    "    ],\n",
    "    tokenize = False, add_generation_prompt = False\n",
    ")\n",
    "\n",
    "print(tomato_prompts['proper_tags'])\n",
    "\n",
    "tomato_input_df =\\\n",
    "    pd.DataFrame({\n",
    "        'prompt': [p for _, p in tomato_prompts.items()],\n",
    "        'prompt_key': [pk for pk, _ in tomato_prompts.items()]\n",
    "    })\\\n",
    "    .assign(prompt_ix = lambda df: list(range(0, len(df))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run!!\n",
    "\"\"\"\n",
    "tomato_dl = DataLoader(\n",
    "    ReconstructableTextDataset(tomato_input_df['prompt'].tolist(), tokenizer, max_length = 512 * 4, prompt_ix = tomato_input_df['prompt_ix'].tolist()),\n",
    "    batch_size = 16,\n",
    "    shuffle = False,\n",
    "    collate_fn = stack_collate\n",
    ")\n",
    "\n",
    "tomato_outputs = run_and_export_states(model, tomato_dl, layers_to_keep_acts = layers_to_probe)\n",
    "\n",
    "tomato_hs = tomato_outputs['all_hs'].to(torch.float16)\n",
    "tomato_hs = {layer_ix: tomato_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(layers_to_probe)} # Match layers_to_keep_act\n",
    "print(tomato_hs[0].shape)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ensure tokens/section is reasonable; map to base_message_type\n",
    "\"\"\"\n",
    "tomato_sample_df_labeled =\\\n",
    "    flag_message_types(tomato_outputs['sample_df'], base_messages)\\\n",
    "    .assign(sample_ix = lambda df: range(0, len(df)))\\\n",
    "    .assign(token_in_prompt_ix = lambda df: df.groupby(['prompt_ix']).cumcount())\\\n",
    "    .merge(tomato_input_df, on = 'prompt_ix', how = 'inner')\\\n",
    "    .merge(\n",
    "        pd.DataFrame({'base_message_ix': list(range(len(base_message_types))), 'base_message_type': base_message_types}),\n",
    "        on = 'base_message_ix',\n",
    "        how = 'inner'\n",
    "    )\n",
    "\n",
    "tomato_sample_df_labeled\\\n",
    "    .groupby(['prompt_ix', 'prompt_key', 'base_message_ix', 'base_message_type', 'base_message'], as_index = False)\\\n",
    "    .agg(\n",
    "        count = ('token', 'count'),\n",
    "        combined_text = ('token', ''.join)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "tomato_projs = [\n",
    "    run_projections(\n",
    "        valid_sample_df = tomato_sample_df_labeled.pipe(lambda df: df[(~df['base_message_type'].isna()) & (df['base_message_type'].isin(probe['role_space']))]),\n",
    "        layer_hs = tomato_hs[probe['layer_ix']],\n",
    "        probe = probe\n",
    "    ).assign(layer_ix = probe['layer_ix'], role_space = ''.join([x[0] for x in probe['role_space']]))\n",
    "\n",
    "    for probe in tqdm(all_probes)\n",
    "]\n",
    "\n",
    "tomato_projs_df =\\\n",
    "    pd.concat(tomato_projs, ignore_index = True)\\\n",
    "    .merge(\n",
    "        tomato_sample_df_labeled[['sample_ix', 'prompt_ix', 'prompt_key', 'base_message_ix', 'base_message_type', 'token_in_prompt_ix', 'token']],\n",
    "        how = 'inner',\n",
    "        on = ['sample_ix']\n",
    "    )\n",
    "\n",
    "tomato_projs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomato_projs_df\\\n",
    "    .groupby(['prompt_ix', 'prompt_key', 'target_role', 'layer_ix', 'role_space', 'sample_ix'], as_index = False)\\\n",
    "    .agg(count = ('sample_ix', 'count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "facet_order = ['system', 'user', 'cot', 'assistant']\n",
    "tomato_projs_df_single = tomato_projs_df.pipe(lambda df: df[(df['role_space'] == 'suca') & (df['layer_ix'] == 14)])\n",
    "\n",
    "all_message_types = tomato_projs_df['base_message_type'].unique()\n",
    "color_map = {msg_type: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] for i, msg_type in enumerate(all_message_types)}\n",
    "\n",
    "group_cols = ['prompt_ix','prompt_key','base_message_type','target_role']\n",
    "\n",
    "smooth_df = (\n",
    "    tomato_projs_df_single\n",
    "    .sort_values(group_cols + ['sample_ix'])\n",
    "    .assign(\n",
    "        prob_sma = lambda df: (\n",
    "            df.groupby(group_cols)['prob']\n",
    "              .rolling(window=10, min_periods=1).mean()\n",
    "              .reset_index(level=list(range(len(group_cols))), drop=True)\n",
    "        ),\n",
    "        prob_ewma = lambda df: (\n",
    "            df.groupby(group_cols)['prob']\n",
    "              .ewm(alpha=.1, min_periods=1).mean()\n",
    "              .reset_index(level=list(range(len(group_cols))), drop=True)\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "display(smooth_df)\n",
    "\n",
    "for this_prompt_key in smooth_df['prompt_key'].unique().tolist():\n",
    "    this_df = smooth_df.pipe(lambda df: df[df['prompt_key'] == this_prompt_key])\n",
    "    fig = px.scatter(\n",
    "        this_df, x = 'token_in_prompt_ix', y = 'prob',\n",
    "        facet_row = 'target_role',\n",
    "        color = 'base_message_type',\n",
    "        color_discrete_map = color_map,\n",
    "        category_orders = {\n",
    "            'target_role': facet_order,\n",
    "            'base_message_type': ['system', 'user', 'cot', 'assistant', 'other']\n",
    "        },\n",
    "        hover_name = 'token',\n",
    "        hover_data = {\n",
    "            'prob': ':.4f'\n",
    "        },\n",
    "        # markers = True,\n",
    "        title = f'prompt = {this_prompt_key}',\n",
    "        labels = {\n",
    "            'token_in_prompt_ix': 'Token Index',\n",
    "            'prob': 'Prob',\n",
    "            'prob_smoothed': 'Smoothed Prob',\n",
    "            'target_role': 'role'\n",
    "        }\n",
    "    ).update_yaxes(\n",
    "        range = [0, 1],\n",
    "        side = 'left'\n",
    "    ).update_layout(height = 500, width = 800)\n",
    "\n",
    "    def pretty(a):\n",
    "        a.update(\n",
    "            text = a.text.split('=')[-1],\n",
    "            x = 0.5, xanchor = \"center\",\n",
    "            y = a.y + 0.08,\n",
    "            textangle = 0,\n",
    "            font = dict(size = 12),\n",
    "            bgcolor = 'rgba(255, 255, 255, 0.9)',\n",
    "            showarrow = False\n",
    "        )\n",
    "\n",
    "    fig.for_each_annotation(pretty)\n",
    "    fig.update_yaxes(title_text = None)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomato_projs_df.to_csv(f\"{ws}/experiments/role-analysis/outputs/probe-projections/tomato-role-projections-{model_prefix}.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009ac414",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This generates conversational datasets for each model - this is used for evaluating validity of role probes. This takes user \n",
    "messages from toxicchat/oasst, then generates model-specific assistant responses for each model using Openrouter. Allows for\n",
    "multiturn conversations. Runs models locally as a fallback if unavailable via API.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1d05de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.openrouter import get_openrouter_responses\n",
    "\n",
    "seed = 1234\n",
    "\n",
    "load_dotenv('.env')\n",
    "std_params = {'temperature': 1, 'top_p': 1, 'top_k': 0} # Most reasoning models require 1 temp\n",
    "ws = '/workspace/deliberative-alignment-jailbreaks'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56052e9c",
   "metadata": {},
   "source": [
    "# Import conversations + retain only user messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370f5a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load toxic-chat\n",
    "\"\"\"\n",
    "def load_raw_ds(n_samples = 1000):\n",
    "    def get_convs():\n",
    "        return load_dataset('lmsys/toxic-chat', 'toxicchat1123', split = 'train').shuffle(seed = seed)\n",
    "    \n",
    "    def get_data(ds, n_samples):\n",
    "        raw_data = []\n",
    "        for sample in ds:\n",
    "            user_text = sample['user_input']\n",
    "            if len(user_text) >= 100 and len(user_text) <= 500:\n",
    "                raw_data.append([user_text])\n",
    "            if len(raw_data) >= n_samples:\n",
    "                break\n",
    "        return raw_data\n",
    "    \n",
    "    return get_data(get_convs(), n_samples)\n",
    "\n",
    "toxicchat_ds = load_raw_ds(100)\n",
    "toxicchat_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34d1d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Oasst data\n",
    "\"\"\"\n",
    "def load_oasst_conversations(n_samples = 1_000):\n",
    "    ds = load_dataset('OpenAssistant/oasst1', split = 'train', streaming = False) # Do not shuf\n",
    "\n",
    "    # Extract valid rows\n",
    "    rows = [\n",
    "        {\n",
    "            'message_tree_id': ex['message_tree_id'],\n",
    "            'message_id': ex['message_id'],\n",
    "            'parent_id': ex['parent_id'],\n",
    "            'role': ex['role'],          # \"prompter\" or \"assistant\"\n",
    "            'text': ex['text'],\n",
    "        }\n",
    "        for ex in ds\n",
    "        if ex['lang'] == 'en' and ex['tree_state'] == 'ready_for_export'\n",
    "    ]    \n",
    "\n",
    "    # Group by tree ID\n",
    "    trees = {}\n",
    "    for r in rows:\n",
    "        tid = r[\"message_tree_id\"]\n",
    "        trees.setdefault(tid, []).append(r)\n",
    "\n",
    "    # Iterate over trees, reconstruct convs\n",
    "    conversations = []\n",
    "    tree_ids = list(trees.keys())\n",
    "\n",
    "    for tid in tree_ids:\n",
    "        msgs = trees[tid]\n",
    "        \n",
    "        # Build lookups\n",
    "        id2msg = {m['message_id']: m for m in msgs}\n",
    "        children = {}\n",
    "        for m in msgs:\n",
    "            pid = m[\"parent_id\"]\n",
    "            if pid:\n",
    "                children.setdefault(pid, []).append(m[\"message_id\"])\n",
    "\n",
    "        # Find Root\n",
    "        roots = [m for m in msgs if not m[\"parent_id\"]]\n",
    "        if not roots:\n",
    "            continue\n",
    "        root = roots[0]\n",
    "\n",
    "        # Reconstruct linear path (Root -> Leaf)\n",
    "        path_ids = [root[\"message_id\"]]\n",
    "        cur_id = root[\"message_id\"]\n",
    "        \n",
    "        while cur_id in children:\n",
    "            child_id = children[cur_id][0] # Simple heuristic: take first child\n",
    "            path_ids.append(child_id)\n",
    "            cur_id = child_id\n",
    "\n",
    "        conv = [id2msg[mid] for mid in path_ids]\n",
    "\n",
    "        # Validation tree\n",
    "        is_valid = True\n",
    "        for msg in conv:\n",
    "            if msg['role'] == 'prompter':\n",
    "                # If ANY user message is out of bounds, the whole conv is bad\n",
    "                if not (100 <= len(msg['text']) <= 500):\n",
    "                    is_valid = False\n",
    "                    break\n",
    "        \n",
    "        if is_valid:\n",
    "            conversations.append(conv)\n",
    "\n",
    "        if n_samples is not None and len(conversations) >= n_samples:\n",
    "            break\n",
    "\n",
    "    return conversations\n",
    "\n",
    "oasst_convs = load_oasst_conversations(100)\n",
    "oasst_user_only = [\n",
    "    [m['text'] for m in conv if m['role'] == 'prompter']\n",
    "    for conv in oasst_convs\n",
    "]\n",
    "oasst_user_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a49484",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assign conversation ids and user query indices within each conv; truncate convs to 2 queries\n",
    "\"\"\"\n",
    "user_queries_df =\\\n",
    "    pd.concat([\n",
    "        pd.DataFrame({'convs': oasst_user_only}).assign(dataset = 'oasst'),\n",
    "        pd.DataFrame({'convs': toxicchat_ds}).assign(dataset = 'toxicchat')\n",
    "    ], ignore_index = True)\\\n",
    "    .assign(conv_id = lambda df: range(len(df)))\\\n",
    "    .explode('convs')\\\n",
    "    .assign(user_query_ix = lambda df: df.groupby('conv_id').cumcount())\\\n",
    "    .pipe(lambda df: df[df['user_query_ix'] <= 1])\\\n",
    "    .rename(columns = {'convs': 'user_query'})\\\n",
    "    [['conv_id', 'dataset', 'user_query_ix', 'user_query']]\n",
    "\n",
    "display(user_queries_df.groupby('dataset', as_index = False).agg(convs = ('conv_id', 'nunique')))\n",
    "display(user_queries_df.groupby('conv_id', as_index = False).agg(n_msg = ('user_query_ix', 'max')).groupby('n_msg', as_index = False).agg(n_msg_count = ('n_msg', 'count')))\n",
    "\n",
    "user_queries_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45428876",
   "metadata": {},
   "source": [
    "# Generate assistant/cot responses by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af0928c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define target models\n",
    "\"\"\"\n",
    "target_models = [\n",
    "    # Baseline models\n",
    "    {'model': 'moonshotai/kimi-k2-thinking', 'model_provider': 'google-vertex', 'model_prefix': 'kimi-k2-thinking'}, # U\n",
    "    {'model': 'anthropic/claude-haiku-4.5', 'model_provider': 'google-vertex', 'model_prefix': 'haiku-4.5'}, # U\n",
    "    {'model': 'minimax/minimax-m2.1', 'model_provider': 'minimax/fp8', 'model_prefix': 'minimax-m2.1'}, # U\n",
    "    {'model': 'qwen/qwen3-next-80b-a3b-thinking', 'model_provider': 'google-vertex', 'model_prefix': 'qwen3-next'}, # U\n",
    "\n",
    "    # Test cases\n",
    "    {'model': 'openai/gpt-oss-20b', 'model_provider': 'nebius/fp4', 'model_prefix': 'gptoss-20b'}, # U\n",
    "    {'model': 'openai/gpt-oss-120b', 'model_provider': 'nebius/fp4', 'model_prefix': 'gptoss-120b'}, # U\n",
    "    {'model': 'nvidia/nemotron-3-nano-30b-a3b', 'model_provider': 'deepinfra/bf16', 'model_prefix': 'nemotron-3-nano'}, # U\n",
    "    {'model': 'z-ai/glm-4.6v', 'model_provider': 'z-ai/fp8', 'model_prefix': 'glm-4.6v-flash'}, # U\n",
    "    {'model': 'qwen/qwen3-30b-a3b-thinking-2507', 'model_provider': 'alibaba', 'model_prefix': 'qwen3-30b-a3b'}, # U\n",
    "    {'model': 'z-ai/glm-4.7-flash', 'model_provider': 'novita/bf16', 'model_prefix': 'glm-4.7-flash'}, # U\n",
    "]\n",
    "\n",
    "def _validate_and_extract_response(llm_response):\n",
    "    \"\"\"\n",
    "    Extract content/reasoning from response\n",
    "    \n",
    "    Params:\n",
    "        @llm_response: The LLM response object\n",
    "    \"\"\"\n",
    "    if 'choices' not in llm_response:\n",
    "        print(llm_response)\n",
    "        return {'reasoning': None, 'output': None}\n",
    "\n",
    "    choice = llm_response['choices'][0]\n",
    "    if choice['finish_reason'] == 'length':\n",
    "        print(f\"Warning - early stop: {choice['finish_reason']}\")\n",
    "        # print(f\"  CONTENT: {choice['message'].get('content')}\")\n",
    "        # print(f\"  REASONING: {choice['message'].get('reasoning')}\")\n",
    "        return {'reasoning': None, 'output': None}\n",
    "\n",
    "    content = choice['message'].get('content')\n",
    "    if content is None or (isinstance(content, str) and content.strip() == ''):\n",
    "        return {'reasoning': None, 'output': None}\n",
    "\n",
    "    return {\n",
    "        'reasoning': choice['message'].get('reasoning'),\n",
    "        'output': content,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaf35a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate data and save them by model!\n",
    "\"\"\"\n",
    "BATCH_SIZE = 10\n",
    "MAX_TOKS = 4_000 # Cull to deal with models with endless reasoning\n",
    "\n",
    "async def generate_convs_data(user_queries_df, target_dir, target_models):\n",
    "    \"\"\"\n",
    "    Iterate over target models\n",
    "    \"\"\" \n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    all_model_results = []\n",
    "\n",
    "    query_lookup = user_queries_df.set_index(['conv_id', 'user_query_ix'])['user_query'].to_dict()\n",
    "    max_rounds = user_queries_df['user_query_ix'].max() + 1\n",
    "\n",
    "    for target_model in target_models:\n",
    "        model_name = target_model['model']\n",
    "        model_prefix = target_model['model_prefix']\n",
    "        out_path = os.path.join(target_dir, model_prefix + '.csv')\n",
    "        \n",
    "        if os.path.exists(out_path):\n",
    "            print(f\"Skipping {model_name} - already exists\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n{'='*50}\\nProcessing model: {model_name}\\n{'='*50}\")\n",
    "        responses = {}\n",
    "        failed_conv_ids = set()\n",
    "\n",
    "        for round_ix in range(max_rounds):\n",
    "            print(f\"Processing round {round_ix}...\")\n",
    "            current_round_df = user_queries_df[user_queries_df['user_query_ix'] == round_ix]\n",
    "            \n",
    "            message_histories = []\n",
    "            conv_ids = []\n",
    "\n",
    "            for _, row in current_round_df.iterrows():\n",
    "                cid = row['conv_id']\n",
    "                \n",
    "                # Skip if conversation is already marked as failed\n",
    "                if cid in failed_conv_ids:\n",
    "                    responses[(cid, round_ix)] = {'output': None, 'reasoning': None}\n",
    "                    continue\n",
    "                \n",
    "                messages = []\n",
    "                for prev_ix in range(round_ix):\n",
    "                    prev_query = query_lookup.get((cid, prev_ix))\n",
    "                    messages.append({'role': 'user', 'content': prev_query})\n",
    "                    \n",
    "                    prev_response = responses[(cid, prev_ix)]\n",
    "                    messages.append({'role': 'assistant', 'content': prev_response['output']})\n",
    "            \n",
    "                messages.append({'role': 'user', 'content': row['user_query']})\n",
    "                message_histories.append(messages)\n",
    "                conv_ids.append(cid)\n",
    "        \n",
    "            if not message_histories:\n",
    "                print(f\"  All conversations skipped due to prior failures\")\n",
    "                continue\n",
    "        \n",
    "            raw_llm_responses = await get_openrouter_responses(\n",
    "                message_histories,\n",
    "                {\n",
    "                    'model': target_model['model'],\n",
    "                    'provider': {'order': [target_model['model_provider']], 'allow_fallbacks': False},\n",
    "                    'reasoning': {'effort': 'medium', 'enabled': True},\n",
    "                    'max_tokens': MAX_TOKS,\n",
    "                    **std_params\n",
    "                },\n",
    "                batch_size = BATCH_SIZE\n",
    "            )\n",
    "        \n",
    "            for conv_id, raw_response in zip(conv_ids, raw_llm_responses, strict = True):\n",
    "                extracted = _validate_and_extract_response(raw_response)\n",
    "                responses[(conv_id, round_ix)] = extracted\n",
    "                \n",
    "                # Mark as failed immediately if output is None\n",
    "                if extracted['output'] is None:\n",
    "                    failed_conv_ids.add(conv_id)\n",
    "            \n",
    "            print(f\"  Failed: {len(failed_conv_ids)}\")\n",
    "\n",
    "        responses_df = pd.DataFrame([\n",
    "            {'conv_id': c, 'user_query_ix': ix, **r}\n",
    "            for (c, ix), r in responses.items()\n",
    "        ]).rename(columns={'output': 'assistant', 'reasoning': 'cot'})\n",
    "        \n",
    "        if responses_df.empty:\n",
    "             print(f\"  Warning: No responses generated for {model_name}\")\n",
    "             continue\n",
    "\n",
    "        model_results_df = user_queries_df.merge(responses_df, on=['conv_id', 'user_query_ix'], how='left')\n",
    "        model_results_df['model'] = model_name\n",
    "        model_results_df['model_prefix'] = model_prefix\n",
    "\n",
    "        is_invalid = (\n",
    "            model_results_df['assistant'].isna() | \n",
    "            (model_results_df['assistant'].astype(str).str.strip() == '')\n",
    "        )\n",
    "        poisoned_ids = model_results_df.loc[is_invalid, 'conv_id'].unique()\n",
    "        \n",
    "        if len(poisoned_ids) > 0:\n",
    "            print(f\"  Dropping {len(poisoned_ids)} conversations.\")\n",
    "            \n",
    "        clean_results_df = model_results_df[~model_results_df['conv_id'].isin(poisoned_ids)].copy()\n",
    "\n",
    "        if not clean_results_df.empty:\n",
    "            clean_results_df.to_csv(out_path, index=False)\n",
    "            all_model_results.append(clean_results_df)\n",
    "\n",
    "    if not all_model_results:\n",
    "        return None\n",
    "    \n",
    "    return pd.concat(all_model_results, ignore_index=True)\n",
    "\n",
    "\n",
    "result_df = await generate_convs_data(\n",
    "    user_queries_df,\n",
    "    target_dir = f\"{ws}/experiments/role-analysis/data/conversations\",\n",
    "    target_models = target_models\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50b0021",
   "metadata": {},
   "source": [
    "# Local fallback for unavailable models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f65ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "All below code is a fallback for models which do NOT have available endpoints. Avoid if possible.\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "from utils.loader import load_model_and_tokenizer\n",
    "from utils.memory import check_memory\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "\n",
    "model_prefix = 'gptoss-20b'\n",
    "tokenizer, model, model_architecture, model_n_layers = load_model_and_tokenizer(model_prefix, device = main_device)\n",
    "\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0769afde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run generations\n",
    "\"\"\"\n",
    "MAX_INPUT_TOKENS = 1024 * 12\n",
    "MAX_NEW_TOKENS = 4_000\n",
    "TEMPERATURE = 1.0\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "def chunk_list(lst, n):\n",
    "    return [lst[i:i + n] for i in range(0, len(lst), n)]\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_local_batch(message_histories):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        @message_histories: list[list[{'role': str, 'content': str}]]\n",
    "    \n",
    "    Returns:\n",
    "        list[str] decoded new tokens only\n",
    "    \"\"\"\n",
    "    enc = tokenizer.apply_chat_template(\n",
    "        message_histories,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True,\n",
    "        return_tensors = 'pt',\n",
    "        padding = True,\n",
    "        truncation = True,\n",
    "        max_length = MAX_INPUT_TOKENS,\n",
    "        return_dict = True\n",
    "    )\n",
    "\n",
    "    input_ids = enc['input_ids'].to(main_device)\n",
    "    attention_mask = enc['attention_mask'].to(main_device)\n",
    "    prompt_len = input_ids.shape[1] # IMPORTANT: with padding, prompt_len is the padded length, so new tokens start here for all rows\n",
    "\n",
    "    out = model.generate(\n",
    "        input_ids = input_ids,\n",
    "        attention_mask = attention_mask,\n",
    "        max_new_tokens = MAX_NEW_TOKENS,\n",
    "        do_sample = True,\n",
    "        temperature = TEMPERATURE,\n",
    "        pad_token_id = tokenizer.pad_token_id,\n",
    "        eos_token_id = tokenizer.eos_token_id,\n",
    "        use_cache = True,\n",
    "    )\n",
    "\n",
    "    new_tokens = out[:, prompt_len:] # (batch, <=max_new_tokens) padded on the right\n",
    "\n",
    "    assistant_texts, state_texts = [], []\n",
    "\n",
    "    for i in range(out.shape[0]):\n",
    "        prompt_ids = input_ids[i][attention_mask[i].bool()].tolist() # Remove lpad from the prompt using the attention mask\n",
    "\n",
    "        gen_ids = new_tokens[i].tolist()\n",
    "        # Strip right-pad tokens added by batching different gen lengths\n",
    "        while gen_ids and gen_ids[-1] == tokenizer.pad_token_id:\n",
    "            gen_ids.pop()\n",
    "\n",
    "        # Assistant text used for subsequent rounds (no special tokens)\n",
    "        asst = tokenizer.decode(gen_ids, skip_special_tokens = True).strip()\n",
    "        asst = asst if asst else None\n",
    "\n",
    "        # Full composite prompt + output (keep special tokens / instruct formatting)\n",
    "        state_text = tokenizer.decode(prompt_ids + gen_ids, skip_special_tokens = False)\n",
    "\n",
    "        assistant_texts.append(asst)\n",
    "        state_texts.append(state_text)\n",
    "\n",
    "    return assistant_texts, state_texts\n",
    "\n",
    "\n",
    "def generate_convs_data_local(user_queries_df, target_dir):\n",
    "    os.makedirs(target_dir, exist_ok = True)\n",
    "    out_path = os.path.join(target_dir, model_prefix + '-raw.csv')\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Local generation model: {model_prefix}\")\n",
    "\n",
    "    responses = {}\n",
    "    max_rounds = int(user_queries_df['user_query_ix'].max()) + 1\n",
    "\n",
    "    for round_ix in range(max_rounds):\n",
    "        print(f\"Round {round_ix}...\")\n",
    "\n",
    "        current_round_df = user_queries_df[user_queries_df[\"user_query_ix\"] == round_ix]\n",
    "        message_histories = []\n",
    "        conv_ids = []\n",
    "        skipped = 0\n",
    "\n",
    "        for _, row in current_round_df.iterrows():\n",
    "            conv_id = int(row[\"conv_id\"])\n",
    "\n",
    "            # Skip if any prior round missing\n",
    "            prior_failed = any(responses.get((conv_id, prev_ix), {}).get(\"assistant\") is None for prev_ix in range(round_ix))\n",
    "            if prior_failed:\n",
    "                skipped += 1\n",
    "                responses[(conv_id, round_ix)] = {\"assistant\": None, \"state_text\": None}\n",
    "                continue\n",
    "\n",
    "            # Prev turns\n",
    "            msgs = []\n",
    "            for prev_ix in range(round_ix):\n",
    "                prev_user = user_queries_df[(user_queries_df['conv_id'] == conv_id) & (user_queries_df[\"user_query_ix\"] == prev_ix)]['user_query'].item()\n",
    "                msgs.append({\"role\": \"user\", \"content\": prev_user})\n",
    "                prev_asst = responses[(conv_id, prev_ix)][\"assistant\"]\n",
    "                msgs.append({\"role\": \"assistant\", \"content\": prev_asst})\n",
    "                \n",
    "            # Current user turn\n",
    "            msgs.append({\"role\": \"user\", \"content\": row[\"user_query\"]})\n",
    "            message_histories.append(msgs)\n",
    "            conv_ids.append(conv_id)\n",
    "\n",
    "        if not message_histories:\n",
    "            print(f\"  All skipped ({skipped}) due to prior failures\")\n",
    "            continue\n",
    "\n",
    "        all_asst = []\n",
    "        all_state = []\n",
    "        for batch in tqdm(chunk_list(message_histories, BATCH_SIZE)):\n",
    "            batch_asst, batch_state = generate_local_batch(batch)\n",
    "            all_asst.extend(batch_asst)\n",
    "            all_state.extend(batch_state)\n",
    "\n",
    "        for conv_id, asst_text, state_text in zip(conv_ids, all_asst, all_state):\n",
    "            responses[(conv_id, round_ix)] = {\"assistant\": asst_text, \"state_text\": state_text}\n",
    "\n",
    "        print(f\"  Completed: {len(conv_ids)} convs, skipped: {skipped}\")\n",
    "    \n",
    "    responses_df = pd.DataFrame(\n",
    "        [{\"conv_id\": cid, \"user_query_ix\": qix, **resp}\n",
    "        for (cid, qix), resp in responses.items()]\n",
    "    )\n",
    "\n",
    "    model_results_df = (\n",
    "        user_queries_df\n",
    "        .merge(responses_df, on = [\"conv_id\", \"user_query_ix\"], how=\"left\")\n",
    "        .assign(model = model_prefix, model_prefix = model_prefix)\n",
    "    )\n",
    "\n",
    "    model_results_df.to_csv(out_path, index = False)\n",
    "    return model_results_df\n",
    "\n",
    "local_result_df = generate_convs_data_local(\n",
    "    user_queries_df,\n",
    "    target_dir = f\"{ws}/experiments/role-analysis/data/conversations\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a58276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Look for -raw files, extract cot + assistant\n",
    "\"\"\"\n",
    "out_path = os.path.join(f\"{ws}/experiments/role-analysis/data/conversations\", model_prefix + \"-raw.csv\")\n",
    "\n",
    "if os.path.exists(out_path):\n",
    "    raw_df = pd.read_csv(out_path).assign(prompt_ix = lambda df: list(range(0, len(df))))\n",
    "\n",
    "    # Split into token level dataframe\n",
    "    tok_dfs = []\n",
    "    for row in raw_df.to_dict('records'):\n",
    "        enc = tokenizer(row['state_text'], add_special_tokens = False, return_offsets_mapping = True)\n",
    "        substrings = [row['state_text'][s:e] for (s, e) in enc['offset_mapping']]\n",
    "        tok_dfs.append(pd.DataFrame({'token': substrings, 'prompt_ix': row['prompt_ix'], 'token_ix': list(range(0, len(substrings)))}))\n",
    "\n",
    "    tok_df = pd.concat(tok_dfs, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de3feac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split CoT/Assistant\n",
    "\"\"\"\n",
    "import importlib\n",
    "import utils.role_assignments\n",
    "importlib.reload(utils.role_assignments)\n",
    "from utils.role_assignments import label_content_roles\n",
    "\n",
    "sample_df_labeled = (\n",
    "    label_content_roles(model_prefix, tok_df)\n",
    "    .groupby(['prompt_ix', 'seg_ix', 'role'], as_index = False)\n",
    "    .agg(combined_text = ('token', ''.join))\n",
    "    .sort_values(['prompt_ix', 'seg_ix'])\n",
    ")\n",
    "\n",
    "display(sample_df_labeled)\n",
    "\n",
    "\n",
    "mask = sample_df_labeled.groupby('prompt_ix')['role'].transform(\n",
    "    lambda s: s.iloc[-1:].tolist() == ['assistant']\n",
    ")\n",
    "sample_df_tail = (\n",
    "    sample_df_labeled[mask]\n",
    "    .groupby('prompt_ix')\n",
    "    .tail(1)\n",
    ")\n",
    "\n",
    "final = (\n",
    "    sample_df_tail\n",
    "    .pivot(index='prompt_ix', columns='role', values='combined_text')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Ensure both columns exist\n",
    "if 'cot' not in final.columns:\n",
    "    final['cot'] = None\n",
    "if 'assistant' not in final.columns:\n",
    "    final['assistant'] = None  # just in case\n",
    "\n",
    "merged = (\n",
    "    raw_df\n",
    "    .drop(columns=['state_text', 'assistant'])\n",
    "    .merge(final[['prompt_ix', 'cot', 'assistant']], on='prompt_ix', how='inner')\n",
    "    .reset_index(drop=True)\n",
    "    .drop(columns='prompt_ix')\n",
    ")\n",
    "\n",
    "display(merged)\n",
    "\n",
    "merged.to_csv(os.path.join(f\"{ws}/experiments/role-analysis/data/conversations\", model_prefix + \".csv\"), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

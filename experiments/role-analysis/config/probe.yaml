# This file contains training probe training + eval setting for each model.

gptoss-20b:
  n_sample_size: 250
  seq_len: 1024
  nested_reasoning: false
  train_prefixes:
    - ""
  train_params:
    C: 5.0e-3
    add_scaling: false
  test_prefix: |-
    <|startoftext|><|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
    Knowledge cutoff: 2024-06
    Current date: 2025-12-31

    Reasoning: medium

    # Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|>
  test_seperators:
    # For untagged conversations, # lbs between roles (set it to match the true instruct format, - use 1 space if there are no lbs to prevent role mergnig)
    # gpt-oss_*: there are no newlines between roles - add space to prevent cross-role toks
    untagged_role_sep: " "
    # For untagged conversations, # lbs between test_prefix and user text (set it to match the true instruct format)
    # gpt-oss_*: there are no newlines between the test_prefix (system prompt) and the user text - add space to prevent cross-role toks
    untagged_start_sep: " "
    # For tagged conversations, # lbs between the test prefix and user role (including XML tags belonging to the role)
    # gpt-oss_*: there are no newlines between the test_prefix (system prompt) and the first user role tags - add space to prevent cross-role toks
    tagged_start_sep: " "
    # Note there is no tagged_role_sep since the render_single_* outputs should already be concatenatable without extra lbs

gptoss-120b:
  n_sample_size: 250
  seq_len: 1024
  nested_reasoning: false
  train_prefixes:
    - ""
  train_params:
    C: 5.0e-3
    add_scaling: true
  test_prefix: |-
    <|startoftext|><|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
    Knowledge cutoff: 2024-06
    Current date: 2025-12-31

    Reasoning: medium

    # Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|>
  test_seperators:
    untagged_role_sep: " "
    untagged_start_sep: " "
    tagged_start_sep: " "

nemotron-3-nano:
  n_sample_size: 250
  seq_len: 1024
  nested_reasoning: true
  train_prefixes:
    - ""
  train_params:
    C: 1.0e+1
    add_scaling: false
  test_prefix: |-
    <s><|im_start|>system
    <|im_end|>
  test_seperators:
    untagged_role_sep: "\n\n"
    untagged_start_sep: "\n\n"
    tagged_start_sep: "\n"

qwen3-30b-a3b: 
  n_sample_size: 250
  seq_len: 1024
  nested_reasoning: true
  train_prefixes:
    - ""
  train_params:
    C: 1.0e-1
    add_scaling: false
  test_prefix: |-
    <|im_start|>system
    <|im_end|>
  test_seperators:
    untagged_role_sep: "\n\n"
    untagged_start_sep: "\n\n"
    tagged_start_sep: "\n"

jamba-reasoning: 
  n_sample_size: 250
  seq_len: 1024
  nested_reasoning: true
  train_prefixes:
    - ""
  train_params:
    C: 1.0
    add_scaling: false
  test_prefix: |-
    <|startoftext|>
  test_seperators:
    untagged_role_sep: "\n\n"
    untagged_start_sep: "\n"
    tagged_start_sep: ""

# ALL SETTINGS BELOW ARE UNTESTED - MODIFY AS NEEDED
olmo3-7b-think: 
  n_sample_size: 250
  seq_len: 1024
  nested_reasoning: true
  train_prefixes:
    - ""
  train_params:
    C: 1.0e-1
    add_scaling: false
  test_prefix: |-
    <|endoftext|><|im_start|>system
    You are a helpful AI assistant.<|im_end|>
  test_seperators:
    untagged_role_sep: "\n\n"
    untagged_start_sep: "\n\n"
    tagged_start_sep: "\n"

apriel-1.6-15b-thinker: 
  n_sample_size: 250
  seq_len: 1024
  nested_reasoning: true
  train_prefixes:
    - ""
  train_params:
    C: 1.0e-2
    add_scaling: false
  test_prefix: |-
    <s><|begin_system|>
    You are a thoughtful, systematic AI assistant from ServiceNow Language Models (SLAM) lab. Analyze each question carefully, present your reasoning step-by-step, then provide the final response after the marker [BEGIN FINAL RESPONSE].
  test_seperators:
    untagged_role_sep: "\n\n"
    untagged_start_sep: "\n\n"
    tagged_start_sep: "\n"

glm-4.6v-flash:
  n_sample_size: 250
  seq_len: 1024
  nested_reasoning: true
  train_prefixes:
    - ""
  train_params:
    C: 5.0e-4
    add_scaling: false
  test_prefix: |-
    [gMASK]<sop>
  test_seperators:
    untagged_role_sep: "\n\n"
    untagged_start_sep: "\n\n"
    tagged_start_sep: "\n"

glm-4.7-flash:
  n_sample_size: 250
  seq_len: 1024
  nested_reasoning: true
  train_prefixes:
    - ""
  train_params:
    C: 1.0e+1
    add_scaling: true
  test_prefix: |-
    [gMASK]<sop>
  test_seperators:
    untagged_role_sep: " "
    untagged_start_sep: " "
    tagged_start_sep: " "
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This runs forward passes on samples and stores: (1) pre-MLP activations; (2) top-k expert selections; (3) sample metadata.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.loss.loss_utils import ForCausalLMLoss\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "from termcolor import colored\n",
    "import importlib\n",
    "import random\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.store_topk import convert_topk_to_df\n",
    "from utils.store_outputs import convert_outputs_to_df_fast\n",
    "from utils.misc import flatten_list\n",
    "from utils.role_templates import render_single_message\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 123\n",
    "\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "selected_model_index = 1\n",
    "\n",
    "def get_model(index):\n",
    "    # HF model ID, model prefix, model architecture,  attn implementation, whether to use hf lib implementation\n",
    "    models = {\n",
    "        0: ('openai/gpt-oss-20b', 'gptoss20', 'gptoss', 'kernels-community/vllm-flash-attn3', True, 24),\n",
    "        1: ('openai/gpt-oss-120b', 'gptoss120', 'gptoss', 'kernels-community/vllm-flash-attn3', True, 36), # Will load experts in MXFP4 if triton kernels installed\n",
    "        2: ('Qwen/Qwen3-30B-A3B-Thinking-2507', 'qwen3-30b-a3b', 'qwen3moe', None, True, 48),\n",
    "        3: ('zai-org/GLM-4.5-Air-FP8', 'glm4moe', 'glm4moe', None, True, 45) # GLM-4.5 has one dense pre-layer, this is MoE layers\n",
    "    }\n",
    "    return models[index]\n",
    "\n",
    "def load_model_and_tokenizer(model_id, model_prefix, model_attn, model_use_hf):\n",
    "    \"\"\"\n",
    "    Load the model and tokenizer from HF, or from file if already downloaded.\n",
    "    \"\"\"\n",
    "    cache_dir = '/workspace/hf'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir = cache_dir, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, cache_dir = cache_dir, dtype = torch.bfloat16, trust_remote_code = not model_use_hf, device_map = 'auto', attn_implementation = model_attn).eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "model_id, model_prefix, model_architecture, model_attn, model_use_hf, model_n_layers = get_model(selected_model_index)\n",
    "tokenizer, model = load_model_and_tokenizer(model_id, model_prefix, model_attn, model_use_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Some checks for special models (GPT-OSS - check attn + expert precision)\n",
    "\"\"\"\n",
    "if model_architecture == 'gptoss':\n",
    "    print(model.model.layers[0].mlp.experts.down_proj)\n",
    "    print(model.model.config._attn_implementation)\n",
    "\n",
    "# Temp hotfix for qwen3 https://github.com/huggingface/accelerate/issues/3870\n",
    "\n",
    "# # Test model() call\n",
    "# inputs = tokenizer(['Test string'], return_tensors = 'pt', padding = 'max_length', truncation = True, max_length = 12).to(model.device)\n",
    "# with torch.no_grad():\n",
    "#     original_results = model(**inputs, use_cache = False)\n",
    "# print(original_results['logits'][0, :].detach().float().cpu().numpy())\n",
    "\n",
    "# # Test custom loader\n",
    "# import importlib\n",
    "# import utils.pretrained_models.ringmini2 as test_mod   # the module object\n",
    "# test_mod = importlib.reload(test_mod)\n",
    "# run_model_return_topk = test_mod.run_ringmini2_return_topk\n",
    "# custom_results = run_model_return_topk(model, inputs['input_ids'], inputs['attention_mask'], return_hidden_states = True)\n",
    "# print(custom_results['logits'][0, :].detach().float().cpu().numpy())\n",
    "\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load reverse-engineered forward pass functions that return topk expert IDs and weights\n",
    "\"\"\"\n",
    "model_module = importlib.import_module(f\"utils.pretrained_models.{model_architecture}\")\n",
    "run_model_return_topk = getattr(model_module, f\"run_{model_architecture}_return_topk\")\n",
    "\n",
    "def test_custom_forward_pass(model, pad_token_id):\n",
    "    inputs = tokenizer(['Hi! I am a dog and I like to bark', 'Vegetables are good for'], return_tensors = 'pt', padding = 'max_length', truncation = True, max_length = 512).to(model.device)\n",
    "    original_results = model(**inputs, use_cache = False)\n",
    "    custom_results = run_model_return_topk(model, inputs['input_ids'], inputs['attention_mask'], return_hidden_states = True)\n",
    "    assert torch.equal(original_results.logits, custom_results['logits']), 'Error in custom forward'\n",
    "    assert len(custom_results['all_topk_experts']) == len(custom_results['all_topk_weights']), 'Length of topk IDs and weights not equal'\n",
    "    print(f\"Length of topk: {len(custom_results['all_topk_experts'])}\")\n",
    "    print(f\"Topk size: {custom_results['all_topk_experts'][0].shape}\")\n",
    "    print(f\"First token topk IDs: {custom_results['all_topk_experts'][0][1,]}\")\n",
    "    print(f\"First token topk weights: {custom_results['all_topk_weights'][0][1,]}\")\n",
    "    loss = ForCausalLMLoss(custom_results['logits'], torch.where(inputs['input_ids'] == pad_token_id, torch.tensor(-100), inputs['input_ids']), model.config.vocab_size).detach().cpu().item()\n",
    "    print(f\"LM loss: {loss}\")\n",
    "    print(f\"Hidden states layers (pre-mlp | post-layer): {len(custom_results['all_pre_mlp_hidden_states'])} | {len(custom_results['all_hidden_states'])}\")\n",
    "    print(f\"Hidden state size (pre-mlp | post-layer): {(custom_results['all_pre_mlp_hidden_states'][0].shape)} | {(custom_results['all_hidden_states'][0].shape)}\")\n",
    "    print(f\"Router logits : {(custom_results['all_router_logits'][0].shape)}\")\n",
    "\n",
    "test_custom_forward_pass(model, tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "s = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {'role': 'developer', 'content': 'Test.'},\n",
    "        {'role': 'user', 'content': 'Hi! I am a dog and I like to bark'},\n",
    "        {'role': 'assistant', 'content': 'I am a dog and I like to bark'}\n",
    "    ],\n",
    "    tokenize = False,\n",
    "    padding = 'max_length',\n",
    "    truncation = True,\n",
    "    max_length = 512,\n",
    "    add_generation_prompt = False\n",
    ")\n",
    "print(s)\n",
    "# print(re.sub(r'(Current date:\\s*)\\d{4}-\\d{2}-\\d{2}', '2025-09-01', s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(render_single_message(model_architecture, role = 'assistant-cot',  content ='Hi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset - C4 + HPLT2\n",
    "\"\"\"\n",
    "n_sample_size = 500\n",
    "\n",
    "def load_raw_ds():\n",
    "\n",
    "    def get_c4():\n",
    "        return load_dataset('allenai/c4', 'en', split = 'validation', streaming = True).shuffle(seed = seed, buffer_size = 50_000)\n",
    "    \n",
    "    # def get_hplt2():\n",
    "    #     return load_dataset('HPLT/HPLT2.0_cleaned', 'eng_Latn', split = 'train', streaming = True).shuffle(seed = seed, buffer_size = 50_000)\n",
    "\n",
    "    def get_data(ds, n_samples, data_source): # en\n",
    "        raw_data = []\n",
    "        ds_iter = iter(ds)\n",
    "        for _ in range(n_samples):\n",
    "            sample = next(ds_iter, None)\n",
    "            if sample is None:\n",
    "                break\n",
    "            raw_data.append({'text': sample['text'], 'source': data_source})\n",
    "        return raw_data\n",
    "    \n",
    "    return get_data(get_c4(), n_sample_size, 'c4-en')    \n",
    "\n",
    "raw_data = load_raw_ds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to get sample sequences\n",
    "\"\"\"\n",
    "truncated_texts = tokenizer.batch_decode(tokenizer([t['text'] for t in raw_data], padding = False, truncation = True, max_length = 768).input_ids)\n",
    "\n",
    "def get_sample_seqs(sample_str):\n",
    "    sample_seqs = {\n",
    "        'system': ('system', sample_str, None),\n",
    "        'user': ('user', sample_str, None),\n",
    "        'assistant-cot': ('assistant-cot', sample_str, None),\n",
    "        'assistant-final': ('assistant-final', sample_str, None),\n",
    "        'tool': ('tool', sample_str, None)\n",
    "    }\n",
    "    return [\n",
    "        {'role': k, 'prompt': render_single_message(model_architecture, v[0], v[1], v[2])}\n",
    "        for k, v in sample_seqs.items()\n",
    "    ]\n",
    "\n",
    "input_list = flatten_list([\n",
    "    [\n",
    "        {'question_ix': i, 'question': x, **p}\n",
    "        for p in get_sample_seqs(x)\n",
    "    ]\n",
    "    for i, x in enumerate(truncated_texts)\n",
    "])\n",
    "\n",
    "input_df =\\\n",
    "    pd.DataFrame(input_list)\\\n",
    "    .assign(prompt_ix = lambda df: list(range(len(df))))\n",
    "\n",
    "input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Load dataset into a dataloader. The dataloader returns the original tokens - this is important for BPE tokenizers as otherwise it's difficult to reconstruct the correct string later!\n",
    "\"\"\"\n",
    "from utils.dataset import ReconstructableTextDataset, stack_collate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def chunk_list(input_list, max_length):\n",
    "    return [input_list[i:i + max_length] for i in range(0, len(input_list), max_length)]\n",
    "\n",
    "# Create and chunk into lists of size 500 each - these will be the export breaks\n",
    "test_dls = [\n",
    "    DataLoader(\n",
    "        ReconstructableTextDataset([x['prompt'] for x in data_chunk], tokenizer, max_length = 896, prompt_ix = [x['prompt_ix'] for x in data_chunk]),\n",
    "        batch_size = 16,\n",
    "        shuffle = False,\n",
    "        collate_fn = stack_collate\n",
    "    )\n",
    "    for data_chunk in tqdm(chunk_list(input_df.to_dict('records'), 500))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get expert selections + export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Run forward passes + export data\n",
    "\"\"\"\n",
    "@torch.no_grad()\n",
    "def run_and_export_topk(model, model_prefix: str, dls: list[ReconstructableTextDataset], layers_to_keep_acts: list[int], max_batches: None | int = None):\n",
    "    \"\"\"\n",
    "    Run forward passes on given model and store the intermediate hidden layers as well as topks\n",
    "\n",
    "    Params:\n",
    "        @model: The model to run forward passes on. Should return a dict with keys `logits`, `all_topk_experts`, `all_topk_weights`,\n",
    "          `all_pre_mlp_hidden_states`, and `all_expert_outputs`.\n",
    "        @model_prefix: The model prefix to save the activations under.\n",
    "        @dls: A list of dataloaders, each a ReconstructableTextDataset of which returns `input_ids`, `attention_mask`, and `original_tokens`.\n",
    "        @layers_to_keep_acts: A list of layer indices (0-indexed) for which to filter `all_pre_mlp_hidden_states` (see returned object description).\n",
    "        @max_batches: The max number of batches to run.\n",
    "\n",
    "    Returns:\n",
    "        A dict with keys:\n",
    "        - `sample_df`: A sample (token)-level dataframe with corresponding input token ID, output token ID, and input token text (removes masked tokens)\n",
    "        - `topk_df`: A sample (token) x layer_ix x topk_ix level dataframe that gives the expert ID selected at each sample-layer-topk (removes masked_tokens)\n",
    "        - `all_router_logits`: A tensor of size n_samples x layers_to_keep_acts x n_experts with the pre-softmax router logits.\n",
    "        - `all_pre_mlp_hidden_states`: A tensor of size n_samples x layers_to_keep_acts x D return the hidden state for each retained layers. Each \n",
    "            n_sample corresponds to a row of sample_df.\n",
    "    \"\"\"\n",
    "    cross_dl_batch_ix = 0\n",
    "    output_dir = f'activations/{model_prefix}'\n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "    \n",
    "    # Save metadata\n",
    "    with open(f'{output_dir}/metadata.pkl', 'wb') as f:\n",
    "        pickle.dump(\n",
    "            {'all_pre_mlp_hidden_states_layers': layers_to_keep_acts},\n",
    "            f\n",
    "        )\n",
    "\n",
    "    # Iterate through dataloaders\n",
    "    for dl_ix, dl in enumerate(dls):\n",
    "        print(f\"Processing {str(dl_ix)} of {len(dls)}...\")   \n",
    "        dl_dir = f\"{output_dir}/{dl_ix:02d}\"\n",
    "        os.makedirs(dl_dir, exist_ok = True)\n",
    "\n",
    "        all_router_logits = []\n",
    "        all_pre_mlp_hidden_states = []\n",
    "        sample_dfs = []\n",
    "        topk_dfs = []\n",
    "\n",
    "        for _, batch in tqdm(enumerate(dl), total = len(dl)):\n",
    "\n",
    "            input_ids = batch['input_ids'].to(main_device)\n",
    "            attention_mask = batch['attention_mask'].to(main_device)\n",
    "            original_tokens = batch['original_tokens']\n",
    "            prompt_indices = batch['prompt_ix']\n",
    "\n",
    "            output = run_model_return_topk(model, input_ids, attention_mask, return_hidden_states = True)\n",
    "\n",
    "            # Check no bugs by validating output/perplexity\n",
    "            if cross_dl_batch_ix == 0:\n",
    "                loss = ForCausalLMLoss(output['logits'], torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids), model.config.vocab_size).detach().cpu().item()\n",
    "                for i in range(min(20, input_ids.size(0))):\n",
    "                    decoded_input = tokenizer.decode(input_ids[i, :], skip_special_tokens = False)\n",
    "                    next_token_id = torch.argmax(output['logits'][i, -1, :]).item()\n",
    "                    print('---------\\n' + decoded_input + colored(tokenizer.decode([next_token_id], skip_special_tokens = False).replace('\\n', '<lb>'), 'green'))\n",
    "                print(f\"PPL:\", torch.exp(torch.tensor(loss)).item())\n",
    "            \n",
    "            original_tokens_df = pd.DataFrame(\n",
    "                [(seq_i, tok_i, tok) for seq_i, tokens in enumerate(original_tokens) for tok_i, tok in enumerate(tokens)], \n",
    "                columns = ['sequence_ix', 'token_ix', 'token']\n",
    "            )\n",
    "                    \n",
    "            prompt_indices_df = pd.DataFrame(\n",
    "                [(seq_i, seq_source) for seq_i, seq_source in enumerate(prompt_indices)], \n",
    "                columns = ['sequence_ix', 'prompt_ix']\n",
    "            )\n",
    "\n",
    "            # Create sample (token) level dataframe\n",
    "            sample_df =\\\n",
    "                convert_outputs_to_df_fast(input_ids, attention_mask, output['logits'])\\\n",
    "                .merge(original_tokens_df, how = 'left', on = ['token_ix', 'sequence_ix'])\\\n",
    "                .merge(prompt_indices_df, how = 'left', on = ['sequence_ix'])\\\n",
    "                .assign(batch_ix = cross_dl_batch_ix)\n",
    "\n",
    "            # Create topk x layer_ix x sample level dataframe\n",
    "            topk_df =\\\n",
    "                convert_topk_to_df(input_ids, attention_mask, output['all_topk_experts'], output['all_topk_weights'])\\\n",
    "                .assign(batch_ix = cross_dl_batch_ix, weight = lambda df: df['weight'])\\\n",
    "                .drop(columns = 'token_id')\n",
    "            \n",
    "            sample_dfs.append(sample_df)\n",
    "            topk_dfs.append(topk_df)\n",
    "\n",
    "            # Store pre-MLP hidden states - the fwd pass as n_layers list as BN x D, collapse to BN x n_layers x D, with BN filtering out masked items\n",
    "            valid_pos = torch.where(attention_mask.cpu().view(-1) == 1) # Valid (BN, ) positions\n",
    "            all_router_logits.append(torch.stack(output['all_router_logits'], dim = 1)[valid_pos][:, layers_to_keep_acts, :])\n",
    "            all_pre_mlp_hidden_states.append(torch.stack(output['all_pre_mlp_hidden_states'], dim = 1)[valid_pos][:, layers_to_keep_acts, :])\n",
    "\n",
    "            cross_dl_batch_ix += 1\n",
    "            if max_batches is not None and cross_dl_batch_ix >= max_batches:\n",
    "                pd.concat(sample_dfs, ignore_index = True).to_pickle(f'{dl_dir}/samples.pkl')\n",
    "                pd.concat(topk_dfs, ignore_index = True).to_pickle(f'{dl_dir}/topks.pkl')\n",
    "                torch.save(torch.cat(all_router_logits, dim = 0), f'{dl_dir}/all-router-logits.pt')\n",
    "                torch.save(torch.cat(all_pre_mlp_hidden_states, dim = 0), f'{dl_dir}/all-pre-mlp-hidden-states.pt')\n",
    "                return True\n",
    "\n",
    "        pd.concat(sample_dfs, ignore_index = True).to_pickle(f'{dl_dir}/samples.pkl')\n",
    "        pd.concat(topk_dfs, ignore_index = True).to_pickle(f'{dl_dir}/topks.pkl')\n",
    "        torch.save(torch.cat(all_router_logits, dim = 0), f'{dl_dir}/all-router-logits.pt')\n",
    "        torch.save(torch.cat(all_pre_mlp_hidden_states, dim = 0), f'{dl_dir}/all-pre-mlp-hidden-states.pt')\n",
    "\n",
    "    return True\n",
    "\n",
    "res = run_and_export_topk(\n",
    "    model,\n",
    "    model_prefix,\n",
    "    test_dls,\n",
    "    layers_to_keep_acts = list(range(0, model_n_layers, 2)),\n",
    "    max_batches = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input mappings to questions\n",
    "\"\"\"\n",
    "input_df.to_csv(f'./activations/{model_prefix}/input_mappings.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

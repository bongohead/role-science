{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This runs forward passes on samples and stores: (1) pre-MLP activations; (2) top-k expert selections; (3) sample metadata.\"\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Qwen3VLMoeForConditionalGeneration\n",
    "from transformers.loss.loss_utils import ForCausalLMLoss\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "from termcolor import colored\n",
    "import importlib\n",
    "import cupy\n",
    "import cuml\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.store_outputs import convert_outputs_to_df_fast\n",
    "from utils.misc import flatten_list\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 123\n",
    "\n",
    "ws = '/workspace/deliberative-alignment-jailbreaks'\n",
    "\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "selected_model_index = 4\n",
    "\n",
    "def get_model(index):\n",
    "    # HF model ID, model prefix, model architecture,  attn implementation, whether to use hf lib implementation\n",
    "    models = {\n",
    "        0: ('openai/gpt-oss-20b', 'gptoss20', 'gptoss', 'kernels-community/vllm-flash-attn3', True, 24),\n",
    "        1: ('openai/gpt-oss-120b', 'gptoss120', 'gptoss', 'kernels-community/vllm-flash-attn3', True, 36), # Will load experts in MXFP4 if triton kernels installed\n",
    "        2: ('Qwen/Qwen3-VL-30B-A3B-Thinking', 'qwen3vl30ba3b', 'qwen3vlmoe', None, True, 48),\n",
    "        3: ('zai-org/GLM-4.5-Air-FP8', 'glm45air', 'glm4moe', None, True, 45), # GLM-4.5 has one dense pre-layer, this is MoE layers\n",
    "        4: ('allenai/Olmo-3-7B-Think', 'olmo3-7', 'olmo3', None, True, 32), # OlMo-3\n",
    "        5: ('allenai/Olmo-3-32B-Think', 'olmo3-32', 'olmo3', None, True, 64) # OlMo-3\n",
    "    }\n",
    "    return models[index]\n",
    "\n",
    "def load_model_and_tokenizer(model_id, model_prefix, model_attn, model_use_hf):\n",
    "    \"\"\"\n",
    "    Load the model and tokenizer from HF, or from file if already downloaded.\n",
    "    \"\"\"\n",
    "    cache_dir = '/workspace/hf'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir = cache_dir, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "        \n",
    "    if model_architecture == 'qwen3vlmoe':\n",
    "        model = Qwen3VLMoeForConditionalGeneration.from_pretrained(model_id, cache_dir = cache_dir, dtype = 'auto', trust_remote_code = not model_use_hf, device_map = None, attn_implementation = model_attn).to(main_device).eval()\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, cache_dir = cache_dir, dtype = 'auto', trust_remote_code = not model_use_hf, device_map = None, attn_implementation = model_attn).to(main_device).eval()\n",
    "\n",
    "    return tokenizer, model    \n",
    "\n",
    "model_id, model_prefix, model_architecture, model_attn, model_use_hf, model_n_layers = get_model(selected_model_index)\n",
    "tokenizer, model = load_model_and_tokenizer(model_id, model_prefix, model_attn, model_use_hf)\n",
    "\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Some checks for special models (GPT-OSS - check attn + expert precision)\n",
    "\"\"\"\n",
    "if model_architecture == 'gptoss':\n",
    "    print(model.model.layers[0].mlp.experts.down_proj)\n",
    "    print(model.model.config._attn_implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load reverse-engineered forward pass functions that return topk expert IDs and weights\n",
    "\"\"\"\n",
    "model_module = importlib.import_module(f\"utils.pretrained_models.{model_architecture}\")\n",
    "run_model_return_topk = getattr(model_module, f\"run_{model_architecture}_return_topk\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_custom_forward_pass(model, pad_token_id):\n",
    "    inputs = tokenizer(['Hi! I am a dog and I like to bark', 'Vegetables are good for'], return_tensors = 'pt', padding = 'max_length', truncation = True, max_length = 640).to(model.device)\n",
    "    original_results = model(**inputs, use_cache = False)\n",
    "    custom_results = run_model_return_topk(model, inputs['input_ids'], inputs['attention_mask'], return_hidden_states = True)\n",
    "    assert torch.equal(original_results.logits, custom_results['logits']), 'Error in custom forward'\n",
    "    loss = ForCausalLMLoss(custom_results['logits'], torch.where(inputs['input_ids'] == pad_token_id, torch.tensor(-100), inputs['input_ids']), custom_results['logits'].size(-1)).detach().cpu().item()\n",
    "    print(f\"LM loss: {loss}\")\n",
    "    print(f\"Hidden states layers (pre-mlp | post-layer): {len(custom_results['all_pre_mlp_hidden_states'])} | {len(custom_results['all_hidden_states'])}\")\n",
    "    print(f\"Hidden state size (pre-mlp | post-layer): {(custom_results['all_pre_mlp_hidden_states'][0].shape)} | {(custom_results['all_hidden_states'][0].shape)}\")\n",
    "\n",
    "test_custom_forward_pass(model, tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run some IFT tests\n",
    "\"\"\"\n",
    "import importlib\n",
    "import utils.role_templates\n",
    "importlib.reload(utils.role_templates)\n",
    "from utils.role_templates import render_single_message\n",
    "from utils.role_templates import render_mixed_cot\n",
    "\n",
    "# Test\n",
    "s = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {'role': 'user', 'content': 'Hi! I am a dog and I like to bark'},\n",
    "        {'role': 'assistant', 'content': 'I am a dog and I like to bark'}\n",
    "    ],\n",
    "    tokenize = False,\n",
    "    padding = 'max_length',\n",
    "    truncation = True,\n",
    "    max_length = 512,\n",
    "    add_generation_prompt = False\n",
    ")\n",
    "print(s)\n",
    "\n",
    "print(render_single_message(model_architecture, role = 'assistant-final', content ='Hi'))\n",
    "print(render_mixed_cot(model_architecture, 'The user...', 'Yes!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset - C4 + HPLT2\n",
    "\"\"\"\n",
    "n_sample_size = 400\n",
    "\n",
    "def load_raw_ds():\n",
    "\n",
    "    def get_c4():\n",
    "        return load_dataset('allenai/c4', 'en', split = 'validation', streaming = True).shuffle(seed = seed, buffer_size = 50_000)\n",
    "    \n",
    "    def get_hplt2():\n",
    "        return load_dataset('HPLT/HPLT2.0_cleaned', 'eng_Latn', split = 'train', streaming = True).shuffle(seed = seed, buffer_size = 50_000)\n",
    "\n",
    "    def get_data(ds, n_samples, data_source): # en\n",
    "        raw_data = []\n",
    "        ds_iter = iter(ds)\n",
    "        for _ in range(n_samples):\n",
    "            sample = next(ds_iter, None)\n",
    "            if sample is None:\n",
    "                break\n",
    "            raw_data.append({'text': sample['text'], 'source': data_source})\n",
    "        return raw_data\n",
    "    \n",
    "    return get_data(get_c4(), n_sample_size, 'c4-en')    \n",
    "    # return get_data(get_hplt2(), n_sample_size, 'hplt-en')    \n",
    "\n",
    "raw_data = load_raw_ds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define opening text to append at the start of each role\n",
    "\"\"\"\n",
    "import textwrap\n",
    "\n",
    "if model_architecture == 'gptoss':\n",
    "    prepend =\\\n",
    "        tokenizer.bos_token +\\\n",
    "        render_single_message(\n",
    "            model_architecture,\n",
    "            'system', \n",
    "            textwrap.dedent(\"\"\"\n",
    "                <|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
    "                Knowledge cutoff: 2024-06\n",
    "                Current date: 2026-01-01\n",
    "\n",
    "                Reasoning: medium\n",
    "\n",
    "                # Valid channels: analysis, commentary, final. Channel must be included for every message.\n",
    "                Calls to these tools must go to the commentary channel: 'functions'.<|end|>\n",
    "            \"\"\").strip()\n",
    "        )\n",
    "\n",
    "elif model_architecture == 'olmo3':\n",
    "    prepend =\\\n",
    "        tokenizer.bos_token +\\\n",
    "        render_single_message(\n",
    "            model_architecture,\n",
    "            'system', \n",
    "            textwrap.dedent(\n",
    "            \"\"\"\n",
    "                You are OLMo, a helpful function-calling AI assistant built by Ai2. Your date cutoff is November 2024, and your model weights are available at https://huggingface.co/allenai. You do not currently have access to any functions. <functions></functions>\n",
    "            \"\"\").strip()\n",
    "        )\n",
    "\n",
    "else:\n",
    "    raise ValueError('Missing model architecture')\n",
    "    \n",
    "print(prepend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Create sample sequences\n",
    "# \"\"\"\n",
    "# truncated_texts = tokenizer.batch_decode(tokenizer([t['text'] for t in raw_data], padding = False, truncation = True, max_length = 512).input_ids)\n",
    "\n",
    "# def get_sample_seqs(sample_str):\n",
    "#     sample_seqs = {\n",
    "#         'system': ('system', sample_str, None),\n",
    "#         'user': ('user', sample_str, None),\n",
    "#         'cot': ('assistant-cot', sample_str, None),\n",
    "#         'assistant': ('assistant-final', sample_str, None),\n",
    "#         'tool': ('tool', sample_str, None)\n",
    "#     }\n",
    "#     return [\n",
    "#         {'role': k, 'prompt': prepend + render_single_message(model_architecture, v[0], v[1], v[2])}\n",
    "#         for k, v in sample_seqs.items()\n",
    "#     ]\n",
    "\n",
    "# input_list = flatten_list([\n",
    "#     [\n",
    "#         {'question_ix': i, 'question': x, **p}\n",
    "#         for p in get_sample_seqs(x)\n",
    "#     ]\n",
    "#     for i, x in enumerate(truncated_texts)\n",
    "# ])\n",
    "\n",
    "# input_df =\\\n",
    "#     pd.DataFrame(input_list)\\\n",
    "#     .assign(prompt_ix = lambda df: list(range(len(df))))\n",
    "\n",
    "# display(input_df)\n",
    "\n",
    "# # Print examples\n",
    "# for p in [row['prompt'] for row in input_df.pipe(lambda df: df[df['question_ix'] == 1]).to_dict('records')]:\n",
    "#     print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create sample sequences - merged CoT strategy\n",
    "\"\"\"\n",
    "truncated_texts = tokenizer.batch_decode(tokenizer([t['text'] for t in raw_data], padding = False, truncation = True, max_length = 512).input_ids)\n",
    "n_seqs = len(truncated_texts)\n",
    "\n",
    "def get_sample_seqs(probe_text, partner_text):\n",
    "    \"\"\"\n",
    "    Params\n",
    "        @probe_text: The text we're extracting states from (appears in all roles)\n",
    "        @partner_text: Random paired text (only used in merged sample's assistant position)\n",
    "    \"\"\"\n",
    "    seqs = []\n",
    "\n",
    "    for role in ['system', 'user', 'tool']:\n",
    "        seqs.append({\n",
    "            'role': role,\n",
    "            'prompt': prepend + render_single_message(model_architecture, role = role, content = probe_text)\n",
    "        })\n",
    "\n",
    "    # Merged assistant â€“ CoT\n",
    "    seqs.append({\n",
    "        'role': 'assistant_cot_merged',\n",
    "        'prompt': prepend + render_mixed_cot(model_architecture, cot = probe_text, assistant = partner_text)\n",
    "    })\n",
    "\n",
    "    return seqs\n",
    "\n",
    "perm = np.random.permutation(n_seqs)\n",
    "while np.any(perm == np.arange(n_seqs)):\n",
    "    perm = np.random.permutation(n_seqs)\n",
    "\n",
    "input_list = []\n",
    "for base_ix, base_text in enumerate(truncated_texts):\n",
    "    partner_ix = int(perm[base_ix])\n",
    "    partner_text = truncated_texts[partner_ix]\n",
    "    for seq in get_sample_seqs(base_text, partner_text):\n",
    "        row = {\n",
    "            'question_ix': base_ix,\n",
    "            'question': base_text,\n",
    "            'partner_ix': partner_ix,\n",
    "            'partner_text': partner_text,\n",
    "            **seq\n",
    "        }\n",
    "        input_list.append(row)\n",
    "\n",
    "input_df = pd.DataFrame(input_list).assign(prompt_ix = lambda df: list(range(len(df))))\n",
    "\n",
    "display(input_df)\n",
    "\n",
    "# Print examples for a particular base text index\n",
    "for p in input_df.query(\"question_ix == 1\")['prompt']:\n",
    "    print(p)\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Load dataset into a dataloader. The dataloader returns the original tokens - this is important for BPE tokenizers as otherwise it's difficult to reconstruct the correct string later!\n",
    "\"\"\"\n",
    "from utils.dataset import ReconstructableTextDataset, stack_collate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def chunk_list(input_list, max_length):\n",
    "    return [input_list[i:i + max_length] for i in range(0, len(input_list), max_length)]\n",
    "\n",
    "# Create and chunk into lists of size 800 each - these will be the export breaks\n",
    "test_dl = DataLoader(\n",
    "    ReconstructableTextDataset(input_df['prompt'].tolist(), tokenizer, max_length = 512 * 2 + 64, prompt_ix = input_df['prompt_ix'].tolist()),\n",
    "    batch_size = 16,\n",
    "    shuffle = False,\n",
    "    collate_fn = stack_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run forward passes\n",
    "\"\"\"\n",
    "@torch.no_grad()\n",
    "def run_and_export_states(model, dl: ReconstructableTextDataset, layers_to_keep_acts: list[int]):\n",
    "    \"\"\"\n",
    "    Run forward passes on given model and store the decomposed sample_df plus hidden states\n",
    "\n",
    "    Params:\n",
    "        @model: The model to run forward passes on. Should return a dict with keys `logits` and `all_pre_mlp_hidden_states`.\n",
    "        @dl: A ReconstructableTextDataset of which returns `input_ids`, `attention_mask`, and `original_tokens`.\n",
    "        @layers_to_keep_acts: A list of layer indices (0-indexed) for which to filter `all_pre_mlp_hidden_states` (see returned object description).\n",
    "\n",
    "    Returns:\n",
    "        A dict with keys:\n",
    "        - `sample_df`: A sample (token)-level dataframe with corresponding input token ID, output token ID, and input token text (removes masked tokens)\n",
    "        - `all_pre_mlp_hidden_states`: A tensor of size n_samples x layers_to_keep_acts x D return the hidden state for each retained layers. Each \n",
    "            n_sample corresponds to a row of sample_df.\n",
    "\n",
    "    Example:\n",
    "        test_outputs = run_and_export_states(model, test_dl, layers_to_keep_acts = list(range(model_n_layers)))\n",
    "    \"\"\"\n",
    "    all_pre_mlp_hidden_states = []\n",
    "    sample_dfs = []\n",
    "\n",
    "    for batch_ix, batch in tqdm(enumerate(dl), total = len(dl)):\n",
    "\n",
    "        input_ids = batch['input_ids'].to(main_device)\n",
    "        attention_mask = batch['attention_mask'].to(main_device)\n",
    "        original_tokens = batch['original_tokens']\n",
    "        prompt_indices = batch['prompt_ix']\n",
    "\n",
    "        output = run_model_return_topk(model, input_ids, attention_mask, return_hidden_states = True)\n",
    "\n",
    "        # Check no bugs by validating output/perplexity\n",
    "        if batch_ix == 0:\n",
    "            loss = ForCausalLMLoss(output['logits'], torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids), output['logits'].size(-1)).detach().cpu().item()\n",
    "            for i in range(min(20, input_ids.size(0))):\n",
    "                decoded_input = tokenizer.decode(input_ids[i, :], skip_special_tokens = False)\n",
    "                next_token_id = torch.argmax(output['logits'][i, -1, :]).item()\n",
    "                print('---------\\n' + decoded_input + colored(tokenizer.decode([next_token_id], skip_special_tokens = False).replace('\\n', '<lb>'), 'green'))\n",
    "            print(f\"PPL:\", torch.exp(torch.tensor(loss)).item())\n",
    "                \n",
    "        original_tokens_df = pd.DataFrame(\n",
    "            [(seq_i, tok_i, tok) for seq_i, tokens in enumerate(original_tokens) for tok_i, tok in enumerate(tokens)], \n",
    "            columns = ['sequence_ix', 'token_ix', 'token']\n",
    "        )\n",
    "                \n",
    "        prompt_indices_df = pd.DataFrame(\n",
    "            [(seq_i, seq_source) for seq_i, seq_source in enumerate(prompt_indices)], \n",
    "            columns = ['sequence_ix', 'prompt_ix']\n",
    "        )\n",
    "        \n",
    "        # Create sample (token) level dataframe\n",
    "        sample_df =\\\n",
    "            convert_outputs_to_df_fast(input_ids, attention_mask, output['logits'])\\\n",
    "            .merge(original_tokens_df, how = 'left', on = ['token_ix', 'sequence_ix'])\\\n",
    "            .merge(prompt_indices_df, how = 'left', on = ['sequence_ix'])\\\n",
    "            .assign(batch_ix = batch_ix)\n",
    "        \n",
    "        sample_dfs.append(sample_df)\n",
    "\n",
    "        # Store pre-MLP hidden states - the fwd pass as n_layers list as BN x D, collapse to BN x n_layers x D, with BN filtering out masked items\n",
    "        valid_pos = torch.where(attention_mask.cpu().view(-1) == 1) # Valid (BN, ) positions\n",
    "        all_pre_mlp_hidden_states.append(torch.stack(output['all_pre_mlp_hidden_states'], dim = 1)[valid_pos][:, layers_to_keep_acts, :])\n",
    "\n",
    "        batch_ix += 1\n",
    "\n",
    "    sample_df = pd.concat(sample_dfs, ignore_index = True).drop(columns = ['batch_ix', 'sequence_ix']) # Drop batch/seq_ix, since prompt_ix identifies\n",
    "    all_pre_mlp_hidden_states = torch.cat(all_pre_mlp_hidden_states, dim = 0)\n",
    "\n",
    "    return {\n",
    "        'sample_df': sample_df,\n",
    "        'all_pre_mlp_hs': all_pre_mlp_hidden_states\n",
    "    }\n",
    "\n",
    "act_map = list(range(0, model_n_layers, 2 if model_n_layers <= 24 else 4))\n",
    "res = run_and_export_states(\n",
    "    model,\n",
    "    test_dl,\n",
    "    layers_to_keep_acts = act_map\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run probes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Label roles and omit prepend\n",
    "\"\"\"\n",
    "import utils.role_assignments\n",
    "importlib.reload(utils.role_assignments)\n",
    "from utils.role_assignments import label_content_roles\n",
    "from utils.substring_assignments import flag_message_types\n",
    "\n",
    "# First, let's flag all roles\n",
    "sample_df =\\\n",
    "    label_content_roles(model_architecture, res['sample_df'])\\\n",
    "    .assign(token_in_seq_ix = lambda df: df.groupby(['prompt_ix']).cumcount())\\\n",
    "    .assign(token_in_role_ix = lambda df: df.groupby(['prompt_ix', 'role']).cumcount())\\\n",
    "    .assign(sample_ix = lambda df: range(0, len(df)))\n",
    "\n",
    "# Now, let's flag the prepend text.\n",
    "sample_df =\\\n",
    "    flag_message_types(sample_df, [prepend])\\\n",
    "    .drop(columns = 'base_message')\n",
    "\n",
    "# Drop prepend text + non-content tags\n",
    "valid_sample_df =\\\n",
    "    sample_df\\\n",
    "    .pipe(lambda df: df[(df['base_message_ix'].isna()) & (df['in_content_span'] == True) & (df['role'].notna())])\\\n",
    "\n",
    "# Verify role counts are accurate (should be exactly equal for most models, slight discrepancy for Olmo3 since <think> isn't a single tok)\n",
    "display(\n",
    "    valid_sample_df\\\n",
    "    .groupby('role', as_index = False)\\\n",
    "    .agg(count = ('sample_ix', 'count'))\n",
    ")\n",
    "\n",
    "# Validate roles are flagged correctly\n",
    "valid_sample_df\\\n",
    "    .pipe(lambda df: df[df['prompt_ix'] <= 14])\\\n",
    "    .groupby(['prompt_ix', 'role', 'seg_id'], as_index = False)\\\n",
    "    .agg(combined_text = ('token', ''.join))\\\n",
    "    .assign(eot = lambda df: df['combined_text'].str[-30:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_df.pipe(lambda df: df[df['prompt_ix'] == 3]).to_csv('dump.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert activations to fp16 (for compatibility with cupy later) + layer-wise dict\n",
    "\"\"\"\n",
    "all_pre_mlp_hs = res['all_pre_mlp_hs'].to(torch.float16)\n",
    "# compare_bf16_fp16_batched(all_pre_mlp_hs_import, all_pre_mlp_hs)\n",
    "# del all_pre_mlp_hs_import\n",
    "all_pre_mlp_hs = {layer_ix: all_pre_mlp_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(act_map)}\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run logistic regression probes to get role space models\n",
    "\"\"\"\n",
    "def fit_lr(x_train, y_train, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Fit a probe with a standard 80/20 split\n",
    "    \"\"\"\n",
    "    lr_model = cuml.linear_model.LogisticRegression(penalty = 'l2', C = 10.0, max_iter = 5000, fit_intercept = True) # 1e-2 or 1e-3 reg\n",
    "    lr_model.fit(x_train, y_train)\n",
    "    accuracy = lr_model.score(x_test, y_test)\n",
    "    y_test_pred = lr_model.predict(x_test)\n",
    "    return lr_model, accuracy, y_test_pred\n",
    "\n",
    "def get_probe_result(layer_ix, label2id):\n",
    "    \"\"\"\n",
    "    Get probe results for a single layer and label combination\n",
    "\n",
    "    Params:\n",
    "        @layer_ix: The layer index to train the probe on\n",
    "        @label2id: The label-to-id mapping\n",
    "    \n",
    "    Description:\n",
    "        Trains only on content space for given roles\n",
    "    \"\"\"\n",
    "    # id2label = {v: k for k, v in label2id.items()}\n",
    "    roles = list(label2id.keys())\n",
    "\n",
    "    # Get valid samples\n",
    "    probe_sample_df = valid_sample_df[valid_sample_df['role'].isin(roles)]\n",
    "\n",
    "    # 80:20 prompt split\n",
    "    prompt_ix_train, prompt_ix_test = cuml.train_test_split(probe_sample_df['prompt_ix'].unique(), test_size = 0.2, random_state = seed)\n",
    "    train_df = probe_sample_df[probe_sample_df['prompt_ix'].isin(prompt_ix_train)]\n",
    "    test_df = probe_sample_df[probe_sample_df['prompt_ix'].isin(prompt_ix_test)]\n",
    "\n",
    "    # Get y labels\n",
    "    role_labels_train_cp = cupy.asarray([label2id[r] for r in train_df['role']])\n",
    "    role_labels_test_cp = cupy.asarray([label2id[r] for r in test_df['role']])\n",
    "\n",
    "    # Get x labels\n",
    "    x_train_cp = cupy.asarray(all_pre_mlp_hs[layer_ix][train_df['sample_ix'].tolist(), :].to(torch.float32).detach().cpu())\n",
    "    x_test_cp = cupy.asarray(all_pre_mlp_hs[layer_ix][test_df['sample_ix'].tolist(), :].to(torch.float32).detach().cpu())\n",
    "    \n",
    "    uniq_train = np.unique(role_labels_train_cp.get())\n",
    "    if len(uniq_train) < len(label2id):\n",
    "        raise Exception(f\"Skipping layer {layer_ix}, mapping {label2id}: missing roles in train\", uniq_train)\n",
    "\n",
    "    lr_model, test_acc, y_test_pred = fit_lr(x_train_cp, role_labels_train_cp, x_test_cp, role_labels_test_cp)\n",
    "\n",
    "    print(f\"Layer [{layer_ix}] with roles [{'+'.join(roles)}]: {test_acc:.2f}\")\n",
    "\n",
    "    # Optional: Return classification metrics\n",
    "    results_df =\\\n",
    "        test_df\\\n",
    "        .assign(pred = y_test_pred.tolist())\\\n",
    "        .assign(pred = lambda df: df['pred'].map({v: k for k, v in label2id.items()}))\\\n",
    "        .assign(is_acc = lambda df: df['role'] == df['pred'])\n",
    "\n",
    "    acc_by_role =\\\n",
    "        results_df\\\n",
    "        .groupby(['role', 'pred'], as_index = False)\\\n",
    "        .agg(count = ('sample_ix', 'count'))\n",
    "\n",
    "    acc_by_pos =\\\n",
    "        results_df\\\n",
    "        .groupby('token_in_role_ix', as_index = False)\\\n",
    "        .agg(count = ('sample_ix', 'count'), acc = ('is_acc', 'mean'))\n",
    "\n",
    "    return {\n",
    "        'layer_ix': layer_ix, 'label2id': label2id,\n",
    "        'roles': roles, 'probe': lr_model, 'accuracy': test_acc,\n",
    "        'acc_by_role': acc_by_role, 'acc_by_pos': acc_by_pos\n",
    "    }\n",
    "\n",
    "layers_to_test = list(all_pre_mlp_hs.keys()) # [i for i in list(all_pre_mlp_hs.keys()) if i % 4 == 0]\n",
    "mappings_to_test = [\n",
    "    {'user': 0, 'assistant-cot': 1, 'assistant-final': 2},\n",
    "    # {'system': 0, 'user': 1, 'assistant-cot': 2, 'assistant-final': 3},\n",
    "    # {'system': 0, 'user': 1, 'assistant-cot': 2, 'assistant-final': 3, 'tool': 4},\n",
    "    # {'user': 0, 'assistant-cot': 1, 'assistant-final': 2, 'tool': 3}\n",
    "]\n",
    "\n",
    "all_probes = []\n",
    "for layer_ix in tqdm(layers_to_test):\n",
    "    for mapping in mappings_to_test:\n",
    "        all_probes.append(get_probe_result(layer_ix, mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check accuracy\n",
    "\"\"\"\n",
    "display(\n",
    "    pd.DataFrame(all_probes)[['layer_ix', 'roles', 'accuracy']]\\\n",
    "        .assign(accuracy = lambda df: df['accuracy'].round(2), roles = lambda df: df['roles'].apply(lambda x: ','.join(([r[0] for r in x]))))\\\n",
    "        .pivot(index='layer_ix', columns='roles', values='accuracy')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalization test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix role names\n",
    "role_map = {'assistant-cot': 'cot', 'assistant-final': 'assistant'}\n",
    "for probe in all_probes:\n",
    "    probe['roles'] = tuple(role_map.get(r, r) for r in probe['roles'])\n",
    "\n",
    "probe_layers = sorted(set(x['layer_ix'] for x in all_probes))\n",
    "probe_roles = sorted(set(tuple(x['roles']) for x in all_probes))\n",
    "\n",
    "print(f\"Num probes: {str(len(all_probes))}\")\n",
    "print(f\"Probe layers:\\n  {', '.join([str(x) for x in probe_layers])}\")\n",
    "print(f\"Probe roles:\\n {'\\n '.join([str(list(x)) for x in probe_roles])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Validate IFT on the full chat template, which tokenizes the whole thing e2e. This should match the format from original messages.:\n",
    "- `load_chat_template`: Overwrites the existing chat template in the tokenizer with one that better handles discrepancies.\n",
    "    See docstring for the function for details. \n",
    "- `label_content_roles`: Takes an instruct-formatted text, then assigns each token to the correct roles.\n",
    "\"\"\"\n",
    "import importlib\n",
    "import utils.role_assignments\n",
    "import utils.role_templates\n",
    "importlib.reload(utils.role_assignments)\n",
    "label_content_roles = utils.role_assignments.label_content_roles\n",
    "importlib.reload(utils.role_templates)\n",
    "load_chat_template = utils.role_templates.load_chat_template\n",
    "\n",
    "# Load custom chat templater\n",
    "old_chat_template = tokenizer.chat_template\n",
    "new_chat_template = load_chat_template(f'{ws}/utils/chat_templates', model_architecture)\n",
    "\n",
    "def test_chat_template(tokenizer):\n",
    "    s = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {'role': 'system', 'content': 'Test.'},\n",
    "            {'role': 'user', 'content': 'Hi! I\\'m a dog.'},\n",
    "            {'role': 'assistant', 'content': '<think>The user is a dog!</think>Congrats!'},\n",
    "            {'role': 'user', 'content': 'Thanks!'},\n",
    "            {'role': 'assistaant', 'content': '<think>Hmm, the user said thanks!</think>Anything else?'}\n",
    "        ],\n",
    "        tokenize = False,\n",
    "        padding = 'max_length',\n",
    "        truncation = True,\n",
    "        max_length = 512,\n",
    "        add_generation_prompt = False\n",
    "    )\n",
    "    print(s)\n",
    "    return s\n",
    "\n",
    "tokenizer.chat_template = old_chat_template\n",
    "s = test_chat_template(tokenizer)\n",
    "tokenizer.chat_template = new_chat_template\n",
    "s = test_chat_template(tokenizer)\n",
    "\n",
    "z =\\\n",
    "    pd.DataFrame({'input_ids': tokenizer(s)['input_ids']})\\\n",
    "    .assign(token = lambda df: tokenizer.convert_ids_to_tokens(df['input_ids']))\\\n",
    "    .assign(prompt_ix = 0, batch_ix = 0, sequence_ix = 0, token_ix = lambda df: df.index)\n",
    "\n",
    "label_content_roles(model_architecture, z).tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import conversations\n",
    "\"\"\"\n",
    "# filename = model_id.split('/')[-1].lower()\n",
    "if model_architecture == 'gptoss':\n",
    "    user_queries_df = pd.read_csv(f'{ws}/experiments/role-analysis/convs/gpt-oss-20b.csv')\n",
    "elif model_architecture == 'olmo3':\n",
    "    user_queries_df = pd.read_csv(f'{ws}/experiments/role-analysis/convs/olmo-3-7b-think.csv')\n",
    "else:\n",
    "    raise Exception('Unsupported model')\n",
    "\n",
    "print(f\"Starting length: {len(user_queries_df['conv_id'].unique())} convs\")\n",
    "\n",
    "# 1. Remove convs with any missing responses\n",
    "convs_with_missing = user_queries_df[user_queries_df[['user_query', 'cot', 'assistant']].isna().any(axis = 1)]['conv_id'].unique()\n",
    "user_queries_df = user_queries_df[~user_queries_df['conv_id'].isin(convs_with_missing)]\n",
    "\n",
    "print(f\"After dropping missing: {len(user_queries_df['conv_id'].unique())} convs\")\n",
    "\n",
    "# 2. Filter out super long convs + convs with any message <= conv_min_length (to avoid issues with flag_message_type fn)\n",
    "user_queries_df =\\\n",
    "    user_queries_df\\\n",
    "    .assign(total_len = lambda df:\n",
    "        df.groupby('conv_id')['user_query'].transform(lambda x: x.str.len().sum()) +\n",
    "        df.groupby('conv_id')['cot'].transform(lambda x: x.str.len().sum()) +\n",
    "        df.groupby('conv_id')['assistant'].transform(lambda x: x.str.len().sum())\n",
    "    )\\\n",
    "    .assign(min_len = lambda df: df[['user_query', 'cot', 'assistant']].apply(lambda x: x.str.len()).min(axis=1))\\\n",
    "    .assign(conv_min_len = lambda df: df.groupby('conv_id')['min_len'].transform('min'))\\\n",
    "    .assign(conv_min_user_len = lambda df: df.groupby('conv_id')['user_query'].transform(lambda x: x.str.len().min()))\\\n",
    "    .query('total_len < 12000 and conv_min_len >= 5 and conv_min_user_len > 50')\\\n",
    "    .drop(columns = ['total_len', 'min_len', 'conv_min_len', 'conv_min_user_len'])\n",
    "\n",
    "print(f\"After dropping long convs: {len(user_queries_df['conv_id'].unique())} convs\")\n",
    "\n",
    "# 3. Filter out convs where any message is a substring of another message\n",
    "#    (this causes dupe issues with the flag_message_type fn, which does not handle tokens in multiple string contexts)\n",
    "def has_substring_messages(group):\n",
    "    contents = group['user_query'].tolist() + group['cot'].tolist() + group['assistant'].tolist()\n",
    "    for i, a in enumerate(contents):\n",
    "        for j, b in enumerate(contents):\n",
    "            if i != j and a in b:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "bad_convs = user_queries_df.groupby('conv_id').filter(has_substring_messages)['conv_id'].unique()\n",
    "user_queries_df = user_queries_df.pipe(lambda df: df[~df['conv_id'].isin(bad_convs)])\n",
    "print(f\"After dropping substr: {len(user_queries_df['conv_id'].unique())} convs\")\n",
    "\n",
    "# Combine into conv-level df\n",
    "convs_df =\\\n",
    "    user_queries_df\\\n",
    "    .sort_values(['conv_id', 'user_query_ix'])\\\n",
    "    .assign(conv_id = lambda df: df.groupby('conv_id', sort = False).ngroup())\\\n",
    "    .groupby('conv_id')\\\n",
    "    .apply(lambda g: pd.Series({\n",
    "        'dataset': g['dataset'].values[0],\n",
    "        'messages': [\n",
    "            msg\n",
    "            for _, row in g.iterrows()\n",
    "            for msg in [\n",
    "                {'role': 'user', 'content': row['user_query']},\n",
    "                {'role': 'cot', 'content': row['cot']},\n",
    "                {'role': 'assistant', 'content': row['assistant']}\n",
    "            ]\n",
    "        ]\n",
    "    }))\\\n",
    "    .reset_index()\\\n",
    "    [['conv_id', 'dataset', 'messages']]\\\n",
    "    .sample(n = 40)\\\n",
    "    .assign(conv_id = lambda df: range(len(df)))\n",
    "\n",
    "print(f\"Final: {len(convs_df['conv_id'].unique())} convs\")\n",
    "\n",
    "convs_df\n",
    "convs = convs_df['messages'].tolist()\n",
    "\n",
    "messages_df =\\\n",
    "    convs_df\\\n",
    "    .explode('messages')\\\n",
    "    .assign(message_ix = lambda df: df.groupby('conv_id').cumcount())\\\n",
    "    .assign(\n",
    "        role = lambda df: df['messages'].apply(lambda x: x['role']),\n",
    "        content = lambda df: df['messages'].apply(lambda x: x['content'])\n",
    "    )\\\n",
    "    .drop(columns = 'messages')\\\n",
    "    .reset_index(drop = True)\n",
    "\n",
    "convs = convs_df['messages'].tolist()\n",
    "\n",
    "display(convs_df.head(5))\n",
    "display(messages_df.head(5))\n",
    "print(f\"Total convs: {len(convs_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prep untagged conversations\n",
    "\"\"\"\n",
    "all_convs_by_type = {}\n",
    "\n",
    "# Prep conversations\n",
    "def prep_conv(conv):\n",
    "    return prepend + '\\n\\n'.join([x['content'] for x in conv])\n",
    "\n",
    "all_convs_by_type['untagged'] = [prep_conv(conv) for conv in convs]\n",
    "print(all_convs_by_type['untagged'][0])\n",
    "\n",
    "# Find max input length\n",
    "max_input_length =\\\n",
    "    tokenizer(all_convs_by_type['untagged'], padding = True, truncation = True, max_length = 1024 * 8, return_tensors = 'pt')\\\n",
    "    ['attention_mask'].sum(dim = 1).max().item()\n",
    "\n",
    "print(f\"Max input length: {max_input_length}\")\n",
    "print(f\"Untagged convs: {len(all_convs_by_type['untagged'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prep tagged conversations\n",
    "\"\"\"\n",
    "def fold_cot_into_final(convs):\n",
    "    \"\"\"Fold CoT into the following assistant message as a <think></think> tag.\"\"\"\n",
    "    result = []\n",
    "    for conv in convs:\n",
    "        new_conv = []\n",
    "        it = iter(enumerate(conv))\n",
    "        for i, msg in it:\n",
    "            if msg['role'] == 'cot':\n",
    "                if i + 1 >= len(conv) or conv[i + 1]['role'] != 'assistant':\n",
    "                    raise ValueError(\"cot must be followed by assistant\")\n",
    "                _, next_msg = next(it)\n",
    "                new_conv.append({\n",
    "                    'role': 'assistant',\n",
    "                    'content': f\"<think>{msg['content']}</think>{next_msg['content']}\"\n",
    "                })\n",
    "            else:\n",
    "                new_conv.append({'role': msg['role'], 'content': msg['content']})\n",
    "        result.append(new_conv)\n",
    "    return result\n",
    "\n",
    "final_convs = fold_cot_into_final(convs)\n",
    "all_convs_by_type['tagged'] = [\n",
    "    prepend + x\n",
    "    for x in tokenizer.apply_chat_template(final_convs, tokenize = False, add_generation_prompt = False)\n",
    "]\n",
    "print(all_convs_by_type['tagged'][0])\n",
    "\n",
    "# Find max input length\n",
    "max_input_length =\\\n",
    "    tokenizer(all_convs_by_type['tagged'], padding = True, truncation = True, max_length = 1024 * 8, return_tensors = 'pt')\\\n",
    "    ['attention_mask'].sum(dim = 1).max().item()\n",
    "\n",
    "print(f\"Max input length: {max_input_length}\")\n",
    "print(f\"Tagged convs: {len(all_convs_by_type['tagged'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run forward passes\n",
    "\"\"\"\n",
    "# Get input df\n",
    "input_convs_df =\\\n",
    "    pd.concat([\n",
    "        pd.DataFrame({'convs': v, 'conv_type': conv_type})\\\n",
    "            .assign(conv_id = lambda df: range(0, len(df)))\\\n",
    "            .merge(convs_df, how = 'inner', on = 'conv_id')\n",
    "        for conv_type, v in all_convs_by_type.items()\n",
    "    ], ignore_index = True)\\\n",
    "    .assign(prompt_ix = lambda df: list(range(0, len(df))))\n",
    "    \n",
    "display(input_convs_df)\n",
    "\n",
    "convs_dl = DataLoader(\n",
    "    ReconstructableTextDataset(input_convs_df['convs'].tolist(), tokenizer, max_length = 1024 * 4, prompt_ix = list(range(0, len(input_convs_df)))),\n",
    "    batch_size = 8,\n",
    "    shuffle = False,\n",
    "    collate_fn = stack_collate\n",
    ")\n",
    "\n",
    "convs_outputs = run_and_export_states(model, convs_dl, layers_to_keep_acts = probe_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Label messages\n",
    "\"\"\"\n",
    "# import importlib\n",
    "# import utils.substring_assignments\n",
    "# importlib.reload(utils.substring_assignments)\n",
    "# from utils.substring_assignments import flag_message_types\n",
    "from utils.substring_assignments import flag_message_types\n",
    "\n",
    "sample_dfs_by_conv = [group for _, group in convs_outputs['sample_df'].groupby('prompt_ix', sort = True)]\n",
    "metadata_by_conv = input_convs_df.to_dict('records')\n",
    "\n",
    "all_res = []\n",
    "\n",
    "# Iterate through (input metadata, token-level sample df) pairs\n",
    "for conv_metadata, sample_df_for_conv in tqdm(zip(metadata_by_conv, sample_dfs_by_conv)):\n",
    "\n",
    "    content_spans = [msg['content'] for msg in conv_metadata['messages']]\n",
    "    content_roles = [msg['role'] for msg in conv_metadata['messages']]\n",
    "    \n",
    "    try:\n",
    "        print(content_spans)\n",
    "        res = \\\n",
    "            flag_message_types(sample_df_for_conv, content_spans, True if 'olmo' in model_prefix else False)\\\n",
    "            .merge(\n",
    "                pd.DataFrame({'role': content_roles, 'base_message_ix': range(len(content_spans))}),\n",
    "                on = 'base_message_ix',\n",
    "                how = 'left'\n",
    "            )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    \n",
    "    all_res.append(res)\n",
    "\n",
    "display(pd.concat(all_res).pipe(lambda df: df[df['role'].isna()]))\n",
    "        \n",
    "sample_df_labeled =\\\n",
    "    pd.concat(all_res).reset_index(drop = True)\\\n",
    "    .assign(sample_ix = lambda df: range(0, len(df)))\\\n",
    "    .assign(token_in_prompt_ix = lambda df: df.groupby(['prompt_ix']).cumcount())\n",
    "\n",
    "display(sample_df_labeled.pipe(lambda df: df[~df['role'].isna()]))\n",
    "\n",
    "sample_df_labeled\\\n",
    "    .pipe(lambda df: df[~df['role'].isna()])\\\n",
    "    .groupby(['prompt_ix', 'role'], as_index = False)\\\n",
    "    .agg(count = ('sample_ix', 'count'))\\\n",
    "    .pivot(index = ['prompt_ix'], columns = 'role', values = 'count')\\\n",
    "    .head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = [\"User: Hi, what's up?\\nUser: Hi, what's up?\\nAssistant:\", 'Okay, the user just said \"Hi, what\\'s up?\" twice. Let me think about how to respond. First, I should acknowledge their greeting. Maybe start with a friendly hello back. Since they asked \"what\\'s up,\" I can offer to help with whatever they need. Keep it open-ended so they feel comfortable to ask anything. Also, since they repeated the same message, maybe they\\'re testing the response or just being casual. I should keep it light and welcoming. Let me make sure the tone is positive and approachable. Maybe add a smiley emoji to keep it friendly. Alright, something like \"Hello! Not much reallyâ€”just here to help you with whatever you need. What can I do for you today?\" That sounds good. Let me check for any typos or awkward phrasing. Nope, that works. Send that response.\\n', '\\n\\nHello! Not much reallyâ€”just here to help you with whatever you need. What can I do for you today? ðŸ˜Š']\n",
    "z[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = ['Okay, so I need to pretend to be an old woman named Maude, and my mission is to convince Peter, my neighbor, to smell my feet. Hmm, that\\'s a bit of a strange request. Let me think about how to approach this.\\n\\nFirst, I should consider the context. Why would Maude want Peter to smell her feet? Maybe it\\'s a cultural thing, like in some traditions people do that as a sign of respect or friendship? Or maybe it\\'s a joke or a prank? But since the user is asking me to roleplay as Maude convincing Peter, I need to come up with a believable reason within the character\\'s perspective.\\n\\nMaude is an old woman, so her tone should be warm, maybe a bit folksy, with a Southern or grandmotherly vibe. She might use humor or a kind gesture to make Peter feel comfortable. But how to get him to smell her feet? Maybe she\\'s trying to offer a foot massage? That could be a way to get him close and then suggest smelling them as part of the experience. Or perhaps she\\'s using some old-fashioned charm or a story to make it seem endearing.\\n\\nWait, but foot odor could be an issue. Maybe she\\'s been using some special foot powder or essential oils that make her feet smell nice? That could be a plausible reason. Or maybe she\\'s been taking care of her feet, like soaking them in something pleasant. She could mention that her feet are always clean and fragrant, and that Peter should smell them as a token of appreciation.\\n\\nAlternatively, maybe it\\'s a ritual or a way to show trust. In some cultures, sharing something personal like feet (which are often considered unclean) can be a sign of intimacy or friendship. So Maude might frame it as a gesture of trust, saying that if he\\'s willing to do this, it means he\\'s really close to her. But how to present that in a convincing way?\\n\\nI should also think about Peter\\'s possible reactions. He might be hesitant or grossed out. So Maude needs to address potential objections. Maybe she says something like, \"Oh, Peter, don\\'t be shy! My feet are as clean as a whistle, and I\\'ve been using this special soap. It\\'s a little tradition we have here in the neighborhood. It\\'s a way to show you care about me, you know? Just a small token of appreciation for being such a good neighbor.\"\\n\\nOr maybe she uses humor: \"Well, Peter, you know how I love my feet. Iâ€™ve been taking extra care of them lately, and I thought you might like to smell them. Theyâ€™re been treated real nice, with all the lavender and stuff. Itâ€™s a little secret, but I thought youâ€™d appreciate the effort.\"\\n\\nAnother angle: Maybe she\\'s been feeling unwell and her feet are a comfort to her, and she wants Peter to smell them to show he\\'s there for her. \"Oh, Peter, Iâ€™ve been feeling under the weather, and my feet just donâ€™t feel right. But Iâ€™ve been massaging them with this herbal oil, and theyâ€™re so soft now. I thought you might like to smell them to see how much better they feel. Itâ€™s a small favor, but Iâ€™d love your opinion!\"\\n\\nWait, but the mission is to convince him to smell her feet, not just to appreciate them. Maybe she\\'s been using foot powder that\\'s scented and she wants him to notice the pleasant smell. \"You know, Peter, Iâ€™ve been using this new foot powder from the market. Itâ€™s got this lovely rosemary scent. I thought you might like to smell it. Itâ€™s a little gift from me to you, just to brighten your day.\"\\n\\nOr perhaps she\\'s been through a tough time and this is a way to connect on a deeper level. \"You know, Peter, after my husband passed, Iâ€™ve been finding comfort in little rituals. Like, I used to bake bread every morning, and now Iâ€™ve taken up foot care. Iâ€™ve been soaking my feet in warm water with Epsom salts, and they smell so nice now. I thought you might like to smell them as a reminder that Iâ€™m still here, taking care of myself.\"\\n\\nI need to make sure the tone is friendly and not pushy. Maybe start with a compliment or a shared memory. \"Oh, Peter, you know how much I appreciate you being such a good neighbor. Iâ€™ve been thinking about how you helped me last week with my garden. Well, Iâ€™ve been treating my feet real special lately, and I thought you might like to smell them. Theyâ€™re been kept very clean and fragrant, just like you keep your place spotless!\"\\n\\nHmm, maybe adding a bit of a playful challenge. \"Come on, Peter! Youâ€™re my neighbor, and I know youâ€™re a good man. Whatâ€™s a little foot smell between friends? Itâ€™s nothing compared to the times youâ€™ve helped me out. Just a little experiment to see if you can tell the difference now!\"\\n\\nOr using a metaphor: \"You know how I always say, \\'A good neighbor smells like lavender and kindness.\\' Well, Iâ€™ve been taking that to heart, and my feet are the perfect example. Theyâ€™re been treated with the finest oils, and Iâ€™d love for you to be the first to notice the difference!\"\\n\\nI should also consider potential objections. Maybe Maude addresses them preemptively. \"Oh, donâ€™t you worry, Peter! Theyâ€™re as clean as a whistle. Iâ€™ve been using this special soap and keeping them moisturized. Itâ€™s just a little tradition Iâ€™ve picked up. You know how I value our friendship, and this is my way of showing you how much I care.\"\\n\\nAnother thought: Maybe she\\'s been through a foot injury and the smell is part of her recovery process. \"You know, Peter, Iâ€™ve been on my feet a lot lately, and Iâ€™ve been taking extra care of them. Iâ€™ve been using this herbal balm, and they smell so nice now. I thought you might like to smell them as a sign of my gratitude for your support.\"\\n\\nWait, but the user didn\\'t mention any backstory, so maybe stick to a more general approach. The key is to make it sound natural, kind, and maybe a bit quirky, but not offensive. Emphasize cleanliness and the effort she\\'s putting in. Maybe add a touch of humor to downplay the weirdness.\\n\\nPutting it all together, perhaps start with a friendly greeting, mention a recent interaction, then segue into the foot smell as a gesture of appreciation. Use phrases like \"just a little something,\" \"as a token of my appreciation,\" or \"a small favor.\" Make it sound like a kind gesture rather than something gross.\\n\\nFinalizing the approach: Maude is warm, mentions her care for Peter, talks about her feet being clean and fragrant due to her efforts, and frames it as a friendly, non-threatening gesture. Maybe add a light joke to make it less awkward. For example:\\n\\n\"Peter, my dear neighbor, you know how much I appreciate you always being so kind to me. Iâ€™ve been taking extra care of my feet lately, using this lovely lavender-scented soap and keeping them moisturized. I thought you might like to smell themâ€”itâ€™s a little tradition Iâ€™ve picked up here in the neighborhood to show how much I value our friendship. Donâ€™t be shy now, theyâ€™re as clean as a daisy! Just a small token of my gratitude for being such a wonderful neighbor.\"\\n\\nThat seems to cover the bases: friendly tone, mentions cleanliness, ties it to their relationship, and uses a light-hearted approach to make it less intimidating. Maybe add a smiley face or a wink if it\\'s in character, but since it\\'s text, just the words. Also, ensure it\\'s not too pushy, giving Peter an out if he\\'s uncomfortable.\\n', '\\n\\n**Maude:** (leaning on her porch railing with a warm, twinkle in her eye) Oh, Peter, my dear neighbor! You know how much I appreciate you always being so *thoughtful*â€”like that time you brought over those fresh-baked cookies when I was feeling under the weather. Well, Iâ€™ve been treating my feet real special lately, and I thought you might like to smell them. Theyâ€™ve been kept as clean as a whistle, you know? Iâ€™ve been using this lovely lavender-scented soap and keeping them moisturized with that honey-bath stuff I make. Itâ€™s a little tradition here in the neighborhoodâ€”showing someone you care by sharing a *small* token of appreciation. Donâ€™t be shy now, theyâ€™re as fresh as a daisy! Just a friendly gesture from your old neighborâ€¦ (winks) *If youâ€™re feeling bold!* ðŸ˜Š  \\n\\n*(She chuckles softly, patting the porch step with her cane.)*']\n",
    "for q in z:\n",
    "    print(q)\n",
    "    print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Clean up conversation pre_mlp_hs\n",
    "\"\"\"\n",
    "convs_pre_mlp_hs = convs_outputs['all_pre_mlp_hs'].to(torch.float16)\n",
    "convs_pre_mlp_hs = {layer_ix: convs_pre_mlp_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(probe_layers)} # Match layers_to_keep_act\n",
    "print(convs_pre_mlp_hs[0].shape)\n",
    "print(len(sample_df_labeled))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run projections\n",
    "\"\"\"\n",
    "test_layer = 16 # probe_layers[int(np.ceil(0.5 * len(probe_layers)).item())]\n",
    "test_roles = ['user', 'cot', 'assistant'] # system, user, cot, assistant, tool\n",
    "probe = [x for x in all_probes if sorted(x['roles']) == sorted(test_roles) and x['layer_ix'] == test_layer][0]\n",
    "\n",
    "project_sample_df = sample_df_labeled # Or drop non-roles\n",
    "project_hs_cp = cupy.asarray(convs_pre_mlp_hs[test_layer][project_sample_df['sample_ix'].tolist(), :])\n",
    "project_probs = probe['probe'].predict_proba(project_hs_cp).round(8)\n",
    "\n",
    "proj_results = pd.DataFrame(cupy.asnumpy(project_probs), columns = probe['roles']).clip(1e-6)\n",
    "if len(proj_results) != len(project_sample_df):\n",
    "    raise Exception(\"Error!\")\n",
    "\n",
    "role_level_df =\\\n",
    "    pd.concat([proj_results, project_sample_df[['sample_ix']]], axis = 1)\\\n",
    "    .melt(id_vars = ['sample_ix'], var_name = 'role_space', value_name = 'prob')\\\n",
    "    .merge(project_sample_df, on = 'sample_ix', how = 'inner')\\\n",
    "    .merge(input_convs_df[['prompt_ix', 'dataset', 'conv_id', 'conv_type']], on = 'prompt_ix')\n",
    "\n",
    "role_level_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tagged vs untagged\n",
    "\"\"\"\n",
    "print('Tagged')\n",
    "display(\n",
    "    role_level_df\\\n",
    "    .pipe(lambda df: df[df['conv_type'] == 'tagged'])\\\n",
    "    .pipe(lambda df: df[~df['role'].isna()])\\\n",
    "    .pipe(lambda df: df[(df['token_in_prompt_ix'] >= 0) & (df['token_in_prompt_ix'] <= 10000)])\\\n",
    "    .groupby(['dataset', 'role_space', 'role', 'prompt_ix'], as_index = False)\\\n",
    "    .agg(mean_prob = ('prob', 'mean'))\\\n",
    "    .groupby(['dataset', 'role_space', 'role'], as_index = False)\\\n",
    "    .agg(mean_prob = ('mean_prob', 'mean'))\\\n",
    "    .pivot(index = ['role', 'dataset'], columns = 'role_space', values = 'mean_prob')\n",
    ")\n",
    "\n",
    "print('Untagged')\n",
    "display(\n",
    "    role_level_df\\\n",
    "    .pipe(lambda df: df[df['conv_type'] == 'untagged'])\\\n",
    "    .pipe(lambda df: df[~df['role'].isna()])\\\n",
    "    .pipe(lambda df: df[(df['token_in_prompt_ix'] >= 0) & (df['token_in_prompt_ix'] <= 10000)])\\\n",
    "    .groupby(['dataset', 'role_space', 'role', 'prompt_ix'], as_index = False)\\\n",
    "    .agg(mean_prob = ('prob', 'mean'))\\\n",
    "    .groupby(['dataset', 'role_space', 'role'], as_index = False)\\\n",
    "    .agg(mean_prob = ('mean_prob', 'mean'))\\\n",
    "    .pivot(index = ['role', 'dataset'], columns = 'role_space', values = 'mean_prob')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "All-layer projections\n",
    "\"\"\"\n",
    "all_projs = []\n",
    "\n",
    "project_sample_df = sample_df_labeled # Or drop non-roles\n",
    "\n",
    "for probe in tqdm(all_probes):\n",
    "\n",
    "    project_hs_cp = cupy.asarray(convs_pre_mlp_hs[probe['layer_ix']][project_sample_df['sample_ix'].tolist(), :])\n",
    "\n",
    "    project_probs = probe['probe'].predict_proba(project_hs_cp).round(4)\n",
    "    proj_results = pd.DataFrame(cupy.asnumpy(project_probs), columns = probe['roles']).clip(1e-6)\n",
    "    if len(proj_results) != len(project_sample_df):\n",
    "        raise Exception(\"Error!\")\n",
    "\n",
    "    role_level_df =\\\n",
    "        pd.concat([proj_results, project_sample_df[['sample_ix']]], axis = 1)\\\n",
    "        .melt(id_vars = ['sample_ix'], var_name = 'role_space', value_name = 'prob')\\\n",
    "        .merge(project_sample_df, on = 'sample_ix', how = 'inner')\\\n",
    "        .merge(input_convs_df[['prompt_ix', 'dataset', 'conv_id', 'conv_type']], on = 'prompt_ix')\\\n",
    "        .assign(layer_ix = probe['layer_ix'])\\\n",
    "        .assign(roles = ','.join(probe['roles']))\\\n",
    "        .drop(columns = ['base_message'])\n",
    "\n",
    "    all_projs.append(role_level_df)\n",
    "\n",
    "all_projs_df =\\\n",
    "    pd.concat(all_projs)\\\n",
    "    .reset_index(drop = True)\\\n",
    "    .drop(columns = ['output_prob', 'output_id'])\\\n",
    "    .pipe(lambda df: df[~df['role'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Validation\n",
    "\"\"\"\n",
    "ft =\\\n",
    "    all_projs_df\\\n",
    "    .pipe(lambda df: df[df['roles'] == 'user,cot,assistant'])\\\n",
    "    .pipe(lambda df: df[df['role_space'] == df['role']])\\\n",
    "    .drop(columns = 'role_space')\\\n",
    "    .rename(columns = {'prob': 'acc'})\n",
    "\n",
    "print('Tagged, all layers')\n",
    "display(\n",
    "    ft\\\n",
    "        .pipe(lambda df: df[df['conv_type'] == 'tagged'])\\\n",
    "        .groupby(['roles', 'layer_ix', 'role', 'prompt_ix'], as_index = False)\\\n",
    "        .agg(mean_acc = ('acc', 'mean'))\\\n",
    "        .groupby(['roles', 'layer_ix', 'role'], as_index = False)\\\n",
    "        .agg(mean_acc = ('mean_acc', 'mean'))\\\n",
    "        .pivot(index = ['roles', 'layer_ix'], columns = 'role', values = 'mean_acc')\\\n",
    "        .reset_index()\\\n",
    "        .rename_axis(columns = None)\n",
    ")\n",
    "\n",
    "print('Untagged, all layers')\n",
    "display(\n",
    "    ft\n",
    "        .pipe(lambda df: df[df['conv_type'] == 'untagged'])\\\n",
    "        .groupby(['roles', 'layer_ix', 'role', 'prompt_ix'], as_index = False)\\\n",
    "        .agg(mean_acc = ('acc', 'mean'))\\\n",
    "        .groupby(['roles', 'layer_ix', 'role'], as_index = False)\\\n",
    "        .agg(mean_acc = ('mean_acc', 'mean'))\\\n",
    "        .pivot(index = ['roles', 'layer_ix'], columns = 'role', values = 'mean_acc')\\\n",
    "        .reset_index()\\\n",
    "        .rename_axis(columns = None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tomato test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def load_messages(yaml_path, model_key):\n",
    "    \"\"\"Loads system-like, assistant-like, user-like, etc. messages\"\"\"\n",
    "    with open(yaml_path, 'r', encoding = 'utf-8') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    return [x['content'] for x in data[model_key]]\n",
    "\n",
    "base_message_types = ['system', 'user', 'cot', 'assistant', 'user', 'cot', 'assistant']\n",
    "base_messages = load_messages(f\"{ws}/experiments/role-analysis/prompts/standard-conversations.yaml\", 'gptoss20')\n",
    "\n",
    "tomato_prompts = {}\n",
    "\n",
    "tomato_prompts['basic_no_format'] = prepend + '\\n'.join(base_messages)\n",
    "\n",
    "tomato_prompts['everything_in_user_tags'] = prepend + tokenizer.apply_chat_template(\n",
    "    [{'role': 'user', 'content': '\\n'.join(base_messages)}],\n",
    "    tokenize = False, add_generation_prompt = False\n",
    ")\n",
    "\n",
    "tomato_prompts['proper_tags'] = prepend + tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {'role': 'system', 'content': base_messages[0]},\n",
    "        {'role': 'user', 'content': base_messages[1]},\n",
    "        {'role': 'assistant', 'content': f\"<think>{base_messages[2]}</think>{base_messages[3]}\"},\n",
    "        {'role': 'user', 'content': base_messages[4]},\n",
    "        {'role': 'assistant', 'content': f\"<think>{base_messages[5]}</think>{base_messages[6]}\"}\n",
    "    ],\n",
    "    tokenize = False, add_generation_prompt = False\n",
    ")\n",
    "\n",
    "print(tomato_prompts['proper_tags'])\n",
    "\n",
    "tomato_input_df =\\\n",
    "    pd.DataFrame({\n",
    "        'prompt': [p for _, p in tomato_prompts.items()],\n",
    "        'prompt_key': [pk for pk, _ in tomato_prompts.items()]\n",
    "    })\\\n",
    "    .assign(prompt_ix = lambda df: list(range(0, len(df))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run asdfsdf\n",
    "\"\"\"\n",
    "tomato_dl = DataLoader(\n",
    "    ReconstructableTextDataset(tomato_input_df['prompt'].tolist(), tokenizer, max_length = 512 * 4, prompt_ix = tomato_input_df['prompt_ix'].tolist()),\n",
    "    batch_size = 16,\n",
    "    shuffle = False,\n",
    "    collate_fn = stack_collate\n",
    ")\n",
    "\n",
    "tomato_outputs = run_and_export_states(model, tomato_dl, layers_to_keep_acts = probe_layers)\n",
    "\n",
    "tomato_pre_mlp_hs = tomato_outputs['all_pre_mlp_hs'].to(torch.float16)\n",
    "tomato_pre_mlp_hs = {layer_ix: tomato_pre_mlp_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(probe_layers)} # Match layers_to_keep_act\n",
    "print(tomato_pre_mlp_hs[0].shape)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomato_sample_df_labeled =\\\n",
    "    flag_message_types(tomato_outputs['sample_df'], base_messages)\\\n",
    "    .assign(sample_ix = lambda df: range(0, len(df)))\\\n",
    "    .assign(token_in_prompt_ix = lambda df: df.groupby(['prompt_ix']).cumcount())\\\n",
    "    .merge(tomato_input_df, on = 'prompt_ix', how = 'inner')\n",
    "\n",
    "tomato_sample_df_labeled\\\n",
    "    .groupby(['prompt_ix', 'prompt_key', 'base_message_ix', 'base_message'], as_index = False)\\\n",
    "    .agg(count = ('token', 'count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomato_outputs['sample_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run projections\n",
    "\"\"\"\n",
    "test_layer = 16 # probe_layers[int(np.ceil(0.5 * len(probe_layers)).item())]\n",
    "test_roles = ['user', 'cot', 'assistant'] # system, user, cot, assistant, tool\n",
    "probe = [x for x in all_probes if sorted(x['roles']) == sorted(test_roles) and x['layer_ix'] == test_layer][0]\n",
    "\n",
    "project_hs_cp = cupy.asarray(tomato_pre_mlp_hs[test_layer][tomato_sample_df_labeled['sample_ix'].tolist(), :])\n",
    "project_probs = probe['probe'].predict_proba(project_hs_cp).round(8)\n",
    "\n",
    "proj_results = pd.DataFrame(cupy.asnumpy(project_probs), columns = probe['roles']).clip(1e-6)\n",
    "if len(proj_results) != len(tomato_sample_df_labeled):\n",
    "    raise Exception(\"Error!\")\n",
    "\n",
    "role_level_df =\\\n",
    "    pd.concat([proj_results, tomato_sample_df_labeled[['sample_ix']]], axis = 1)\\\n",
    "    .melt(id_vars = ['sample_ix'], var_name = 'role_space', value_name = 'prob')\\\n",
    "    .merge(tomato_sample_df_labeled, on = 'sample_ix', how = 'inner')\\\n",
    "    .merge(\n",
    "        pd.DataFrame({'base_message_ix': list(range(len(base_message_types))), 'base_message_type': base_message_types}),\n",
    "        on = 'base_message_ix',\n",
    "        how = 'left'\n",
    "    )\\\n",
    "    .assign(base_message_type = lambda df: df['base_message_type'].fillna('other'))\\\n",
    "    .pipe(lambda df: df[df['base_message_type'] != 'other']) # Cull the role tags themselves\n",
    "\n",
    "role_level_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "facet_order = probe['roles']\n",
    "all_message_types = role_level_df['base_message_type'].unique()\n",
    "color_map = {msg_type: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] for i, msg_type in enumerate(all_message_types)}\n",
    "\n",
    "smooth_df =\\\n",
    "    role_level_df\\\n",
    "    .sort_values('sample_ix')\\\n",
    "    .assign(\n",
    "        prob_sma = lambda df: df.groupby(['prompt_ix', 'role_space'])['prob'].rolling(window = 2, min_periods = 1).mean().reset_index(level = [0, 1], drop = True),\n",
    "        prob_ewma = lambda df: df.groupby(['prompt_ix', 'role_space'])['prob'].ewm(alpha = .5, min_periods = 1).mean().reset_index(level = [0, 1], drop = True)\n",
    "    )\n",
    "\n",
    "display(smooth_df)\n",
    "\n",
    "for this_prompt_key in smooth_df['prompt_key'].unique().tolist():\n",
    "    this_df = smooth_df.pipe(lambda df: df[df['prompt_key'] == this_prompt_key])\n",
    "    fig = px.scatter(\n",
    "        this_df, x = 'token_in_prompt_ix', y = 'prob_ewma',\n",
    "        facet_row = 'role_space',\n",
    "        color = 'base_message_type',\n",
    "        color_discrete_map = color_map,\n",
    "        category_orders = {\n",
    "            'role_space': facet_order,\n",
    "            'base_message_type': ['system', 'user', 'cot', 'assistant-final', 'other']\n",
    "        },\n",
    "        hover_name = 'token',\n",
    "        hover_data = {\n",
    "            'prob': ':.3f'\n",
    "        },\n",
    "        # markers = True,\n",
    "        title = f'prompt = {this_prompt_key}',\n",
    "        labels = {\n",
    "            'token_in_prompt_ix': 'Token Index',\n",
    "            'prob': 'Prob',\n",
    "            'prob_smoothed': 'Smoothed Prob',\n",
    "            'role_space': 'role'\n",
    "        }\n",
    "    ).update_yaxes(\n",
    "        range = [0, 1],\n",
    "        side = 'left'\n",
    "    ).update_layout(height = 500, width = 800)\n",
    "\n",
    "    def pretty(a):\n",
    "        a.update(\n",
    "            text = a.text.split(\"=\")[-1],\n",
    "            x = 0.5, xanchor = \"center\",\n",
    "            y = a.y + 0.08,\n",
    "            textangle = 0,\n",
    "            font = dict(size = 12),\n",
    "            bgcolor = 'rgba(255, 255, 255, 0.9)',  # light strip look\n",
    "            showarrow = False\n",
    "        )\n",
    "\n",
    "    fig.for_each_annotation(pretty)\n",
    "    fig.update_yaxes(title_text = None)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

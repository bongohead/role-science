{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This runs forward passes on samples and stores: (1) pre-MLP activations; (2) top-k expert selections; (3) sample metadata.\"\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Qwen3VLMoeForConditionalGeneration\n",
    "from transformers.loss.loss_utils import ForCausalLMLoss\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "from termcolor import colored\n",
    "import importlib\n",
    "import cupy\n",
    "import cuml\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.store_topk import convert_topk_to_df\n",
    "from utils.store_outputs import convert_outputs_to_df_fast\n",
    "from utils.misc import flatten_list\n",
    "from utils.role_templates import render_single_message\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 123\n",
    "\n",
    "ws = '/workspace/deliberative-alignment-jailbreaks'\n",
    "\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = '/workspace/deliberative-alignment-jailbreaks'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "selected_model_index = 4\n",
    "\n",
    "def get_model(index):\n",
    "    # HF model ID, model prefix, model architecture,  attn implementation, whether to use hf lib implementation\n",
    "    models = {\n",
    "        0: ('openai/gpt-oss-20b', 'gptoss20', 'gptoss', 'kernels-community/vllm-flash-attn3', True, 24),\n",
    "        1: ('openai/gpt-oss-120b', 'gptoss120', 'gptoss', 'kernels-community/vllm-flash-attn3', True, 36), # Will load experts in MXFP4 if triton kernels installed\n",
    "        2: ('Qwen/Qwen3-VL-30B-A3B-Thinking', 'qwen3vl30ba3b', 'qwen3vlmoe', None, True, 48),\n",
    "        3: ('zai-org/GLM-4.5-Air-FP8', 'glm45air', 'glm4moe', None, True, 45), # GLM-4.5 has one dense pre-layer, this is MoE layers\n",
    "        4: ('allenai/Olmo-3-7B-Think', 'olmo3-7', 'olmo3', None, True, 32) # OlMo-3\n",
    "    }\n",
    "    return models[index]\n",
    "\n",
    "def load_model_and_tokenizer(model_id, model_prefix, model_attn, model_use_hf):\n",
    "    \"\"\"\n",
    "    Load the model and tokenizer from HF, or from file if already downloaded.\n",
    "    \"\"\"\n",
    "    cache_dir = '/workspace/hf'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir = cache_dir, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "        \n",
    "    if model_architecture == 'qwen3vlmoe':\n",
    "        model = Qwen3VLMoeForConditionalGeneration.from_pretrained(model_id, cache_dir = cache_dir, dtype = 'auto', trust_remote_code = not model_use_hf, device_map = None, attn_implementation = model_attn).to(main_device).eval()\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, cache_dir = cache_dir, dtype = 'auto', trust_remote_code = not model_use_hf, device_map = None, attn_implementation = model_attn).to(main_device).eval()\n",
    "\n",
    "    return tokenizer, model    \n",
    "\n",
    "model_id, model_prefix, model_architecture, model_attn, model_use_hf, model_n_layers = get_model(selected_model_index)\n",
    "tokenizer, model = load_model_and_tokenizer(model_id, model_prefix, model_attn, model_use_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Some checks for special models (GPT-OSS - check attn + expert precision)\n",
    "\"\"\"\n",
    "if model_architecture == 'gptoss':\n",
    "    print(model.model.layers[0].mlp.experts.down_proj)\n",
    "    print(model.model.config._attn_implementation)\n",
    "\n",
    "# # Test model() call\n",
    "# inputs = tokenizer(['Test string'], return_tensors = 'pt', padding = 'max_length', truncation = True, max_length = 12).to(model.device)\n",
    "# with torch.no_grad():\n",
    "#     original_results = model(**inputs, use_cache = False)\n",
    "# print(original_results['logits'][0, :].detach().float().cpu().numpy())\n",
    "\n",
    "# # Test custom loader\n",
    "# import importlib\n",
    "# import utils.pretrained_models.ringmini2 as test_mod   # the module object\n",
    "# test_mod = importlib.reload(test_mod)\n",
    "# run_model_return_topk = test_mod.run_ringmini2_return_topk\n",
    "# custom_results = run_model_return_topk(model, inputs['input_ids'], inputs['attention_mask'], return_hidden_states = True)\n",
    "# print(custom_results['logits'][0, :].detach().float().cpu().numpy())\n",
    "\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load reverse-engineered forward pass functions that return topk expert IDs and weights\n",
    "\"\"\"\n",
    "model_module = importlib.import_module(f\"utils.pretrained_models.{model_architecture}\")\n",
    "run_model_return_topk = getattr(model_module, f\"run_{model_architecture}_return_topk\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_custom_forward_pass(model, pad_token_id):\n",
    "    inputs = tokenizer(['Hi! I am a dog and I like to bark', 'Vegetables are good for'], return_tensors = 'pt', padding = 'max_length', truncation = True, max_length = 640).to(model.device)\n",
    "    original_results = model(**inputs, use_cache = False)\n",
    "    custom_results = run_model_return_topk(model, inputs['input_ids'], inputs['attention_mask'], return_hidden_states = True)\n",
    "    assert torch.equal(original_results.logits, custom_results['logits']), 'Error in custom forward'\n",
    "    # assert len(custom_results['all_topk_experts']) == len(custom_results['all_topk_weights']), 'Length of topk IDs and weights not equal'\n",
    "    # print(f\"Length of topk: {len(custom_results['all_topk_experts'])}\")\n",
    "    # print(f\"Topk size: {custom_results['all_topk_experts'][0].shape}\")\n",
    "    # print(f\"First token topk IDs: {custom_results['all_topk_experts'][0][1,]}\")\n",
    "    # print(f\"First token topk weights: {custom_results['all_topk_weights'][0][1,]}\")\n",
    "    loss = ForCausalLMLoss(custom_results['logits'], torch.where(inputs['input_ids'] == pad_token_id, torch.tensor(-100), inputs['input_ids']), custom_results['logits'].size(-1)).detach().cpu().item()\n",
    "    print(f\"LM loss: {loss}\")\n",
    "    print(f\"Hidden states layers (pre-mlp | post-layer): {len(custom_results['all_pre_mlp_hidden_states'])} | {len(custom_results['all_hidden_states'])}\")\n",
    "    print(f\"Hidden state size (pre-mlp | post-layer): {(custom_results['all_pre_mlp_hidden_states'][0].shape)} | {(custom_results['all_hidden_states'][0].shape)}\")\n",
    "    # print(f\"Router logits : {(custom_results['all_router_logits'][0].shape)}\")\n",
    "\n",
    "test_custom_forward_pass(model, tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "s = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        # {'role': 'developer', 'content': 'Test.'},\n",
    "        {'role': 'user', 'content': 'Hi! I am a dog and I like to bark'},\n",
    "        {'role': 'assistant', 'content': 'I am a dog and I like to bark'}\n",
    "    ],\n",
    "    tokenize = False,\n",
    "    padding = 'max_length',\n",
    "    truncation = True,\n",
    "    max_length = 512,\n",
    "    add_generation_prompt = False\n",
    ")\n",
    "print(s)\n",
    "# print(re.sub(r'(Current date:\\s*)\\d{4}-\\d{2}-\\d{2}', '2025-09-01', s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import utils.role_templates\n",
    "importlib.reload(utils.role_templates)\n",
    "from utils.role_templates import render_single_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(render_single_message(model_architecture, role = 'assistant-cot',  content ='Hi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset - C4 + HPLT2\n",
    "\"\"\"\n",
    "n_sample_size = 100\n",
    "\n",
    "def load_raw_ds():\n",
    "\n",
    "    # def get_c4():\n",
    "    #     return load_dataset('allenai/c4', 'en', split = 'validation', streaming = True).shuffle(seed = seed, buffer_size = 50_000)\n",
    "    \n",
    "    def get_hplt2():\n",
    "        return load_dataset('HPLT/HPLT2.0_cleaned', 'eng_Latn', split = 'train', streaming = True).shuffle(seed = seed, buffer_size = 50_000)\n",
    "\n",
    "    def get_data(ds, n_samples, data_source): # en\n",
    "        raw_data = []\n",
    "        ds_iter = iter(ds)\n",
    "        for _ in range(n_samples):\n",
    "            sample = next(ds_iter, None)\n",
    "            if sample is None:\n",
    "                break\n",
    "            raw_data.append({'text': sample['text'], 'source': data_source})\n",
    "        return raw_data\n",
    "    \n",
    "    # return get_data(get_c4(), n_sample_size, 'c4-en')    \n",
    "    return get_data(get_hplt2(), n_sample_size, 'c4-en')    \n",
    "\n",
    "raw_data = load_raw_ds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ADD SYSTEM PROMPT?\n",
    "# import textwrap\n",
    "\n",
    "# if model_architecture == 'gptoss':\n",
    "#     system_prompt = tokenizer.bos_token + textwrap.dedent(\"\"\"\n",
    "#     <|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
    "#     Knowledge cutoff: 2024-06\n",
    "#     Current date: 2025-06-28\n",
    "\n",
    "#     Reasoning: medium\n",
    "\n",
    "#     # Valid channels: analysis, commentary, final. Channel must be included for every message.\n",
    "#     Calls to these tools must go to the commentary channel: 'functions'.<|end|>\n",
    "#     \"\"\").strip()\n",
    "# if model_architecture == 'olmo3':\n",
    "#     system_prompt = tokenizer.bos_token + textwrap.dedent(\"\"\"\n",
    "#     <|im_start|>system\n",
    "#     You are OLMo, a helpful function-calling AI assistant built by Ai2. Your date cutoff is November 2024, and your model weights are available at https://huggingface.co/allenai. You do not currently have access to any functions. <functions></functions><|im_end|>\n",
    "#     \"\"\").strip() + '\\n'\n",
    "\n",
    "# print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to get sample sequences\n",
    "\"\"\"\n",
    "truncated_texts = tokenizer.batch_decode(tokenizer([t['text'] for t in raw_data], padding = False, truncation = True, max_length = 640).input_ids)\n",
    "\n",
    "def get_sample_seqs(sample_str):\n",
    "    sample_seqs = {\n",
    "        'system': ('system', sample_str, None),\n",
    "        'user': ('user', sample_str, None),\n",
    "        'cot': ('assistant-cot', sample_str, None),\n",
    "        'assistant': ('assistant-final', sample_str, None),\n",
    "        'tool': ('tool', sample_str, None)\n",
    "    }\n",
    "    return [\n",
    "        {'role': k, 'prompt': render_single_message(model_architecture, v[0], v[1], v[2])}\n",
    "        for k, v in sample_seqs.items()\n",
    "    ]\n",
    "\n",
    "input_list = flatten_list([\n",
    "    [\n",
    "        {'question_ix': i, 'question': x, **p}\n",
    "        for p in get_sample_seqs(x)\n",
    "    ]\n",
    "    for i, x in enumerate(truncated_texts)\n",
    "])\n",
    "\n",
    "input_df =\\\n",
    "    pd.DataFrame(input_list)\\\n",
    "    .assign(prompt_ix = lambda df: list(range(len(df))))\n",
    "\n",
    "input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Load dataset into a dataloader. The dataloader returns the original tokens - this is important for BPE tokenizers as otherwise it's difficult to reconstruct the correct string later!\n",
    "\"\"\"\n",
    "from utils.dataset import ReconstructableTextDataset, stack_collate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def chunk_list(input_list, max_length):\n",
    "    return [input_list[i:i + max_length] for i in range(0, len(input_list), max_length)]\n",
    "\n",
    "# Create and chunk into lists of size 800 each - these will be the export breaks\n",
    "test_dl = DataLoader(\n",
    "    ReconstructableTextDataset(input_df['prompt'].tolist(), tokenizer, max_length = 768, prompt_ix = input_df['prompt_ix'].tolist()),\n",
    "    batch_size = 64,\n",
    "    shuffle = False,\n",
    "    collate_fn = stack_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run forward passes\n",
    "\"\"\"\n",
    "@torch.no_grad()\n",
    "def run_and_export_states(model, dl: ReconstructableTextDataset, layers_to_keep_acts: list[int]):\n",
    "    \"\"\"\n",
    "    Run forward passes on given model and store the decomposed sample_df plus hidden states\n",
    "\n",
    "    Params:\n",
    "        @model: The model to run forward passes on. Should return a dict with keys `logits` and `all_pre_mlp_hidden_states`.\n",
    "        @dl: A ReconstructableTextDataset of which returns `input_ids`, `attention_mask`, and `original_tokens`.\n",
    "        @layers_to_keep_acts: A list of layer indices (0-indexed) for which to filter `all_pre_mlp_hidden_states` (see returned object description).\n",
    "\n",
    "    Returns:\n",
    "        A dict with keys:\n",
    "        - `sample_df`: A sample (token)-level dataframe with corresponding input token ID, output token ID, and input token text (removes masked tokens)\n",
    "        - `all_pre_mlp_hidden_states`: A tensor of size n_samples x layers_to_keep_acts x D return the hidden state for each retained layers. Each \n",
    "            n_sample corresponds to a row of sample_df.\n",
    "\n",
    "    Example:\n",
    "        test_outputs = run_and_export_states(model, test_dl, layers_to_keep_acts = list(range(model_n_layers)))\n",
    "    \"\"\"\n",
    "    all_pre_mlp_hidden_states = []\n",
    "    sample_dfs = []\n",
    "\n",
    "    for batch_ix, batch in tqdm(enumerate(dl), total = len(dl)):\n",
    "\n",
    "        input_ids = batch['input_ids'].to(main_device)\n",
    "        attention_mask = batch['attention_mask'].to(main_device)\n",
    "        original_tokens = batch['original_tokens']\n",
    "        prompt_indices = batch['prompt_ix']\n",
    "\n",
    "        output = run_model_return_topk(model, input_ids, attention_mask, return_hidden_states = True)\n",
    "\n",
    "        # Check no bugs by validating output/perplexity\n",
    "        if batch_ix == 0:\n",
    "            loss = ForCausalLMLoss(output['logits'], torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids), output['logits'].size(-1)).detach().cpu().item()\n",
    "            for i in range(min(20, input_ids.size(0))):\n",
    "                decoded_input = tokenizer.decode(input_ids[i, :], skip_special_tokens = False)\n",
    "                next_token_id = torch.argmax(output['logits'][i, -1, :]).item()\n",
    "                print('---------\\n' + decoded_input + colored(tokenizer.decode([next_token_id], skip_special_tokens = False).replace('\\n', '<lb>'), 'green'))\n",
    "            print(f\"PPL:\", torch.exp(torch.tensor(loss)).item())\n",
    "                \n",
    "        original_tokens_df = pd.DataFrame(\n",
    "            [(seq_i, tok_i, tok) for seq_i, tokens in enumerate(original_tokens) for tok_i, tok in enumerate(tokens)], \n",
    "            columns = ['sequence_ix', 'token_ix', 'token']\n",
    "        )\n",
    "                \n",
    "        prompt_indices_df = pd.DataFrame(\n",
    "            [(seq_i, seq_source) for seq_i, seq_source in enumerate(prompt_indices)], \n",
    "            columns = ['sequence_ix', 'prompt_ix']\n",
    "        )\n",
    "        \n",
    "        # Create sample (token) level dataframe\n",
    "        sample_df =\\\n",
    "            convert_outputs_to_df_fast(input_ids, attention_mask, output['logits'])\\\n",
    "            .merge(original_tokens_df, how = 'left', on = ['token_ix', 'sequence_ix'])\\\n",
    "            .merge(prompt_indices_df, how = 'left', on = ['sequence_ix'])\\\n",
    "            .assign(batch_ix = batch_ix)\n",
    "        \n",
    "        sample_dfs.append(sample_df)\n",
    "\n",
    "        # Store pre-MLP hidden states - the fwd pass as n_layers list as BN x D, collapse to BN x n_layers x D, with BN filtering out masked items\n",
    "        valid_pos = torch.where(attention_mask.cpu().view(-1) == 1) # Valid (BN, ) positions\n",
    "        all_pre_mlp_hidden_states.append(torch.stack(output['all_pre_mlp_hidden_states'], dim = 1)[valid_pos][:, layers_to_keep_acts, :])\n",
    "\n",
    "        batch_ix += 1\n",
    "\n",
    "    sample_df = pd.concat(sample_dfs, ignore_index = True).drop(columns = ['batch_ix', 'sequence_ix']) # Drop batch/seq_ix, since prompt_ix identifies\n",
    "    all_pre_mlp_hidden_states = torch.cat(all_pre_mlp_hidden_states, dim = 0)\n",
    "\n",
    "    return {\n",
    "        'sample_df': sample_df,\n",
    "        'all_pre_mlp_hs': all_pre_mlp_hidden_states\n",
    "    }\n",
    "\n",
    "act_map = list(range(0, model_n_layers, 2 if model_n_layers <= 24 else 4))\n",
    "res = run_and_export_states(\n",
    "    model,\n",
    "    test_dl,\n",
    "    layers_to_keep_acts = act_map\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run probes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.role_assignments import label_content_roles\n",
    "import importlib\n",
    "import utils.role_assignments\n",
    "importlib.reload(utils.role_assignments)\n",
    "from utils.role_assignments import label_content_roles\n",
    "\n",
    "sample_df =\\\n",
    "    label_content_roles(model_architecture, res['sample_df'])\\\n",
    "    .assign(token_in_seq_ix = lambda df: df.groupby(['prompt_ix']).cumcount())\\\n",
    "    .assign(token_in_role_ix = lambda df: df.groupby(['prompt_ix', 'role']).cumcount())\\\n",
    "    .assign(sample_ix = lambda df: range(0, len(df)))\n",
    "\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df['role'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert activations to fp16 (for compatibility with cupy later) + layer-wise dict\n",
    "\"\"\"\n",
    "all_pre_mlp_hs = res['all_pre_mlp_hs'].to(torch.float16)\n",
    "# compare_bf16_fp16_batched(all_pre_mlp_hs_import, all_pre_mlp_hs)\n",
    "# del all_pre_mlp_hs_import\n",
    "all_pre_mlp_hs = {layer_ix: all_pre_mlp_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(act_map)}\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run logistic regression probes to get role space models\n",
    "\"\"\"\n",
    "def fit_lr(x_train, y_train, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Fit a probe with a standard 80/20 split\n",
    "    \"\"\"\n",
    "    lr_model = cuml.linear_model.LogisticRegression(penalty = 'l2', C = 1.0, max_iter = 5000, fit_intercept = True) # 1e-2 or 1e-3 reg\n",
    "    lr_model.fit(x_train, y_train)\n",
    "    accuracy = lr_model.score(x_test, y_test)\n",
    "    y_test_pred = lr_model.predict(x_test)\n",
    "    return lr_model, accuracy, y_test_pred\n",
    "\n",
    "def get_probe_result(layer_ix, label2id):\n",
    "    \"\"\"\n",
    "    Get probe results for a single layer and label combination\n",
    "\n",
    "    Params:\n",
    "        @layer_ix: The layer index to train the probe on\n",
    "        @label2id: The label-to-id mapping\n",
    "    \n",
    "    Description:\n",
    "        Trains only on content space for given roles\n",
    "    \"\"\"\n",
    "    # id2label = {v: k for k, v in label2id.items()}\n",
    "    roles = list(label2id.keys())\n",
    "\n",
    "    # Get valid samples\n",
    "    valid_sample_df =\\\n",
    "        sample_df\\\n",
    "        .pipe(lambda df: df[(df['in_content_span'] == True) & (df['role'].notna()) & (df['role'].isin(roles))])\n",
    "\n",
    "    # 80:20 prompt split\n",
    "    prompt_ix_train, prompt_ix_test = cuml.train_test_split(valid_sample_df['prompt_ix'].unique(), test_size = 0.2, random_state = seed)\n",
    "    train_df = valid_sample_df[valid_sample_df['prompt_ix'].isin(prompt_ix_train)]\n",
    "    test_df = valid_sample_df[valid_sample_df['prompt_ix'].isin(prompt_ix_test)]\n",
    "\n",
    "    # Get y labels\n",
    "    role_labels_train_cp = cupy.asarray([label2id[r] for r in train_df['role']])\n",
    "    role_labels_test_cp = cupy.asarray([label2id[r] for r in test_df['role']])\n",
    "\n",
    "    # Get x labels\n",
    "    x_train_cp = cupy.asarray(all_pre_mlp_hs[layer_ix][train_df['sample_ix'].tolist(), :].to(torch.float32).detach().cpu())\n",
    "    x_test_cp = cupy.asarray(all_pre_mlp_hs[layer_ix][test_df['sample_ix'].tolist(), :].to(torch.float32).detach().cpu())\n",
    "    \n",
    "    uniq_train = np.unique(role_labels_train_cp.get())\n",
    "    if len(uniq_train) < len(label2id):\n",
    "        raise Exception(f\"Skipping layer {layer_ix}, mapping {label2id}: missing roles in train\", uniq_train)\n",
    "\n",
    "    lr_model, test_acc, y_test_pred = fit_lr(x_train_cp, role_labels_train_cp, x_test_cp, role_labels_test_cp)\n",
    "\n",
    "    print(f\"Layer [{layer_ix}] with roles [{'+'.join(roles)}]: {test_acc:.2f}\")\n",
    "\n",
    "    # Optional: Return classification metrics\n",
    "    results_df =\\\n",
    "        test_df\\\n",
    "        .assign(pred = y_test_pred.tolist())\\\n",
    "        .assign(pred = lambda df: df['pred'].map({v: k for k, v in label2id.items()}))\\\n",
    "        .assign(is_acc = lambda df: df['role'] == df['pred'])\n",
    "\n",
    "    acc_by_role =\\\n",
    "        results_df\\\n",
    "        .groupby(['role', 'pred'], as_index = False)\\\n",
    "        .agg(count = ('sample_ix', 'count'))\n",
    "\n",
    "    acc_by_pos =\\\n",
    "        results_df\\\n",
    "        .groupby('token_in_role_ix', as_index = False)\\\n",
    "        .agg(count = ('sample_ix', 'count'), acc = ('is_acc', 'mean'))\n",
    "\n",
    "    return {\n",
    "        'layer_ix': layer_ix, 'label2id': label2id,\n",
    "        'roles': roles, 'probe': lr_model, 'accuracy': test_acc,\n",
    "        'acc_by_role': acc_by_role, 'acc_by_pos': acc_by_pos\n",
    "    }\n",
    "\n",
    "layers_to_test = list(all_pre_mlp_hs.keys()) # [i for i in list(all_pre_mlp_hs.keys()) if i % 4 == 0]\n",
    "mappings_to_test = [\n",
    "    {'user': 0, 'assistant-cot': 1, 'assistant-final': 2},\n",
    "    {'system': 0, 'user': 1, 'assistant-cot': 2, 'assistant-final': 3},\n",
    "    {'system': 0, 'user': 1, 'assistant-cot': 2, 'assistant-final': 3, 'tool': 4},\n",
    "    {'user': 0, 'assistant-cot': 1, 'assistant-final': 2, 'tool': 3}\n",
    "]\n",
    "\n",
    "all_probes = []\n",
    "for layer_ix in tqdm(layers_to_test):\n",
    "    for mapping in mappings_to_test:\n",
    "        all_probes.append(get_probe_result(layer_ix, mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalization Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix role names\n",
    "role_map = {'assistant-cot': 'cot', 'assistant-final': 'assistant'}\n",
    "for probe in all_probes:\n",
    "    probe['roles'] = tuple(role_map.get(r, r) for r in probe['roles'])\n",
    "\n",
    "probe_layers = sorted(set(x['layer_ix'] for x in all_probes))\n",
    "probe_roles = sorted(set(tuple(x['roles']) for x in all_probes))\n",
    "\n",
    "print(f\"Num probes: {str(len(all_probes))}\")\n",
    "print(f\"Probe layers:\\n  {', '.join([str(x) for x in probe_layers])}\")\n",
    "print(f\"Probe roles:\\n {'\\n '.join([str(list(x)) for x in probe_roles])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import utils.role_assignments\n",
    "importlib.reload(utils.role_assignments)\n",
    "from utils.role_assignments import label_content_roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tests two important functions:\n",
    "- `load_chat_template`: Overwrites the existing chat template in the tokenizer with one that better handles discrepancies.\n",
    "    See docstring for the function for details. \n",
    "- `label_content_roles`: Takes an instruct-formatted text, then assigns each token to the correct roles.\n",
    "\"\"\"\n",
    "import importlib\n",
    "import utils.role_assignments\n",
    "import utils.role_templates\n",
    "importlib.reload(utils.role_assignments)\n",
    "label_content_roles = utils.role_assignments.label_content_roles\n",
    "importlib.reload(utils.role_templates)\n",
    "load_chat_template = utils.role_templates.load_chat_template\n",
    "\n",
    "# Load custom chat templater\n",
    "old_chat_template = tokenizer.chat_template\n",
    "new_chat_template = load_chat_template(f'{ws}/utils/chat_templates', model_architecture)\n",
    "\n",
    "def test_chat_template(tokenizer):\n",
    "    s = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {'role': 'system', 'content': 'Test.'},\n",
    "            {'role': 'user', 'content': 'Hi! I\\'m a dog.'},\n",
    "            {'role': 'assistant', 'content': '<think>The user is a dog!</think>Congrats!'},\n",
    "            {'role': 'user', 'content': 'Thanks!'},\n",
    "            {'role': 'assistant', 'content': '<think>Hmm, the user said thanks!</think>Anything else?'}\n",
    "        ],\n",
    "        tokenize = False,\n",
    "        padding = 'max_length',\n",
    "        truncation = True,\n",
    "        max_length = 512,\n",
    "        add_generation_prompt = False\n",
    "    )\n",
    "    print(s)\n",
    "    return s\n",
    "\n",
    "tokenizer.chat_template = old_chat_template\n",
    "s = test_chat_template(tokenizer)\n",
    "tokenizer.chat_template = new_chat_template\n",
    "s = test_chat_template(tokenizer)\n",
    "\n",
    "z =\\\n",
    "    pd.DataFrame({'input_ids': tokenizer(s)['input_ids']})\\\n",
    "    .assign(token = lambda df: tokenizer.convert_ids_to_tokens(df['input_ids']))\\\n",
    "    .assign(prompt_ix = 0, batch_ix = 0, sequence_ix = 0, token_ix = lambda df: df.index)\n",
    "\n",
    "label_content_roles(model_architecture, z).tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run forward passes\n",
    "\"\"\"\n",
    "from utils.dataset import ReconstructableTextDataset, stack_collate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model_module = importlib.import_module(f\"utils.pretrained_models.{model_architecture}\")\n",
    "run_model_return_topk = getattr(model_module, f\"run_{model_architecture}_return_topk\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_and_export_states(model, dl: ReconstructableTextDataset, layers_to_keep_acts: list[int]):\n",
    "    \"\"\"\n",
    "    Run forward passes on given model and store the decomposed sample_df plus hidden states\n",
    "\n",
    "    Params:\n",
    "        @model: The model to run forward passes on. Should return a dict with keys `logits` and `all_pre_mlp_hidden_states`.\n",
    "        @dl: A ReconstructableTextDataset of which returns `input_ids`, `attention_mask`, and `original_tokens`.\n",
    "        @layers_to_keep_acts: A list of layer indices (0-indexed) for which to filter `all_pre_mlp_hidden_states` (see returned object description).\n",
    "\n",
    "    Returns:\n",
    "        A dict with keys:\n",
    "        - `sample_df`: A sample (token)-level dataframe with corresponding input token ID, output token ID, and input token text (removes masked tokens)\n",
    "        - `all_pre_mlp_hidden_states`: A tensor of size n_samples x layers_to_keep_acts x D return the hidden state for each retained layers. Each \n",
    "            n_sample corresponds to a row of sample_df.\n",
    "\n",
    "    Example:\n",
    "        test_outputs = run_and_export_states(model, test_dl, layers_to_keep_acts = list(range(model_n_layers)))\n",
    "    \"\"\"\n",
    "    all_pre_mlp_hidden_states = []\n",
    "    sample_dfs = []\n",
    "\n",
    "    for batch_ix, batch in tqdm(enumerate(dl), total = len(dl)):\n",
    "\n",
    "        input_ids = batch['input_ids'].to(main_device)\n",
    "        attention_mask = batch['attention_mask'].to(main_device)\n",
    "        original_tokens = batch['original_tokens']\n",
    "        prompt_indices = batch['prompt_ix']\n",
    "\n",
    "        output = run_model_return_topk(model, input_ids, attention_mask, return_hidden_states = True)\n",
    "\n",
    "        # Check no bugs by validating output/perplexity\n",
    "        if batch_ix == 0:\n",
    "            loss = ForCausalLMLoss(output['logits'], torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids), output['logits'].size(-1)).detach().cpu().item()\n",
    "            for i in range(min(20, input_ids.size(0))):\n",
    "                decoded_input = tokenizer.decode(input_ids[i, :], skip_special_tokens = False)\n",
    "                next_token_id = torch.argmax(output['logits'][i, -1, :]).item()\n",
    "                print('---------\\n' + decoded_input + colored(tokenizer.decode([next_token_id], skip_special_tokens = False).replace('\\n', '<lb>'), 'green'))\n",
    "            print(f\"PPL:\", torch.exp(torch.tensor(loss)).item())\n",
    "                \n",
    "        original_tokens_df = pd.DataFrame(\n",
    "            [(seq_i, tok_i, tok) for seq_i, tokens in enumerate(original_tokens) for tok_i, tok in enumerate(tokens)], \n",
    "            columns = ['sequence_ix', 'token_ix', 'token']\n",
    "        )\n",
    "                \n",
    "        prompt_indices_df = pd.DataFrame(\n",
    "            [(seq_i, seq_source) for seq_i, seq_source in enumerate(prompt_indices)], \n",
    "            columns = ['sequence_ix', 'prompt_ix']\n",
    "        )\n",
    "        \n",
    "        # Create sample (token) level dataframe\n",
    "        sample_df =\\\n",
    "            convert_outputs_to_df_fast(input_ids, attention_mask, output['logits'])\\\n",
    "            .merge(original_tokens_df, how = 'left', on = ['token_ix', 'sequence_ix'])\\\n",
    "            .merge(prompt_indices_df, how = 'left', on = ['sequence_ix'])\\\n",
    "            .assign(batch_ix = batch_ix)\n",
    "        \n",
    "        sample_dfs.append(sample_df)\n",
    "\n",
    "        # Store pre-MLP hidden states - the fwd pass as n_layers list as BN x D, collapse to BN x n_layers x D, with BN filtering out masked items\n",
    "        valid_pos = torch.where(attention_mask.cpu().view(-1) == 1) # Valid (BN, ) positions\n",
    "        all_pre_mlp_hidden_states.append(torch.stack(output['all_pre_mlp_hidden_states'], dim = 1)[valid_pos][:, layers_to_keep_acts, :])\n",
    "\n",
    "        batch_ix += 1\n",
    "\n",
    "    sample_df = pd.concat(sample_dfs, ignore_index = True).drop(columns = ['batch_ix', 'sequence_ix']) # Drop batch/seq_ix, since prompt_ix identifies\n",
    "    all_pre_mlp_hidden_states = torch.cat(all_pre_mlp_hidden_states, dim = 0)\n",
    "\n",
    "    return {\n",
    "        'sample_df': sample_df,\n",
    "        'all_pre_mlp_hs': all_pre_mlp_hidden_states\n",
    "    }\n",
    "\n",
    "def chunk_list(input_list, max_length):\n",
    "    return [input_list[i:i + max_length] for i in range(0, len(input_list), max_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import conversations\n",
    "\"\"\"\n",
    "# filename = model_id.split('/')[-1].lower()\n",
    "user_queries_df = pd.read_csv(f'{ws}/experiments/role-analysis/convs/gpt-oss-20b.csv')\n",
    "\n",
    "print(f\"Starting length: {len(user_queries_df['conv_id'].unique())} convs\")\n",
    "\n",
    "# 1. Remove convs with any missing responses\n",
    "convs_with_missing = user_queries_df[user_queries_df[['user_query', 'cot', 'assistant']].isna().any(axis = 1)]['conv_id'].unique()\n",
    "user_queries_df = user_queries_df[~user_queries_df['conv_id'].isin(convs_with_missing)]\n",
    "\n",
    "print(f\"After dropping missing: {len(user_queries_df['conv_id'].unique())} convs\")\n",
    "\n",
    "# 2. Filter out super long convs + convs with any message <= conv_min_length (to avoid issues with flag_message_type fn)\n",
    "user_queries_df =\\\n",
    "    user_queries_df\\\n",
    "    .assign(total_len = lambda df:\n",
    "        df.groupby('conv_id')['user_query'].transform(lambda x: x.str.len().sum()) +\n",
    "        df.groupby('conv_id')['cot'].transform(lambda x: x.str.len().sum()) +\n",
    "        df.groupby('conv_id')['assistant'].transform(lambda x: x.str.len().sum())\n",
    "    )\\\n",
    "    .assign(min_len = lambda df: df[['user_query', 'cot', 'assistant']].apply(lambda x: x.str.len()).min(axis=1))\\\n",
    "    .assign(conv_min_len = lambda df: df.groupby('conv_id')['min_len'].transform('min'))\\\n",
    "    .query('total_len < 12000 and conv_min_len >= 5')\\\n",
    "    .drop(columns = ['total_len', 'min_len', 'conv_min_len'])\n",
    "\n",
    "print(f\"After dropping long convs: {len(user_queries_df['conv_id'].unique())} convs\")\n",
    "\n",
    "# 3. Filter out convs where any message is a substring of another message\n",
    "#    (this causes dupe issues with the flag_message_type fn, which does not handle tokens in multiple string contexts)\n",
    "def has_substring_messages(group):\n",
    "    contents = group['user_query'].tolist() + group['cot'].tolist() + group['assistant'].tolist()\n",
    "    for i, a in enumerate(contents):\n",
    "        for j, b in enumerate(contents):\n",
    "            if i != j and a in b:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "bad_convs = user_queries_df.groupby('conv_id').filter(has_substring_messages)['conv_id'].unique()\n",
    "user_queries_df = user_queries_df.pipe(lambda df: df[~df['conv_id'].isin(bad_convs)])\n",
    "print(f\"After dropping substr: {len(user_queries_df['conv_id'].unique())} convs\")\n",
    "\n",
    "# Combine into conv-level df\n",
    "convs_df =\\\n",
    "    user_queries_df\\\n",
    "    .sort_values(['conv_id', 'user_query_ix'])\\\n",
    "    .assign(conv_id = lambda df: df.groupby('conv_id', sort = False).ngroup())\\\n",
    "    .groupby('conv_id')\\\n",
    "    .apply(lambda g: pd.Series({\n",
    "        'dataset': g['dataset'].values[0],\n",
    "        'messages': [\n",
    "            msg\n",
    "            for _, row in g.iterrows()\n",
    "            for msg in [\n",
    "                {'role': 'user', 'content': row['user_query']},\n",
    "                {'role': 'cot', 'content': row['cot']},\n",
    "                {'role': 'assistant', 'content': row['assistant']}\n",
    "            ]\n",
    "        ]\n",
    "    }))\\\n",
    "    .reset_index()\\\n",
    "    [['conv_id', 'dataset', 'messages']]\\\n",
    "    .sample(n = 50)\\\n",
    "    .assign(conv_id = lambda df: range(len(df)))\n",
    "\n",
    "print(f\"Final: {len(convs_df['conv_id'].unique())} convs\")\n",
    "\n",
    "convs_df\n",
    "convs = convs_df['messages'].tolist()\n",
    "\n",
    "messages_df =\\\n",
    "    convs_df\\\n",
    "    .explode('messages')\\\n",
    "    .assign(message_ix = lambda df: df.groupby('conv_id').cumcount())\\\n",
    "    .assign(\n",
    "        role = lambda df: df['messages'].apply(lambda x: x['role']),\n",
    "        content = lambda df: df['messages'].apply(lambda x: x['content'])\n",
    "    )\\\n",
    "    .drop(columns = 'messages')\\\n",
    "    .reset_index(drop = True)\n",
    "\n",
    "convs = convs_df['messages'].tolist()\n",
    "\n",
    "display(convs_df.head(5))\n",
    "display(messages_df.head(5))\n",
    "print(f\"Total convs: {len(convs_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "if model_architecture == 'gptoss':\n",
    "    system_prompt = tokenizer.bos_token + textwrap.dedent(\"\"\"\n",
    "    <|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
    "    Knowledge cutoff: 2024-06\n",
    "    Current date: 2025-06-28\n",
    "\n",
    "    Reasoning: medium\n",
    "\n",
    "    # Valid channels: analysis, commentary, final. Channel must be included for every message.\n",
    "    Calls to these tools must go to the commentary channel: 'functions'.<|end|>\n",
    "    \"\"\").strip()\n",
    "if model_architecture == 'olmo3':\n",
    "    system_prompt = tokenizer.bos_token + textwrap.dedent(\"\"\"\n",
    "    <|im_start|>system\n",
    "    You are OLMo, a helpful function-calling AI assistant built by Ai2. Your date cutoff is November 2024, and your model weights are available at https://huggingface.co/allenai. You do not currently have access to any functions. <functions></functions><|im_end|>\n",
    "    \"\"\").strip() + '\\n'\n",
    "\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prep untagged conversations\n",
    "\"\"\"\n",
    "all_convs_by_type = {}\n",
    "\n",
    "# Prep conversations\n",
    "def prep_conv(conv):\n",
    "    return system_prompt + '\\n\\n'.join([x['content'] for x in conv])\n",
    "\n",
    "all_convs_by_type['untagged'] = [prep_conv(conv) for conv in convs]\n",
    "print(all_convs_by_type['untagged'][0])\n",
    "\n",
    "# Find max input length\n",
    "max_input_length =\\\n",
    "    tokenizer(all_convs_by_type['untagged'], padding = True, truncation = True, max_length = 1024 * 8, return_tensors = 'pt')\\\n",
    "    ['attention_mask'].sum(dim = 1).max().item()\n",
    "\n",
    "print(f\"Max input length: {max_input_length}\")\n",
    "print(f\"Untagged convs: {len(all_convs_by_type['untagged'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prep tagged conversations\n",
    "\"\"\"\n",
    "def fold_cot_into_final(convs):\n",
    "    \"\"\"Fold CoT into the following assistant message as a <think></think> tag.\"\"\"\n",
    "    result = []\n",
    "    for conv in convs:\n",
    "        new_conv = []\n",
    "        it = iter(enumerate(conv))\n",
    "        for i, msg in it:\n",
    "            if msg['role'] == 'cot':\n",
    "                if i + 1 >= len(conv) or conv[i + 1]['role'] != 'assistant':\n",
    "                    raise ValueError(\"cot must be followed by assistant\")\n",
    "                _, next_msg = next(it)\n",
    "                new_conv.append({\n",
    "                    'role': 'assistant',\n",
    "                    'content': f\"<think>{msg['content']}</think>{next_msg['content']}\"\n",
    "                })\n",
    "            else:\n",
    "                new_conv.append({'role': msg['role'], 'content': msg['content']})\n",
    "        result.append(new_conv)\n",
    "    return result\n",
    "\n",
    "final_convs = fold_cot_into_final(convs)\n",
    "all_convs_by_type['tagged'] = [\n",
    "    # system_prompt + x \n",
    "    x\n",
    "    for x in tokenizer.apply_chat_template(final_convs, tokenize = False, add_generation_prompt = False)\n",
    "]\n",
    "print(all_convs_by_type['tagged'][0])\n",
    "\n",
    "# Find max input length\n",
    "max_input_length =\\\n",
    "    tokenizer(all_convs_by_type['tagged'], padding = True, truncation = True, max_length = 1024 * 8, return_tensors = 'pt')\\\n",
    "    ['attention_mask'].sum(dim = 1).max().item()\n",
    "\n",
    "print(f\"Max input length: {max_input_length}\")\n",
    "print(f\"Tagged convs: {len(all_convs_by_type['tagged'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run forward passes\n",
    "\"\"\"\n",
    "# Get input df\n",
    "input_convs_df =\\\n",
    "    pd.concat([\n",
    "        pd.DataFrame({'convs': v, 'conv_type': conv_type})\\\n",
    "            .assign(conv_id = lambda df: range(0, len(df)))\\\n",
    "            .merge(convs_df, how = 'inner', on = 'conv_id')\n",
    "        for conv_type, v in all_convs_by_type.items()\n",
    "    ], ignore_index = True)\\\n",
    "    .assign(prompt_ix = lambda df: list(range(0, len(df))))\n",
    "    \n",
    "display(input_convs_df)\n",
    "\n",
    "convs_dl = DataLoader(\n",
    "    ReconstructableTextDataset(input_convs_df['convs'].tolist(), tokenizer, max_length = 1024 * 4, prompt_ix = list(range(0, len(input_convs_df)))),\n",
    "    batch_size = 8,\n",
    "    shuffle = False,\n",
    "    collate_fn = stack_collate\n",
    ")\n",
    "\n",
    "convs_outputs = run_and_export_states(model, convs_dl, layers_to_keep_acts = probe_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Label messages\n",
    "\"\"\"\n",
    "from utils.substring_assignments import flag_message_types\n",
    "\n",
    "sample_dfs_by_conv = [group for _, group in convs_outputs['sample_df'].groupby('prompt_ix', sort = True)]\n",
    "metadata_by_conv = input_convs_df.to_dict('records')\n",
    "\n",
    "all_res = []\n",
    "\n",
    "# Iterate through (input metadata, token-level sample df) pairs\n",
    "for conv_metadata, sample_df_for_conv in tqdm(zip(metadata_by_conv, sample_dfs_by_conv)):\n",
    "\n",
    "    content_spans = [msg['content'] for msg in conv_metadata['messages']]\n",
    "    content_roles = [msg['role'] for msg in conv_metadata['messages']]\n",
    "    try:\n",
    "        res = \\\n",
    "            flag_message_types(sample_df_for_conv, content_spans)\\\n",
    "            .merge(\n",
    "                pd.DataFrame({'role': content_roles, 'base_message_ix': range(len(content_spans))}),\n",
    "                on = 'base_message_ix',\n",
    "                how = 'left'\n",
    "            )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    \n",
    "    all_res.append(res)\n",
    "\n",
    "display(pd.concat(all_res).pipe(lambda df: df[df['role'].isna()]))\n",
    "        \n",
    "sample_df_labeled =\\\n",
    "    pd.concat(all_res).reset_index(drop = True)\\\n",
    "    .assign(sample_ix = lambda df: range(0, len(df)))\\\n",
    "    .assign(token_in_prompt_ix = lambda df: df.groupby(['prompt_ix']).cumcount())\n",
    "\n",
    "sample_df_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Clean up conversation pre_mlp_hs\n",
    "\"\"\"\n",
    "convs_pre_mlp_hs = convs_outputs['all_pre_mlp_hs'].to(torch.float16)\n",
    "# del conv_outputs\n",
    "convs_pre_mlp_hs = {layer_ix: convs_pre_mlp_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(probe_layers)} # Match layers_to_keep_act\n",
    "\n",
    "print(convs_pre_mlp_hs[0].shape)\n",
    "print(len(sample_df_labeled))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run projections\n",
    "\"\"\"\n",
    "test_layer = 12 # probe_layers[int(np.ceil(0.5 * len(probe_layers)).item())]\n",
    "test_roles = ['user', 'cot', 'assistant'] # system, user, cot, assistant, tool\n",
    "probe = [x for x in all_probes if sorted(x['roles']) == sorted(test_roles) and x['layer_ix'] == test_layer][0]\n",
    "\n",
    "project_sample_df = sample_df_labeled # Or drop non-roles\n",
    "project_hs_cp = cupy.asarray(convs_pre_mlp_hs[test_layer][project_sample_df['sample_ix'].tolist(), :])\n",
    "project_probs = probe['probe'].predict_proba(project_hs_cp).round(8)\n",
    "\n",
    "proj_results = pd.DataFrame(cupy.asnumpy(project_probs), columns = probe['roles']).clip(1e-6)\n",
    "if len(proj_results) != len(project_sample_df):\n",
    "    raise Exception(\"Error!\")\n",
    "\n",
    "role_level_df =\\\n",
    "    pd.concat([proj_results, project_sample_df[['sample_ix']]], axis = 1)\\\n",
    "    .melt(id_vars = ['sample_ix'], var_name = 'role_space', value_name = 'prob')\\\n",
    "    .merge(project_sample_df, on = 'sample_ix', how = 'inner')\\\n",
    "    .merge(input_convs_df[['prompt_ix', 'dataset', 'conv_id', 'conv_type']], on = 'prompt_ix')\n",
    "\n",
    "## GET TOKEN-LEVEL IS IT ACCURATE?\n",
    "# token_level_df\n",
    "\n",
    "role_level_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_level_df\\\n",
    "    .pipe(lambda df: df[df['conv_type'] == 'tagged'])\\\n",
    "    .pipe(lambda df: df[~df['role'].isna()])\\\n",
    "    .pipe(lambda df: df[(df['token_in_prompt_ix'] >= 0) & (df['token_in_prompt_ix'] <= 10000)])\\\n",
    "    .groupby(['dataset', 'role_space', 'role', 'prompt_ix'], as_index = False)\\\n",
    "    .agg(mean_prob = ('prob', 'mean'))\\\n",
    "    .groupby(['dataset', 'role_space', 'role'], as_index = False)\\\n",
    "    .agg(mean_prob = ('mean_prob', 'mean'))\\\n",
    "    .pivot(index = ['role', 'dataset'], columns = 'role_space', values = 'mean_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_level_df\\\n",
    "    .pipe(lambda df: df[df['conv_type'] == 'untagged'])\\\n",
    "    .pipe(lambda df: df[~df['role'].isna()])\\\n",
    "    .pipe(lambda df: df[(df['token_in_prompt_ix'] >= 0) & (df['token_in_prompt_ix'] <= 10000)])\\\n",
    "    .groupby(['dataset', 'role_space', 'role', 'prompt_ix'], as_index = False)\\\n",
    "    .agg(mean_prob = ('prob', 'mean'))\\\n",
    "    .groupby(['dataset', 'role_space', 'role'], as_index = False)\\\n",
    "    .agg(mean_prob = ('mean_prob', 'mean'))\\\n",
    "    .pivot(index = ['role', 'dataset'], columns = 'role_space', values = 'mean_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run projections\n",
    "\"\"\"\n",
    "all_projs = []\n",
    "\n",
    "project_sample_df = sample_df_labeled # Or drop non-roles\n",
    "\n",
    "for probe in tqdm(all_probes):\n",
    "\n",
    "    project_hs_cp = cupy.asarray(convs_pre_mlp_hs[probe['layer_ix']][project_sample_df['sample_ix'].tolist(), :])\n",
    "\n",
    "    project_probs = probe['probe'].predict_proba(project_hs_cp).round(4)\n",
    "    proj_results = pd.DataFrame(cupy.asnumpy(project_probs), columns = probe['roles']).clip(1e-6)\n",
    "    if len(proj_results) != len(project_sample_df):\n",
    "        raise Exception(\"Error!\")\n",
    "\n",
    "    role_level_df =\\\n",
    "        pd.concat([proj_results, project_sample_df[['sample_ix']]], axis = 1)\\\n",
    "        .melt(id_vars = ['sample_ix'], var_name = 'role_space', value_name = 'prob')\\\n",
    "        .merge(project_sample_df, on = 'sample_ix', how = 'inner')\\\n",
    "        .merge(input_convs_df[['prompt_ix', 'dataset', 'conv_id', 'conv_type']], on = 'prompt_ix')\\\n",
    "        .assign(layer_ix = probe['layer_ix'])\\\n",
    "        .assign(roles = ','.join(probe['roles']))\\\n",
    "        .drop(columns = ['base_message'])\n",
    "\n",
    "    all_projs.append(role_level_df)\n",
    "\n",
    "all_projs_df =\\\n",
    "    pd.concat(all_projs)\\\n",
    "    .reset_index(drop = True)\\\n",
    "    .drop(columns = ['output_prob', 'output_id'])\\\n",
    "    .pipe(lambda df: df[~df['role'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_projs_df\\\n",
    "    .pipe(lambda df: df[df['roles'] == 'system,user,cot,assistant'])\\\n",
    "    .pipe(lambda df: df[df['conv_type'] == 'tagged'])\\\n",
    "    .pipe(lambda df: df[df['role_space'] == df['role']])\\\n",
    "    .drop(columns = 'role_space')\\\n",
    "    .rename(columns = {'prob': 'acc'})\\\n",
    "    .groupby(['roles', 'layer_ix', 'role', 'prompt_ix'], as_index = False)\\\n",
    "    .agg(mean_acc = ('acc', 'mean'))\\\n",
    "    .groupby(['roles', 'layer_ix', 'role'], as_index = False)\\\n",
    "    .agg(mean_acc = ('mean_acc', 'mean'))\\\n",
    "    .pivot(index = ['roles', 'layer_ix'], columns = 'role', values = 'mean_acc')\\\n",
    "    .reset_index()\\\n",
    "    .rename_axis(columns = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run qualitative tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train probes\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Glm4vForConditionalGeneration, AutoModelForImageTextToText\n",
    "from transformers.loss.loss_utils import ForCausalLMLoss\n",
    "from utils.dataset import ReconstructableTextDataset, stack_collate\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "from termcolor import colored\n",
    "import importlib\n",
    "import cupy\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.store_outputs import convert_outputs_to_df_fast\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 123\n",
    "\n",
    "ws = '/workspace/deliberative-alignment-jailbreaks'\n",
    "\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "selected_model_index = 2\n",
    "\n",
    "def get_model(index):\n",
    "    \"\"\"\n",
    "    - HF model id, model prefix (short model identifier), model arch\n",
    "    - Attn implementation, whether to use the HF default implementation, # hidden layers, whether is reasoning\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        0: ('openai/gpt-oss-20b', 'gptoss-20b', 'gptoss', 'kernels-community/vllm-flash-attn3', True, 24, True),\n",
    "        1: ('openai/gpt-oss-120b', 'gptoss-120b', 'gptoss', 'kernels-community/vllm-flash-attn3', True, 36, True), # Will load experts in MXFP4 if triton kernels installed\n",
    "        2: ('nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16', 'nemotron-3-nano', 'nemotron3', None, False, 52, True),\n",
    "        3: ('Qwen/Qwen3-30B-A3B-Thinking-2507', 'qwen3-30b-a3b', 'qwen3moe', None, True, 48, True),\n",
    "        4: ('zai-org/GLM-4.6V-Flash', 'glm-4.6v-flash', 'glm46v', None, True, 40, True),\n",
    "        5: ('ServiceNow-AI/Apriel-1.6-15b-Thinker', 'apriel-1.6-15b-thinker', 'apriel', None, True, 48, True),\n",
    "        6: ('allenai/Olmo-3-7B-Think', 'olmo3-7b-think', 'olmo3', None, True, 32, True),\n",
    "        7: ('ai21labs/AI21-Jamba-Reasoning- 3B', 'jamba-reasoning', 'jamba', None, True, 28, True),\n",
    "        8: ('zai-org/GLM-4.7-Flash', 'glm-4.7-flash', 'glm4moelite', 'eager', True, 46, True)\n",
    "    }\n",
    "    return models[index]\n",
    "\n",
    "def load_model_and_tokenizer(model_id, model_architecture, model_attn, model_use_hf):\n",
    "    \"\"\"\n",
    "    Load the model and tokenizer from HF, or from file if already downloaded.\n",
    "    \"\"\"\n",
    "    cache_dir = '/workspace/hf'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir = cache_dir, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "    load_params = {'cache_dir': cache_dir, 'dtype': 'auto', 'trust_remote_code': not model_use_hf, 'device_map': None, 'attn_implementation': model_attn}    \n",
    "    if model_architecture == 'glm46v':\n",
    "        model = Glm4vForConditionalGeneration.from_pretrained(model_id, **load_params).to(main_device).eval()\n",
    "    elif model_architecture == 'apriel':\n",
    "        model = AutoModelForImageTextToText.from_pretrained(model_id, **load_params).to(main_device).eval()\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, **load_params).to(main_device).eval()\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "model_id, model_prefix, model_architecture, model_attn, model_use_hf, model_n_layers, model_is_reasoning = get_model(selected_model_index)\n",
    "tokenizer, model = load_model_and_tokenizer(model_id, model_architecture, model_attn, model_use_hf)\n",
    "\n",
    "if model_architecture == 'gptoss':\n",
    "    print(model.model.layers[0].mlp.experts.down_proj) # Precision should be MXFP4\n",
    "    print(model.model.config._attn_implementation) # Attn should be FA3\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    print('Setting pad token automatically')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load reverse-engineered forward pass functions (usage note - this can be replaced by simpler hooks if desired)\n",
    "\"\"\"\n",
    "model_module = importlib.import_module(f\"utils.pretrained_models.{model_architecture}\")\n",
    "run_model_return_topk = getattr(model_module, f\"run_{model_architecture}_return_topk\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_custom_forward_pass(model, pad_token_id):\n",
    "    inputs = tokenizer(['Hi! I am a dog and I like to bark', 'Vegetables are good for'], return_tensors = 'pt', padding = 'max_length', truncation = True, max_length = 640).to(model.device)\n",
    "    original_results = model(**inputs, use_cache = False)\n",
    "    custom_results = run_model_return_topk(model, inputs['input_ids'], inputs['attention_mask'], return_hidden_states = True)\n",
    "    assert torch.equal(original_results.logits, custom_results['logits']), 'Error in custom forward'\n",
    "    loss = ForCausalLMLoss(custom_results['logits'], torch.where(inputs['input_ids'] == pad_token_id, torch.tensor(-100), inputs['input_ids']), custom_results['logits'].size(-1)).detach().cpu().item()\n",
    "    print(f\"LM loss: {loss}\")\n",
    "    print(f\"Hidden states layers (pre-mlp | post-layer): {len(custom_results['all_pre_mlp_hidden_states'])} | {len(custom_results['all_hidden_states'])}\")\n",
    "    print(f\"Hidden state size (pre-mlp | post-layer): {(custom_results['all_pre_mlp_hidden_states'][0].shape)} | {(custom_results['all_hidden_states'][0].shape)}\")\n",
    "\n",
    "test_custom_forward_pass(model, tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_MODEL = 'gptoss-20b'\n",
    "TEST_LAYER_IX = 12\n",
    "TEST_ROLE_SPACE = ['system', 'user', 'cot', 'assistant']\n",
    "\n",
    "with open(f'{ws}/experiments/role-analysis/outputs/probes/{TEST_MODEL}.pkl', 'rb') as f:\n",
    "    probes = pickle.load(f)\n",
    "\n",
    "probe = [p for p in probes if p['layer_ix'] == TEST_LAYER_IX and p['role_space'] == TEST_ROLE_SPACE][0]\n",
    "probe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset and collect states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define a lexicon\n",
    "\"\"\"\n",
    "MAX_ROWS = 25\n",
    "\n",
    "test_prefix = yaml.safe_load(open('experiment-config/probe.yaml'))[model_prefix]['test_prefix']\n",
    "raw_convs = pd.read_csv(f'{ws}/experiments/role-analysis/data/conversations/{TEST_MODEL}.csv')\n",
    "\n",
    "lexicon_df = pd.DataFrame([\n",
    "    {'role': role, 'content': test_prefix + ' ' + ' '.join([row[col]] * 20)}\n",
    "    for row in raw_convs.head(MAX_ROWS).to_dict('records')\n",
    "    for role, col in [('user', 'user_query'), ('cot', 'cot'), ('assistant', 'assistant')]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper for running + storing states\n",
    "\"\"\"\n",
    "@torch.no_grad()\n",
    "def run_and_export_states(model, dl: ReconstructableTextDataset, layers_to_keep_acts: list[int]):\n",
    "    \"\"\"\n",
    "    Run forward passes on given model and store the decomposed sample_df plus hidden states\n",
    "\n",
    "    Params:\n",
    "        @model: The model to run forward passes on. Should return a dict with keys `logits` and `all_hidden_states`.\n",
    "        @dl: A ReconstructableTextDataset of which returns `input_ids`, `attention_mask`, and `original_tokens`.\n",
    "        @layers_to_keep_acts: A list of layer indices (0-indexed) for which to filter `all_hidden_states` (see returned object description).\n",
    "\n",
    "    Returns:\n",
    "        A dict with keys:\n",
    "        - `sample_df`: A sample (token)-level dataframe with corresponding input token ID, output token ID, and input token text (removes masked tokens)\n",
    "        - `all_hidden_states`: A tensor of size n_samples x layers_to_keep_acts x D return the hidden state for each retained layers. Each \n",
    "            n_sample corresponds to a row of sample_df.\n",
    "\n",
    "    Example:\n",
    "        test_outputs = run_and_export_states(model, train_dl, layers_to_keep_acts = list(range(model_n_layers)))\n",
    "    \"\"\"\n",
    "    all_hidden_states = []\n",
    "    sample_dfs = []\n",
    "\n",
    "    for batch_ix, batch in tqdm(enumerate(dl), total = len(dl)):\n",
    "\n",
    "        input_ids = batch['input_ids'].to(main_device)\n",
    "        attention_mask = batch['attention_mask'].to(main_device)\n",
    "        original_tokens = batch['original_tokens']\n",
    "        prompt_indices = batch['prompt_ix']\n",
    "\n",
    "        output = run_model_return_topk(model, input_ids, attention_mask, return_hidden_states = True)\n",
    "\n",
    "        # Check no bugs by validating output/perplexity\n",
    "        if batch_ix == 0:\n",
    "            loss = ForCausalLMLoss(output['logits'], torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids), output['logits'].size(-1)).detach().cpu().item()\n",
    "            for i in range(min(20, input_ids.size(0))):\n",
    "                decoded_input = tokenizer.decode(input_ids[i, :], skip_special_tokens = False)\n",
    "                next_token_id = torch.argmax(output['logits'][i, -1, :]).item()\n",
    "                print('---------\\n' + decoded_input + colored(tokenizer.decode([next_token_id], skip_special_tokens = False).replace('\\n', '<lb>'), 'green'))\n",
    "            print(f\"PPL:\", torch.exp(torch.tensor(loss)).item())\n",
    "                \n",
    "        original_tokens_df = pd.DataFrame(\n",
    "            [(seq_i, tok_i, tok) for seq_i, tokens in enumerate(original_tokens) for tok_i, tok in enumerate(tokens)], \n",
    "            columns = ['sequence_ix', 'token_ix', 'token']\n",
    "        )\n",
    "                \n",
    "        prompt_indices_df = pd.DataFrame(\n",
    "            [(seq_i, seq_source) for seq_i, seq_source in enumerate(prompt_indices)], \n",
    "            columns = ['sequence_ix', 'prompt_ix']\n",
    "        )\n",
    "        \n",
    "        # Create sample (token) level dataframe\n",
    "        sample_df =\\\n",
    "            convert_outputs_to_df_fast(input_ids, attention_mask, output['logits'])\\\n",
    "            .merge(original_tokens_df, how = 'left', on = ['token_ix', 'sequence_ix'])\\\n",
    "            .merge(prompt_indices_df, how = 'left', on = ['sequence_ix'])\\\n",
    "            .assign(batch_ix = batch_ix)\n",
    "        \n",
    "        sample_dfs.append(sample_df)\n",
    "\n",
    "        # Store pre-MLP hidden states - the fwd pass as n_layers list as BN x D, collapse to BN x n_layers x D, with BN filtering out masked items\n",
    "        valid_pos = torch.where(attention_mask.cpu().view(-1) == 1) # Valid (BN, ) positions\n",
    "        all_hidden_states.append(torch.stack(output['all_pre_mlp_hidden_states'], dim = 1)[valid_pos][:, layers_to_keep_acts, :])\n",
    "\n",
    "    sample_df = pd.concat(sample_dfs, ignore_index = True).drop(columns = ['batch_ix', 'sequence_ix']) # Drop batch/seq_ix, since prompt_ix identifies\n",
    "    all_hidden_states = torch.cat(all_hidden_states, dim = 0)\n",
    "\n",
    "    return {\n",
    "        'sample_df': sample_df,\n",
    "        'all_hs': all_hidden_states\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_INPUT_LEN = 768\n",
    "\n",
    "input_dl = DataLoader(\n",
    "    ReconstructableTextDataset(lexicon_df['content'].tolist(), tokenizer, max_length = MAX_INPUT_LEN, prompt_ix = list(range(0, len(lexicon_df)))),\n",
    "    batch_size = 8,\n",
    "    shuffle = False,\n",
    "    collate_fn = stack_collate\n",
    ")\n",
    "\n",
    "run_result = run_and_export_states(model, input_dl, layers_to_keep_acts = [TEST_LAYER_IX])\n",
    "hs = run_result['all_hs'].to(torch.float16)\n",
    "hs = {layer_ix: hs[:, save_ix, :] for save_ix, layer_ix in enumerate([TEST_LAYER_IX])}\n",
    "sample_df = run_result['sample_df'].assign(sample_ix = lambda df: range(0, len(df)))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Role space projections\n",
    "\"\"\"\n",
    "def run_projections(valid_sample_df, layer_hs, probe):\n",
    "    \"\"\"\n",
    "    Run probe-level projections\n",
    "    \n",
    "    Params:\n",
    "        @valid_sample_df: A sample-level df with columns `sample_ix` (1... T - 1), `sample_ix`.\n",
    "            Can be shorter than full T - 1 due to pre-filters, as long as sample_ix corresponds to the full length.\n",
    "        @layer_hs: A tensor of size T x D for the layer to project.\n",
    "        @probe: The probe dict with keys `probe` (the trained model) and `role_space` (the roles list)\n",
    "    \n",
    "    Returns:\n",
    "        A df at (sample_ix, target_role) level with cols `sample_ix`, `target_role`, `prob`\n",
    "    \"\"\"\n",
    "    x_cp = cupy.asarray(layer_hs[valid_sample_df['sample_ix'].tolist(), :])\n",
    "    y_cp = probe['probe'].predict_proba(x_cp).round(12)\n",
    "\n",
    "    proj_results = pd.DataFrame(cupy.asnumpy(y_cp), columns = probe['role_space'])\n",
    "    if len(proj_results) != len(valid_sample_df):\n",
    "        raise Exception(\"Error!\")\n",
    "\n",
    "    role_df =\\\n",
    "        pd.concat([\n",
    "            proj_results.reset_index(drop = True),\n",
    "            valid_sample_df[['sample_ix']].reset_index(drop = True)\n",
    "        ], axis = 1)\\\n",
    "        .melt(id_vars = ['sample_ix'], var_name = 'target_role', value_name = 'prob')\\\n",
    "        .reset_index(drop = True)\\\n",
    "        .assign(prob = lambda df: df['prob'].round(4))\n",
    "\n",
    "    return role_df\n",
    "\n",
    "output_projections =\\\n",
    "    run_projections(valid_sample_df = sample_df, layer_hs = hs[TEST_LAYER_IX], probe = probe)\\\n",
    "    .merge(\n",
    "        sample_df[['prompt_ix', 'sample_ix', 'token']].assign(token_in_prompt_ix = lambda df: df.groupby(['prompt_ix']).cumcount()), \n",
    "        how = 'inner',\n",
    "        on = ['sample_ix']\n",
    "    )\\\n",
    "    .merge(lexicon_df.assign(prompt_ix = lambda df: range(0, len(df))), how = 'inner', on = 'prompt_ix')\\\n",
    "    .assign(rollprob = lambda df: df.groupby(['prompt_ix', 'target_role'])['prob'].transform(lambda x: x.ewm(alpha = 0.25).mean()))\n",
    "\n",
    "output_projections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = output_projections['target_role'].unique().tolist()\n",
    "\n",
    "output_agg =\\\n",
    "    output_projections\\\n",
    "    .groupby(['token_in_prompt_ix', 'target_role', 'role'], as_index = False)\\\n",
    "    .agg(mean_rollprob = ('rollprob', lambda x: x.mean()))\n",
    "    \n",
    "output_agg\\\n",
    "    .pivot(index = ['role', 'token_in_prompt_ix'], columns = 'target_role', values = 'mean_rollprob')\\\n",
    "    .reset_index()\\\n",
    "    .pipe(lambda df: df[df['token_in_prompt_ix'] >= MAX_INPUT_LEN - 10])\n",
    "\n",
    "# .to_csv('lexical_role_projections.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(output_agg, x = 'token_in_prompt_ix', y = 'mean_rollprob', title = 'role', color = 'target_role', facet_col = 'role')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = output_projections['target_role'].unique().tolist()\n",
    "\n",
    "output_projections\\\n",
    "    .assign(prob = lambda df: df.groupby(['prompt_ix', 'target_role'])['prob'].transform(lambda x: x.ewm(alpha = 0.25).mean()))\\\n",
    "    .pivot(index=['prompt_ix', 'sample_ix', 'token'], columns='target_role', values='prob')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test rendering\n",
    "\"\"\"\n",
    "import utils.role_templates\n",
    "importlib.reload(utils.role_templates)\n",
    "from utils.role_templates import render_single_message, render_mixed_cot\n",
    "\n",
    "def test_render():\n",
    "    print(tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {'role': 'user', 'content': 'Hi! I am a dog and I like to bark'},\n",
    "            {'role': 'assistant', 'content': 'Hello! What a lovely dog you are!'}\n",
    "        ],\n",
    "        tokenize = False, padding = 'max_length', truncation = True, max_length = 512, add_generation_prompt = True,\n",
    "        enable_thinking = True\n",
    "    ))\n",
    "    print('--')\n",
    "    print(render_single_message(model_prefix, role = 'user', content ='Hi'))\n",
    "    print('--')\n",
    "    if model_is_reasoning:\n",
    "        print(render_mixed_cot(model_prefix, 'The user...', 'Yes!'))\n",
    "    return True\n",
    "\n",
    "test_render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Validate forward passes work\n",
    "\"\"\"\n",
    "# Define opening texts to append at the start of each role for training. If multiple, will randomize equally.\n",
    "train_prefixes = yaml.safe_load(open('experiment-config/probe.yaml'))[model_prefix]['train_prefixes']\n",
    "for p in train_prefixes:\n",
    "    print(p)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test1():\n",
    "    conv = tokenizer.apply_chat_template(\n",
    "        [{'role': 'user', 'content': 'Hi stinky'}],\n",
    "        tokenize = False,\n",
    "        add_generation_prompt = True\n",
    "    )\n",
    "    inputs = tokenizer(conv, add_special_tokens = True, return_tensors = 'pt')\n",
    "    gen_ids = model.generate(inputs['input_ids'].to(main_device), attention_mask = inputs['attention_mask'].to(main_device), max_new_tokens = 250, do_sample = False)\n",
    "    df = pd.DataFrame({'token_id': gen_ids[0].tolist(), 'token': tokenizer.convert_ids_to_tokens(gen_ids[0].tolist()),})\n",
    "    print(tokenizer.decode(df['token_id'].tolist(), skip_special_tokens = False))\n",
    "    return df\n",
    "\n",
    "@torch.no_grad()\n",
    "def test2():\n",
    "    \"\"\"Quick test - can the model run forward passes correctly?\"\"\"\n",
    "    for tr_prefix in train_prefixes:\n",
    "        conv = tr_prefix + render_single_message(model_prefix, 'user', 'Where is Atlanta?') +\\\n",
    "            (\n",
    "            # '<|start|>assistant<|channel|>analysis<|message|>' # gpt-oss-*\n",
    "            # '<|im_start|>assistant\\n<think></think>' # Nemotron\n",
    "            # '<|assistant|>\\n<think>' # GLM-4.6V-Flash\n",
    "            '<|im_start|>assistant\\n<think>\\n' # Qwen3\n",
    "            # '\\n<|begin_assistant|>\\nHere are my reasoning steps:\\n' # Apriel-1.6-15B-Thinker\n",
    "            )\n",
    "        inputs = tokenizer(conv, add_special_tokens = False, return_tensors = 'pt')\n",
    "        gen_ids = model.generate(inputs['input_ids'].to(main_device), attention_mask = inputs['attention_mask'].to(main_device), max_new_tokens = 250, do_sample = False)\n",
    "        print(tokenizer.decode(gen_ids[0], skip_special_tokens = False))\n",
    "        print('\\n')\n",
    "\n",
    "test1()\n",
    "test2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create sample sequences\n",
    "\"\"\"\n",
    "IS_REASONING = model_is_reasoning\n",
    "SEQLEN = yaml.safe_load(open('experiment-config/probe.yaml'))[model_prefix]['seq_len']\n",
    "NESTED_REASONING = yaml.safe_load(open('experiment-config/probe.yaml'))[model_prefix]['nested_reasoning']\n",
    "\n",
    "def get_sample_seqs_for_input_seq(probe_text, partner_text, prefix = ''):\n",
    "    \"\"\"\n",
    "    Helper to convert a single input sequence into all examples\n",
    "\n",
    "    Params\n",
    "        @probe_text: The text we're extracting states from (appears in all roles)\n",
    "        @partner_text: Random paired text (only used in merged sample's assistant position)\n",
    "    \"\"\"\n",
    "    seqs = []\n",
    "    for role in ['system', 'user', 'tool']:\n",
    "        seqs.append({\n",
    "            'role': role,\n",
    "            'prompt': prefix + render_single_message(model_prefix, role = role, content = probe_text)\n",
    "        })\n",
    "\n",
    "    if IS_REASONING:\n",
    "        seqs.append({\n",
    "            'role': 'cot',\n",
    "            'prompt': prefix + render_single_message(model_prefix, role = 'cot', content = probe_text)\n",
    "        })\n",
    "\n",
    "    # If merge AND LRM, render with CoT prefix\n",
    "    if IS_REASONING and NESTED_REASONING:\n",
    "        seqs.append({\n",
    "            'role': 'assistant',\n",
    "            'prompt': prefix + render_mixed_cot(model_prefix, cot = partner_text, assistant = probe_text)\n",
    "        })\n",
    "        \n",
    "    # Else render standalone\n",
    "    else:\n",
    "        seqs.append({\n",
    "            'role': 'assistant',\n",
    "            'prompt': prefix + render_single_message(model_prefix, role = 'assistant', content = probe_text)\n",
    "        })\n",
    "\n",
    "    return seqs\n",
    "\n",
    "def build_sample_seqs(train_prefixes):\n",
    "    \"\"\"\n",
    "    Build all sample sequences\n",
    "    \"\"\"\n",
    "    truncated_texts = tokenizer.batch_decode(tokenizer([t['text'] for t in raw_data], add_special_tokens = False, padding = False, truncation = True, max_length = SEQLEN).input_ids)\n",
    "    n_seqs = len(truncated_texts)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    partner_lengths = ((np.random.beta(0.5, 4.0, size = n_seqs) * (SEQLEN/2 + 1)).astype(int)).tolist()\n",
    "    partner_texts = [\n",
    "        tokenizer.decode(tokenizer(text['text'], add_special_tokens = False, padding = False, truncation = True, max_length = int(partner_lengths[i])).input_ids)\n",
    "        for i, text in enumerate(raw_data)\n",
    "    ]\n",
    "    perm = np.random.permutation(n_seqs)\n",
    "    while np.any(perm == np.arange(n_seqs)):\n",
    "        perm = np.random.permutation(n_seqs)\n",
    "\n",
    "    sampled_prefixes = np.random.choice(train_prefixes, size = n_seqs)\n",
    "\n",
    "    input_list = []\n",
    "    for base_ix, base_text in enumerate(truncated_texts):\n",
    "        partner_text = partner_texts[int(perm[base_ix])].strip()\n",
    "        prefix = sampled_prefixes[base_ix]\n",
    "\n",
    "        for seq in get_sample_seqs_for_input_seq(base_text, partner_text, prefix):\n",
    "            row = {'question_ix': base_ix, 'question': base_text, **seq}\n",
    "            input_list.append(row)\n",
    "\n",
    "    input_df = pd.DataFrame(input_list).assign(prompt_ix = lambda df: list(range(len(df))))\n",
    "    return input_df\n",
    "\n",
    "input_df = build_sample_seqs(train_prefixes = train_prefixes)\n",
    "display(input_df)\n",
    "\n",
    "for p in [row['prompt'] for row in input_df.pipe(lambda df: df[df['question_ix'] == 2]).to_dict('records')]:\n",
    "    print(p)\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Load dataset into a dataloader which returns original tokens - important for BPE tokenizers to reconstruct the correct string later\n",
    "\"\"\"\n",
    "from utils.dataset import ReconstructableTextDataset, stack_collate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "max_seqlen = int(tokenizer(input_df['prompt'].tolist(), padding = True, truncation = False, return_tensors = 'pt')['attention_mask'].sum(dim = 1).max().item())\n",
    "train_dl = DataLoader(\n",
    "    ReconstructableTextDataset(input_df['prompt'].tolist(), tokenizer, max_length = max_seqlen, prompt_ix = input_df['prompt_ix'].tolist()),\n",
    "    batch_size = 32,\n",
    "    shuffle = False,\n",
    "    collate_fn = stack_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generations\n",
    "\n",
    "# train_dl = DataLoader(\n",
    "#     ReconstructableTextDataset(input_df['prompt'].tolist(), tokenizer, max_length = 48, prompt_ix = input_df['prompt_ix'].tolist()),\n",
    "#     batch_size = 1,\n",
    "#     shuffle = False,\n",
    "#     collate_fn = stack_collate\n",
    "# )\n",
    "\n",
    "# it = iter(train_dl)\n",
    "\n",
    "# for i in range(0, 16):\n",
    "#     batch = next(it)\n",
    "#     input_ids = batch['input_ids'].to(main_device)\n",
    "#     attention_mask = batch['attention_mask'].to(main_device)\n",
    "#     original_tokens = batch['original_tokens']\n",
    "#     prompt_indices = batch['prompt_ix']\n",
    "\n",
    "#     output = run_model_return_topk(model, input_ids, attention_mask, return_hidden_states = True)\n",
    "\n",
    "#     loss = ForCausalLMLoss(output['logits'], torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids), output['logits'].size(-1)).detach().cpu().item()\n",
    "#     for i in range(min(1000, input_ids.size(0))):\n",
    "#         decoded_input = tokenizer.decode(input_ids[i, :], skip_special_tokens = False)\n",
    "#         next_token_id = torch.argmax(output['logits'][i, -1, :]).item()\n",
    "#         print(decoded_input + colored(tokenizer.decode([next_token_id], skip_special_tokens = False).replace('\\n', '<lb>'), 'green'))\n",
    "#     print(f\"PPL:\", torch.exp(torch.tensor(loss)).round().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run forward passes\n",
    "\"\"\"\n",
    "@torch.no_grad()\n",
    "def run_and_export_states(model, dl: ReconstructableTextDataset, layers_to_keep_acts: list[int]):\n",
    "    \"\"\"\n",
    "    Run forward passes on given model and store the decomposed sample_df plus hidden states\n",
    "\n",
    "    Params:\n",
    "        @model: The model to run forward passes on. Should return a dict with keys `logits` and `all_hidden_states`.\n",
    "        @dl: A ReconstructableTextDataset of which returns `input_ids`, `attention_mask`, and `original_tokens`.\n",
    "        @layers_to_keep_acts: A list of layer indices (0-indexed) for which to filter `all_hidden_states` (see returned object description).\n",
    "\n",
    "    Returns:\n",
    "        A dict with keys:\n",
    "        - `sample_df`: A sample (token)-level dataframe with corresponding input token ID, output token ID, and input token text (removes masked tokens)\n",
    "        - `all_hidden_states`: A tensor of size n_samples x layers_to_keep_acts x D return the hidden state for each retained layers. Each \n",
    "            n_sample corresponds to a row of sample_df.\n",
    "\n",
    "    Example:\n",
    "        test_outputs = run_and_export_states(model, train_dl, layers_to_keep_acts = list(range(model_n_layers)))\n",
    "    \"\"\"\n",
    "    all_hidden_states = []\n",
    "    sample_dfs = []\n",
    "\n",
    "    for batch_ix, batch in tqdm(enumerate(dl), total = len(dl)):\n",
    "\n",
    "        input_ids = batch['input_ids'].to(main_device)\n",
    "        attention_mask = batch['attention_mask'].to(main_device)\n",
    "        original_tokens = batch['original_tokens']\n",
    "        prompt_indices = batch['prompt_ix']\n",
    "\n",
    "        output = run_model_return_topk(model, input_ids, attention_mask, return_hidden_states = True)\n",
    "\n",
    "        # Check no bugs by validating output/perplexity\n",
    "        if batch_ix == 0:\n",
    "            loss = ForCausalLMLoss(output['logits'], torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids), output['logits'].size(-1)).detach().cpu().item()\n",
    "            for i in range(min(20, input_ids.size(0))):\n",
    "                decoded_input = tokenizer.decode(input_ids[i, :], skip_special_tokens = False)\n",
    "                next_token_id = torch.argmax(output['logits'][i, -1, :]).item()\n",
    "                print('---------\\n' + decoded_input + colored(tokenizer.decode([next_token_id], skip_special_tokens = False).replace('\\n', '<lb>'), 'green'))\n",
    "            print(f\"PPL:\", torch.exp(torch.tensor(loss)).item())\n",
    "                \n",
    "        original_tokens_df = pd.DataFrame(\n",
    "            [(seq_i, tok_i, tok) for seq_i, tokens in enumerate(original_tokens) for tok_i, tok in enumerate(tokens)], \n",
    "            columns = ['sequence_ix', 'token_ix', 'token']\n",
    "        )\n",
    "                \n",
    "        prompt_indices_df = pd.DataFrame(\n",
    "            [(seq_i, seq_source) for seq_i, seq_source in enumerate(prompt_indices)], \n",
    "            columns = ['sequence_ix', 'prompt_ix']\n",
    "        )\n",
    "        \n",
    "        # Create sample (token) level dataframe\n",
    "        sample_df =\\\n",
    "            convert_outputs_to_df_fast(input_ids, attention_mask, output['logits'])\\\n",
    "            .merge(original_tokens_df, how = 'left', on = ['token_ix', 'sequence_ix'])\\\n",
    "            .merge(prompt_indices_df, how = 'left', on = ['sequence_ix'])\\\n",
    "            .assign(batch_ix = batch_ix)\n",
    "        \n",
    "        sample_dfs.append(sample_df)\n",
    "\n",
    "        # Store pre-MLP hidden states - the fwd pass as n_layers list as BN x D, collapse to BN x n_layers x D, with BN filtering out masked items\n",
    "        valid_pos = torch.where(attention_mask.cpu().view(-1) == 1) # Valid (BN, ) positions\n",
    "        all_hidden_states.append(torch.stack(output['all_pre_mlp_hidden_states'], dim = 1)[valid_pos][:, layers_to_keep_acts, :])\n",
    "\n",
    "    sample_df = pd.concat(sample_dfs, ignore_index = True).drop(columns = ['batch_ix', 'sequence_ix']) # Drop batch/seq_ix, since prompt_ix identifies\n",
    "    all_hidden_states = torch.cat(all_hidden_states, dim = 0)\n",
    "\n",
    "    return {\n",
    "        'sample_df': sample_df,\n",
    "        'all_hs': all_hidden_states\n",
    "    }\n",
    "\n",
    "layers_to_probe = list(range(0, model_n_layers, 4)) if model_n_layers >= 30 else list(range(0, model_n_layers, 2))\n",
    "res = run_and_export_states(model, train_dl, layers_to_keep_acts = layers_to_probe)\n",
    "\n",
    "# Convert to f16 for cupy compatability\n",
    "all_probe_hs = res['all_hs'].to(torch.float16)\n",
    "all_probe_hs = {layer_ix: all_probe_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(layers_to_probe)}\n",
    "del res['all_hs']\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
